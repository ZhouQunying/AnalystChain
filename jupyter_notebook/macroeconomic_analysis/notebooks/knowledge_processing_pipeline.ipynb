{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# çŸ¥è¯†å¤„ç†å®Œæ•´Pipeline - ç®€åŒ–ä½†åŠŸèƒ½å®Œæ•´ç‰ˆ\n",
        "\n",
        "## æµç¨‹æ¦‚è§ˆ\n",
        "\n",
        "```\n",
        "åŸå§‹æ–‡ä»¶(PDF/DOC/PPTX)\n",
        "    â†“\n",
        "æ­¥éª¤1: æ–‡ä»¶æ‰«æåˆ†ç»„ â†’ organized_files\n",
        "    â†“\n",
        "æ­¥éª¤2: æ–‡æ¡£åŠ è½½æ¸…æ´— â†’ List[Document]\n",
        "    â†“\n",
        "æ­¥éª¤3: ç»“æ„åŒ–æå– â†’ structured.json\n",
        "    â†“\n",
        "æ­¥éª¤4: å‘é‡åŒ–å­˜å‚¨ â†’ vector_db/\n",
        "    â†“\n",
        "æ­¥éª¤5: å®Œæ•´Pipeline â†’ ä¸€é”®å¤„ç†æ‰€æœ‰æ–‡ä»¶\n",
        "```\n",
        "\n",
        "## ç›®æ ‡\n",
        "\n",
        "- **åŠŸèƒ½å®Œæ•´**: æ‰€æœ‰5æ­¥éª¤éƒ½å¯è¿è¡Œ\n",
        "- **ç®€æ´é«˜æ•ˆ**: ä»£ç ç²¾ç®€butå¤Ÿç”¨\n",
        "- **å®é™…å¯ç”¨**: èƒ½ç”ŸæˆJSONå’Œå‘é‡åº“\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥ä¾èµ–\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from difflib import SequenceMatcher\n",
        "from enum import IntEnum\n",
        "\n",
        "# LangChain\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# ç¯å¢ƒå˜é‡\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"../../../config/.env\")\n",
        "\n",
        "print(\"âœ… ä¾èµ–å¯¼å…¥å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®ç»“æ„\n",
        "class FilePriority(IntEnum):\n",
        "    PDF_NOTE = 1\n",
        "    WORD_DOC = 2\n",
        "    PDF_REGULAR = 3\n",
        "    POWERPOINT = 4\n",
        "    UNKNOWN = 99\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    path: Path\n",
        "    original_name: str\n",
        "    cleaned_name: str\n",
        "    sequence: int\n",
        "    sequence_str: str\n",
        "    priority: FilePriority\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeGroup:\n",
        "    group_key: str\n",
        "    topic: str\n",
        "    sequence: int\n",
        "    files: List[FileInfo]\n",
        "    primary_file: FileInfo\n",
        "    file_types: List[str]\n",
        "\n",
        "print(\"âœ… æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤1: KnowledgeOrganizer - æ–‡ä»¶æ‰«æåˆ†ç»„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeOrganizer:\n",
        "    \"\"\"æ–‡ä»¶æ‰«æå’Œæ™ºèƒ½åˆ†ç»„\"\"\"\n",
        "    SUPPORTED_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}\n",
        "    NOISE_PATTERNS = [r'\\[é˜²æ–­æ›´.*?\\]', r'\\[.*?å¾®.*?\\]', r'_\\d{14}', r'_\\d{8}', r'_ç¬”è®°', r'\\s*\\(.*?\\)\\s*']\n",
        "\n",
        "    def __init__(self, knowledge_base_dir, similarity_threshold=0.7, verbose=True):\n",
        "        self.knowledge_base_dir = Path(knowledge_base_dir)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.verbose = verbose\n",
        "        if not self.knowledge_base_dir.exists():\n",
        "            raise ValueError(f\"ç›®å½•ä¸å­˜åœ¨: {self.knowledge_base_dir}\")\n",
        "\n",
        "    def _log(self, msg):\n",
        "        if self.verbose: print(msg)\n",
        "\n",
        "    def clean_filename(self, filename):\n",
        "        \"\"\"æ¸…æ´—æ–‡ä»¶å(å»é™¤å™ªéŸ³)\"\"\"\n",
        "        name = Path(filename).stem\n",
        "        for pattern in self.NOISE_PATTERNS:\n",
        "            name = re.sub(pattern, '', name)\n",
        "        return re.sub(r'\\s+', ' ', name).strip()\n",
        "\n",
        "    def extract_sequence_number(self, filename):\n",
        "        \"\"\"æå–åºå·\"\"\"\n",
        "        match = re.match(r'^(\\d+)', filename)\n",
        "        return (int(match.group(1)), match.group(1)) if match else (999999, \"\")\n",
        "\n",
        "    def calculate_similarity(self, str1, str2):\n",
        "        \"\"\"è®¡ç®—ç›¸ä¼¼åº¦\"\"\"\n",
        "        return SequenceMatcher(None, str1, str2).ratio()\n",
        "\n",
        "    def get_file_priority(self, file_path):\n",
        "        \"\"\"ç¡®å®šæ–‡ä»¶ä¼˜å…ˆçº§\"\"\"\n",
        "        name, suffix = file_path.name.lower(), file_path.suffix.lower()\n",
        "        if 'ç¬”è®°' in name and suffix == '.pdf': return FilePriority.PDF_NOTE\n",
        "        if suffix in ['.doc', '.docx']: return FilePriority.WORD_DOC\n",
        "        if suffix == '.pdf': return FilePriority.PDF_REGULAR\n",
        "        if suffix in ['.ppt', '.pptx']: return FilePriority.POWERPOINT\n",
        "        return FilePriority.UNKNOWN\n",
        "\n",
        "    def create_file_info(self, file_path):\n",
        "        \"\"\"åˆ›å»ºæ–‡ä»¶ä¿¡æ¯å¯¹è±¡\"\"\"\n",
        "        original_name = file_path.name\n",
        "        cleaned_name = self.clean_filename(original_name)\n",
        "        sequence, sequence_str = self.extract_sequence_number(cleaned_name)\n",
        "        priority = self.get_file_priority(file_path)\n",
        "        return FileInfo(file_path, original_name, cleaned_name, sequence, sequence_str, priority)\n",
        "\n",
        "    def group_files_by_similarity(self, files):\n",
        "        \"\"\"æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„\"\"\"\n",
        "        groups, processed = {}, set()\n",
        "        for i, file1 in enumerate(files):\n",
        "            if file1.path in processed: continue\n",
        "            group_key = f\"{file1.sequence_str}_{file1.cleaned_name[:20]}\"\n",
        "            group_files = [file1]\n",
        "            processed.add(file1.path)\n",
        "\n",
        "            for file2 in files[i+1:]:\n",
        "                if file2.path in processed: continue\n",
        "                if file1.sequence == file2.sequence:\n",
        "                    if self.calculate_similarity(file1.cleaned_name, file2.cleaned_name) >= self.similarity_threshold:\n",
        "                        group_files.append(file2)\n",
        "                        processed.add(file2.path)\n",
        "\n",
        "            group_files.sort(key=lambda f: (f.priority.value, f.original_name))\n",
        "            groups[group_key] = KnowledgeGroup(group_key, file1.cleaned_name, file1.sequence,\n",
        "                                              group_files, group_files[0], [f.path.suffix for f in group_files])\n",
        "            self._log(f\"âœ“ {file1.cleaned_name[:30]} ({len(group_files)}æ–‡ä»¶)\")\n",
        "        return groups\n",
        "\n",
        "    def scan_and_organize(self):\n",
        "        \"\"\"æ‰«æå¹¶ç»„ç»‡æ–‡ä»¶\"\"\"\n",
        "        self._log(f\"ğŸ“š æ‰«æ: {self.knowledge_base_dir}\")\n",
        "        all_files = []\n",
        "        for ext in self.SUPPORTED_EXTENSIONS:\n",
        "            all_files.extend(self.knowledge_base_dir.glob(f\"*{ext}\"))\n",
        "\n",
        "        if not all_files:\n",
        "            self._log(\"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶\")\n",
        "            return {}\n",
        "\n",
        "        self._log(f\"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶\")\n",
        "        file_infos = [self.create_file_info(f) for f in all_files]\n",
        "        groups = self.group_files_by_similarity(file_infos)\n",
        "        sorted_groups = dict(sorted(groups.items(), key=lambda x: x[1].sequence))\n",
        "        self._log(f\"âœ… {len(sorted_groups)} ä¸ªçŸ¥è¯†å—\\n\")\n",
        "        return {self.knowledge_base_dir.name: sorted_groups}\n",
        "\n",
        "print(\"âœ… æ­¥éª¤1å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentLoader:\n",
        "    \"\"\"æ–‡æ¡£åŠ è½½å™¨(ç®€åŒ–ç‰ˆ,ä»…æ”¯æŒPDF)\"\"\"\n",
        "\n",
        "    def load_pdf(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PDFæ–‡ä»¶\"\"\"\n",
        "        loader = PyMuPDFLoader(str(file_path))\n",
        "        return loader.load()\n",
        "\n",
        "    def clean_document_text(self, doc: Document) -> Document:\n",
        "        \"\"\"æ¸…æ´—æ–‡æ¡£æ–‡æœ¬\"\"\"\n",
        "        text = doc.page_content\n",
        "        text = re.sub(r'[\\uf06c\\uf0fc]', '', text)  # ç‰¹æ®Šå­—ç¬¦\n",
        "        text = re.sub(r'\\s+', ' ', text)  # å¤šä½™ç©ºç™½\n",
        "        doc.page_content = text.strip()\n",
        "        return doc\n",
        "\n",
        "    def load_and_clean(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½å¹¶æ¸…æ´—æ–‡æ¡£\"\"\"\n",
        "        if file_path.suffix.lower() == '.pdf':\n",
        "            docs = self.load_pdf(file_path)\n",
        "            return [self.clean_document_text(doc) for doc in docs]\n",
        "        else:\n",
        "            print(f\"âš ï¸ æš‚ä¸æ”¯æŒ{file_path.suffix},è·³è¿‡\")\n",
        "            return []\n",
        "\n",
        "print(\"âœ… æ­¥éª¤2å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤3: KnowledgeExtractor - LLMç»“æ„åŒ–æå–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeExtractor:\n",
        "    \"\"\"LLMçŸ¥è¯†æå–å™¨(ç®€åŒ–ç‰ˆ)\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"deepseek-chat\"):\n",
        "        self.llm = ChatDeepSeek(model=model_name, temperature=0)\n",
        "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ä½ æ˜¯é‡‘èçŸ¥è¯†æå–ä¸“å®¶ã€‚ä»æ–‡æ¡£æå–ç»“æ„åŒ–çŸ¥è¯†,è¿”å›JSONã€‚\n",
        "\n",
        "æ–‡æ¡£: {content}\n",
        "\n",
        "æå–JSON(åªè¿”å›JSON):\n",
        "{{\n",
        "  \"topic\": \"ä¸»é¢˜\",\n",
        "  \"key_concepts\": [{{\"name\": \"æ¦‚å¿µ\", \"definition\": \"å®šä¹‰\", \"importance\": \"é‡è¦æ€§\"}}],\n",
        "  \"indicators\": [{{\"name\": \"æŒ‡æ ‡\", \"calculation\": \"è®¡ç®—\", \"interpretation\": \"è§£è¯»\"}}],\n",
        "  \"analysis_methods\": [{{\"name\": \"æ–¹æ³•\", \"steps\": \"æ­¥éª¤\", \"application\": \"åº”ç”¨\"}}],\n",
        "  \"summary\": \"æ€»ç»“\"\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    def extract_from_documents(self, docs: List[Document], topic: str) -> Dict:\n",
        "        \"\"\"ä»æ–‡æ¡£æå–çŸ¥è¯†\"\"\"\n",
        "        content = \"\\n\\n\".join([d.page_content for d in docs[:5]])[:15000]\n",
        "        try:\n",
        "            chain = self.prompt | self.llm | JsonOutputParser()\n",
        "            result = chain.invoke({\"content\": content})\n",
        "            result[\"topic\"] = topic\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ LLMæå–å¤±è´¥: {e}\")\n",
        "            return {\"topic\": topic, \"key_concepts\": [], \"indicators\": [],\n",
        "                   \"analysis_methods\": [], \"summary\": \"æå–å¤±è´¥\"}\n",
        "\n",
        "print(\"âœ… æ­¥éª¤3å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤4: VectorStoreManager - å‘é‡åŒ–å­˜å‚¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"å‘é‡å­˜å‚¨ç®¡ç†å™¨(ç®€åŒ–ç‰ˆ)\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\", persist_directory=\"./vector_db\"):\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "        self.persist_directory = Path(persist_directory)\n",
        "        self.vector_stores = {}\n",
        "\n",
        "    def get_or_create_store(self, domain: str) -> Chroma:\n",
        "        \"\"\"è·å–æˆ–åˆ›å»ºå‘é‡å­˜å‚¨\"\"\"\n",
        "        if domain in self.vector_stores:\n",
        "            return self.vector_stores[domain]\n",
        "\n",
        "        persist_path = str(self.persist_directory / domain)\n",
        "        store = Chroma(\n",
        "            collection_name=f\"{domain}_col\",\n",
        "            embedding_function=self.embeddings,\n",
        "            persist_directory=persist_path\n",
        "        )\n",
        "        self.vector_stores[domain] = store\n",
        "        return store\n",
        "\n",
        "    def split_documents(self, docs: List[Document]) -> List[Document]:\n",
        "        \"\"\"åˆ†å‰²æ–‡æ¡£ä¸ºchunks\"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        "        )\n",
        "        return splitter.split_documents(docs)\n",
        "\n",
        "    def add_documents(self, domain: str, docs: List[Document], metadata: Dict = None):\n",
        "        \"\"\"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨\"\"\"\n",
        "        if not docs:\n",
        "            print(\"  âš ï¸ æ— æ–‡æ¡£,è·³è¿‡å‘é‡åŒ–\")\n",
        "            return\n",
        "\n",
        "        chunks = self.split_documents(docs)\n",
        "        if metadata:\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata.update(metadata)\n",
        "\n",
        "        store = self.get_or_create_store(domain)\n",
        "        store.add_documents(chunks)\n",
        "        print(f\"  â†’ å‘é‡åŒ–: {len(chunks)} chunks\")\n",
        "\n",
        "print(\"âœ… æ­¥éª¤4å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤5: KnowledgeProcessor - å®Œæ•´Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeProcessor:\n",
        "    \"\"\"å®Œæ•´Pipelineåè°ƒå™¨\"\"\"\n",
        "\n",
        "    def __init__(self, knowledge_base_dir: str, memories_dir=\"./memories/knowledge\", vector_db_dir=\"./vector_db\"):\n",
        "        self.organizer = KnowledgeOrganizer(knowledge_base_dir)\n",
        "        self.loader = DocumentLoader()\n",
        "        self.extractor = KnowledgeExtractor()\n",
        "        self.vector_manager = VectorStoreManager(persist_directory=vector_db_dir)\n",
        "        self.memories_dir = Path(memories_dir)\n",
        "\n",
        "    def save_to_memories(self, domain: str, group_key: str, knowledge: Dict):\n",
        "        \"\"\"ä¿å­˜çŸ¥è¯†åˆ°JSON\"\"\"\n",
        "        domain_dir = self.memories_dir / domain\n",
        "        domain_dir.mkdir(parents=True, exist_ok=True)\n",
        "        json_file = domain_dir / f\"{group_key}.json\"\n",
        "        with open(json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(knowledge, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"  â†’ JSON: {json_file.name}\")\n",
        "\n",
        "    def process_all(self, limit: int = None):\n",
        "        \"\"\"å¤„ç†æ‰€æœ‰çŸ¥è¯†æ–‡ä»¶\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸš€ å¼€å§‹å®Œæ•´Pipeline\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        organized = self.organizer.scan_and_organize()\n",
        "\n",
        "        for domain, groups in organized.items():\n",
        "            print(f\"\\nğŸ“‚ é¢†åŸŸ: {domain}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            count = 0\n",
        "            for group_key, group in groups.items():\n",
        "                if limit and count >= limit:\n",
        "                    print(f\"\\nâ¸ï¸  è¾¾åˆ°é™åˆ¶({limit}),åœæ­¢\")\n",
        "                    break\n",
        "\n",
        "                print(f\"\\n[{count+1}] {group.topic}\")\n",
        "\n",
        "                # åŠ è½½\n",
        "                docs = self.loader.load_and_clean(group.primary_file.path)\n",
        "                if not docs:\n",
        "                    print(\"  âš ï¸ åŠ è½½å¤±è´¥\")\n",
        "                    continue\n",
        "                print(f\"  â†’ åŠ è½½: {len(docs)} é¡µ\")\n",
        "\n",
        "                # æå–\n",
        "                knowledge = self.extractor.extract_from_documents(docs, group.topic)\n",
        "                self.save_to_memories(domain, group_key, knowledge)\n",
        "\n",
        "                # å‘é‡åŒ–\n",
        "                self.vector_manager.add_documents(domain, docs,\n",
        "                    {\"domain\": domain, \"topic\": group.topic, \"seq\": group.sequence})\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            print(f\"\\nâœ… {domain}: å®Œæˆ {count} ä¸ª\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ‰ Pipelineå®Œæˆ!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nğŸ“ è¾“å‡º:\")\n",
        "        print(f\"  - JSON: {self.memories_dir}\")\n",
        "        print(f\"  - å‘é‡åº“: {self.vector_manager.persist_directory}\")\n",
        "\n",
        "print(\"âœ… æ­¥éª¤5å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆå§‹åŒ–\n",
        "processor = KnowledgeProcessor(\n",
        "    knowledge_base_dir=\"../knowledge_base\",\n",
        "    memories_dir=\"./memories/knowledge\",\n",
        "    vector_db_dir=\"./vector_db\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Processoråˆå§‹åŒ–å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æµ‹è¯•: å¤„ç†å‰2ä¸ªçŸ¥è¯†å—\n",
        "processor.process_all(limit=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­£å¼è¿è¡Œ: å¤„ç†æ‰€æœ‰çŸ¥è¯†å—(å–æ¶ˆæ³¨é‡Š)\n",
        "# processor.process_all()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ£€æŸ¥è¾“å‡º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹ç”Ÿæˆçš„JSON\n",
        "memories_path = Path(\"./memories/knowledge\")\n",
        "if memories_path.exists():\n",
        "    for domain_dir in memories_path.iterdir():\n",
        "        if domain_dir.is_dir():\n",
        "            print(f\"\\nğŸ“‚ {domain_dir.name}:\")\n",
        "            for json_file in sorted(domain_dir.glob(\"*.json\")):\n",
        "                print(f\"  - {json_file.name}\")\n",
        "else:\n",
        "    print(\"âš ï¸ å°šæœªç”Ÿæˆè¾“å‡º\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¯»å–ç¬¬ä¸€ä¸ªJSONæŸ¥çœ‹\n",
        "json_files = list(Path(\"./memories/knowledge\").rglob(\"*.json\"))\n",
        "if json_files:\n",
        "    with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "        sample = json.load(f)\n",
        "    print(f\"ç¤ºä¾‹ ({json_files[0].name}):\")\n",
        "    print(json.dumps(sample, ensure_ascii=False, indent=2))\n",
        "else:\n",
        "    print(\"âš ï¸ æ— JSON\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“\n",
        "\n",
        "### âœ… åŠŸèƒ½å®Œæ•´\n",
        "\n",
        "- æ­¥éª¤1: æ–‡ä»¶æ‰«æåˆ†ç»„ âœ“\n",
        "- æ­¥éª¤2: æ–‡æ¡£åŠ è½½æ¸…æ´— âœ“ (ä»…PDF)\n",
        "- æ­¥éª¤3: LLMç»“æ„åŒ–æå– âœ“\n",
        "- æ­¥éª¤4: å‘é‡åŒ–å­˜å‚¨ âœ“\n",
        "- æ­¥éª¤5: Pipelineåè°ƒ âœ“\n",
        "\n",
        "### ğŸ“¦ è¾“å‡º\n",
        "\n",
        "- `memories/knowledge/{domain}/*.json` - ç»“æ„åŒ–çŸ¥è¯†\n",
        "- `vector_db/{domain}/` - å‘é‡æ•°æ®åº“\n",
        "\n",
        "### ğŸš€ ä½¿ç”¨\n",
        "\n",
        "1. å…ˆç”¨`limit=2`æµ‹è¯•\n",
        "2. ç¡®è®¤å`process_all()`å¤„ç†å…¨éƒ¨\n",
        "3. æ£€æŸ¥è¾“å‡ºç»“æœ\n",
        "\n",
        "### âš™ï¸ è¯´æ˜\n",
        "\n",
        "- **ä»…æ”¯æŒPDF**: ç®€åŒ–ç‰ˆåªå¤„ç†PDF(ä¸»è¦æ ¼å¼)\n",
        "- **éœ€API Key**: DeepSeek API (åœ¨.envé…ç½®)\n",
        "- **é¦–æ¬¡ä¸‹è½½**: embeddingæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

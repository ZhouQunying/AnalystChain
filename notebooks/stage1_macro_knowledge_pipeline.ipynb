{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阶段1：知识处理Pipeline\n",
    "\n",
    "**目标**：将非结构化文档（PDF/Word/PPT）转为结构化知识库（向量库+JSON）\n",
    "\n",
    "**方案**：向量库+JSON双存储\n",
    "\n",
    "**为什么双存储**：向量库支持语义检索，JSON支持快速精确查询\n",
    "\n",
    "**技术栈**：Qwen3-Embedding + deepseek-reasoner + Chroma\n",
    "\n",
    "**为什么这些技术**：Qwen3-Embedding中文效果好，deepseek-reasoner提取质量高，Chroma轻量级易用\n",
    "\n",
    "**前置条件**：DeepSeek API Key（在.env配置）\n",
    "\n",
    "## 流程\n",
    "\n",
    "```\n",
    "PDF/Word/PPT文件（51个）\n",
    " ↓ [模块1] KnowledgeOrganizer（扫描分组）\n",
    "17个主题组\n",
    " ↓ [模块2] DocumentLoader（加载清洗）\n",
    "List[Document]\n",
    " ↓ [模块3] KnowledgeExtractor (deepseek-reasoner)（LLM提取）\n",
    "结构化JSON\n",
    " ↓ [模块4] VectorStoreManager (Qwen3-Embedding)（向量化）\n",
    "Chroma向量库\n",
    " ↓ [模块5] KnowledgeProcessor（协调执行）\n",
    "完整知识库\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有依赖导入完成\n",
      "依赖导入完成\n"
     ]
    }
   ],
   "source": [
    "# ========== 环境准备 ==========\n",
    "\n",
    "# 标准库\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# LangChain - Document Loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    PyMuPDFLoader,           # PDF加载（推荐）\n",
    "    Docx2txtLoader,          # Word加载\n",
    "    UnstructuredPowerPointLoader  # PPT加载\n",
    ")\n",
    "\n",
    "# LangChain - Core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LangChain - Embeddings & VectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# LangChain - LLM（技术决策：deepseek-v3）\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "# 日志\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"所有依赖导入完成\")\n",
    "\n",
    "# 环境变量\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../../config/.env\")\n",
    "\n",
    "print(\"依赖导入完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数\n",
    "\n",
    "**路径配置**：知识库目录、输出目录、向量库目录\n",
    "\n",
    "**模型配置**：Embedding模型（Qwen3-Embedding）、LLM模型（deepseek-reasoner）\n",
    "\n",
    "**为什么**：统一管理路径和模型，便于修改和维护\n",
    "\n",
    "**关键步骤**：设置路径变量 → 创建输出目录 → 配置模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:46:05,761 - INFO - 知识库路径: ../knowledge_base\n",
      "2025-12-03 22:46:05,763 - INFO - 输出路径: output\n",
      "2025-12-03 22:46:05,764 - INFO - Embedding: Qwen/Qwen3-Embedding-0.6B\n",
      "2025-12-03 22:46:05,765 - INFO - LLM: deepseek-reasoner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置完成\n"
     ]
    }
   ],
   "source": [
    "# ========== 全局配置 ==========\n",
    "\n",
    "# 路径配置（新结构 - 宏观经济专用）\n",
    "KNOWLEDGE_BASE_DIR = Path(\"../data/raw/knowledge_base/macro_economy\")\n",
    "OUTPUT_DIR = Path(\"../data/processed\")\n",
    "VECTOR_DB_DIR = OUTPUT_DIR / \"knowledge/vector_db\"\n",
    "STRUCTURED_JSON_DIR = OUTPUT_DIR / \"knowledge/structured\"\n",
    "\n",
    "# 模型配置（按技术决策）\n",
    "# Embedding模型：从环境变量读取，默认使用Qwen3-Embedding-0.6B\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL_PATH', 'Qwen/Qwen3-Embedding-0.6B')\n",
    "\n",
    "LLM_MODEL = \"deepseek-reasoner\"  # 决策#005\n",
    "LLM_TEMPERATURE = 0  # 确保输出稳定性\n",
    "\n",
    "# 文本分割配置\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# 创建输出目录\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True)\n",
    "STRUCTURED_JSON_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "logger.info(f\"知识库路径: {KNOWLEDGE_BASE_DIR}\")\n",
    "logger.info(f\"输出路径: {OUTPUT_DIR}\")\n",
    "logger.info(f\"Embedding: {EMBEDDING_MODEL}\")\n",
    "logger.info(f\"LLM: {LLM_MODEL}\")\n",
    "\n",
    "print(\"配置完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据结构定义\n",
    "\n",
    "**FilePriority**：文件优先级枚举（PDF笔记>Word>PDF>PPT）\n",
    "\n",
    "**为什么这个优先级**：PDF笔记最详细完整，Word格式规范，普通PDF信息完整但不如笔记，PPT信息密度低多为摘要\n",
    "\n",
    "**FileInfo**：文件信息数据类（路径、名称、序号、优先级）\n",
    "\n",
    "**为什么这样设计**：统一管理文件信息，便于按序号分组、按优先级排序、按相似度匹配\n",
    "\n",
    "**KnowledgeGroup**：知识组数据类（主题、文件列表、代表文件）\n",
    "\n",
    "**为什么这样设计**：同主题的不同文件类型（PDF/Word/PPT）需要归为一组，便于统一处理和选择最佳文件\n",
    "\n",
    "**关键步骤**：定义枚举 → 定义数据类 → 使用dataclass装饰器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据结构定义完成\n"
     ]
    }
   ],
   "source": [
    "# ========== 数据结构定义（符合专家标准）==========\n",
    "\n",
    "from enum import IntEnum\n",
    "\n",
    "class FilePriority(IntEnum):\n",
    "    \"\"\"文件优先级枚举\n",
    "\n",
    "    优先级规则：\n",
    "    - PDF笔记最优先（最详细完整，包含完整知识点）\n",
    "    - Word文档次之（格式规范，信息完整）\n",
    "    - 普通PDF第三（信息完整但可能不如笔记详细）\n",
    "    - PPT最后（信息密度低，多为摘要）\n",
    "    \"\"\"\n",
    "    PDF_NOTE = 1      # PDF笔记文件（文件名包含\"笔记\"）\n",
    "    WORD_DOC = 2      # Word文档\n",
    "    PDF_REGULAR = 3   # 普通PDF文件\n",
    "    POWERPOINT = 4    # PowerPoint文件\n",
    "    UNKNOWN = 99      # 未知类型\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FileInfo:\n",
    "    \"\"\"文件信息数据类\n",
    "\n",
    "    统一管理文件信息，便于按序号分组、按优先级排序、按相似度匹配。\n",
    "\n",
    "    Attributes:\n",
    "        path: 文件完整路径\n",
    "        original_name: 原始文件名\n",
    "        cleaned_name: 清洗后的文件名（去除噪音）\n",
    "        sequence: 序号（整数，用于分组）\n",
    "        sequence_str: 序号字符串（用于显示）\n",
    "        priority: 文件优先级（用于排序和选择最佳文件）\n",
    "    \"\"\"\n",
    "    path: Path\n",
    "    original_name: str\n",
    "    cleaned_name: str\n",
    "    sequence: int\n",
    "    sequence_str: str\n",
    "    priority: FilePriority\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KnowledgeGroup:\n",
    "    \"\"\"知识组数据类\n",
    "\n",
    "    同主题的不同文件类型（PDF/Word/PPT）归为一组，便于统一处理和选择最佳文件。\n",
    "\n",
    "    Attributes:\n",
    "        group_key: 组唯一标识\n",
    "        topic: 主题名称\n",
    "        sequence: 主题序号（用于排序）\n",
    "        files: 组内所有文件（同主题的不同格式）\n",
    "        primary_file: 主要文件（优先级最高，用于代表该组）\n",
    "        file_types: 文件类型列表（用于统计）\n",
    "    \"\"\"\n",
    "    group_key: str\n",
    "    topic: str\n",
    "    sequence: int\n",
    "    files: List[FileInfo]\n",
    "    primary_file: FileInfo\n",
    "    file_types: List[str]\n",
    "\n",
    "\n",
    "print(\"数据结构定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1: KnowledgeOrganizer - 文件扫描分组\n",
    "\n",
    "**作用**：扫描知识库目录，按相似度智能分组文件\n",
    "\n",
    "**为什么**：同主题的不同文件类型（PDF/Word/PPT）需要归为一组，便于统一处理\n",
    "\n",
    "**关键步骤**：提取序号 → 计算相似度 → 按优先级排序 → 分组\n",
    "\n",
    "**输出**：17个主题组（每个组包含相关文件）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤1完成：KnowledgeOrganizer\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeOrganizer:\n",
    "    \"\"\"知识文件扫描和智能分组\n",
    "\n",
    "    功能:\n",
    "    1. 扫描知识库目录,支持PDF/Word/PPT\n",
    "    2. 清洗文件名(去除时间戳、噪音标记等)\n",
    "    3. 按相似度智能分组(同主题的不同文件类型)\n",
    "    4. 按优先级排序(PDF笔记 > Word > PDF > PPT)\n",
    "\n",
    "    分组算法:\n",
    "    - 提取序号(如\"01第一节\")作为主键\n",
    "    - 计算文件名相似度\n",
    "    - 相似度超过阈值的归为一组\n",
    "    \"\"\"\n",
    "\n",
    "    # 支持的文件扩展名\n",
    "    SUPPORTED_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}\n",
    "\n",
    "    # 文件名噪音模式(正则表达式)\n",
    "    NOISE_PATTERNS = [\n",
    "        r'\\[防断更.*?\\]',  # [防断更微coc36666]\n",
    "        r'\\[.*?微.*?\\]',   # [微信号xxx]\n",
    "        r'_\\d{14}',        # _20250706193405\n",
    "        r'_\\d{8}',         # _20250706\n",
    "        r'_笔记',          # _笔记\n",
    "        r'\\s*\\(.*?\\)\\s*'   # (备注)\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 knowledge_base_dir: str,\n",
    "                 similarity_threshold: float = 0.7,\n",
    "                 verbose: bool = True):\n",
    "        \"\"\"初始化知识组织器\n",
    "\n",
    "        Args:\n",
    "            knowledge_base_dir: 知识库根目录\n",
    "            similarity_threshold: 文件名相似度阈值(0-1),默认0.7\n",
    "            verbose: 是否打印详细日志\n",
    "\n",
    "        Raises:\n",
    "            ValueError: 目录不存在时抛出\n",
    "        \"\"\"\n",
    "        self.knowledge_base_dir = Path(knowledge_base_dir)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.verbose = verbose\n",
    "        if not self.knowledge_base_dir.exists():\n",
    "            raise ValueError(f\"目录不存在: {self.knowledge_base_dir}\")\n",
    "\n",
    "    def _log(self, msg):\n",
    "        if self.verbose: print(msg)\n",
    "\n",
    "    def clean_filename(self, filename: str) -> str:\n",
    "        \"\"\"清洗文件名，去除时间戳和噪音标记\n",
    "\n",
    "        Args:\n",
    "            filename: 原始文件名（含扩展名）\n",
    "\n",
    "        Returns:\n",
    "            清洗后的文件名（不含扩展名）\n",
    "\n",
    "        Example:\n",
    "            >>> clean_filename(\"01报告_20231201[防断更].pdf\")\n",
    "            \"01报告\"\n",
    "        \"\"\"\n",
    "        name = Path(filename).stem\n",
    "        for pattern in self.NOISE_PATTERNS:\n",
    "            name = re.sub(pattern, '', name)\n",
    "        return re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "    def extract_sequence_number(self, filename: str) -> Tuple[int, str]:\n",
    "        \"\"\"提取文件名开头的序号\n",
    "\n",
    "        Args:\n",
    "            filename: 文件名\n",
    "\n",
    "        Returns:\n",
    "            (序号整数, 序号字符串)，如(1, \"01\")或(999999, \"\")表示无序号\n",
    "\n",
    "        Example:\n",
    "            >>> extract_sequence_number(\"01报告\")\n",
    "            (1, \"01\")\n",
    "        \"\"\"\n",
    "        match = re.match(r'^(\\d+)', filename)\n",
    "        return (int(match.group(1)), match.group(1)) if match else (999999, \"\")\n",
    "\n",
    "    def calculate_similarity(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"计算两个字符串的相似度\n",
    "\n",
    "        Args:\n",
    "            str1: 字符串1\n",
    "            str2: 字符串2\n",
    "\n",
    "        Returns:\n",
    "            相似度分数(0-1)，1表示完全相同\n",
    "        \"\"\"\n",
    "        return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "    def get_file_priority(self, file_path: Path) -> FilePriority:\n",
    "        \"\"\"根据文件类型和名称确定优先级\n",
    "\n",
    "        Args:\n",
    "            file_path: 文件路径对象\n",
    "\n",
    "        Returns:\n",
    "            FilePriority枚举值\n",
    "\n",
    "        Note:\n",
    "            优先级：PDF笔记 > Word > PDF > PPT > 未知\n",
    "        \"\"\"\n",
    "        name, suffix = file_path.name.lower(), file_path.suffix.lower()\n",
    "        if '笔记' in name and suffix == '.pdf': return FilePriority.PDF_NOTE\n",
    "        if suffix in ['.doc', '.docx']: return FilePriority.WORD_DOC\n",
    "        if suffix == '.pdf': return FilePriority.PDF_REGULAR\n",
    "        if suffix in ['.ppt', '.pptx']: return FilePriority.POWERPOINT\n",
    "        return FilePriority.UNKNOWN\n",
    "\n",
    "    def create_file_info(self, file_path: Path) -> FileInfo:\n",
    "        \"\"\"为单个文件创建信息对象\n",
    "\n",
    "        Args:\n",
    "            file_path: 文件路径\n",
    "\n",
    "        Returns:\n",
    "            FileInfo对象，包含原始名、清洗名、序号、优先级等\n",
    "        \"\"\"\n",
    "        original_name = file_path.name\n",
    "        cleaned_name = self.clean_filename(original_name)\n",
    "        sequence, sequence_str = self.extract_sequence_number(cleaned_name)\n",
    "        priority = self.get_file_priority(file_path)\n",
    "        return FileInfo(file_path, original_name, cleaned_name, sequence, sequence_str, priority)\n",
    "\n",
    "    def group_files_by_similarity(self, files: List[FileInfo]) -> Dict[str, KnowledgeGroup]:\n",
    "        \"\"\"按序号和相似度智能分组文件\n",
    "\n",
    "        Args:\n",
    "            files: FileInfo对象列表\n",
    "\n",
    "        Returns:\n",
    "            字典{group_key: KnowledgeGroup}，每组包含相似的文件\n",
    "\n",
    "        Note:\n",
    "            同序号且相似度>=threshold的文件归为一组\n",
    "        \"\"\"\n",
    "        groups, processed = {}, set()\n",
    "        for i, file1 in enumerate(files):\n",
    "            if file1.path in processed: continue\n",
    "            group_key = f\"{file1.sequence_str}_{file1.cleaned_name[:20]}\"\n",
    "            group_files = [file1]\n",
    "            processed.add(file1.path)\n",
    "\n",
    "            for file2 in files[i+1:]:\n",
    "                if file2.path in processed: continue\n",
    "                if file1.sequence == file2.sequence:\n",
    "                    if self.calculate_similarity(file1.cleaned_name, file2.cleaned_name) >= self.similarity_threshold:\n",
    "                        group_files.append(file2)\n",
    "                        processed.add(file2.path)\n",
    "\n",
    "            group_files.sort(key=lambda f: (f.priority.value, f.original_name))\n",
    "            groups[group_key] = KnowledgeGroup(group_key, file1.cleaned_name, file1.sequence,\n",
    "                                              group_files, group_files[0], [f.path.suffix for f in group_files])\n",
    "            self._log(f\"[完成] {file1.cleaned_name[:30]} ({len(group_files)}文件)\")\n",
    "        return groups\n",
    "\n",
    "    def scan_and_organize(self) -> Dict[str, Dict[str, KnowledgeGroup]]:\n",
    "        \"\"\"扫描知识库目录并智能分组\n",
    "\n",
    "        Returns:\n",
    "            字典{domain: {group_key: KnowledgeGroup}}\n",
    "\n",
    "        Example:\n",
    "            >>> result = organizer.scan_and_organize()\n",
    "            >>> # {'macro_economy': {'01_中国经济': KnowledgeGroup(...)}}\n",
    "        \"\"\"\n",
    "        self._log(f\"扫描目录: {self.knowledge_base_dir}\")\n",
    "        all_files = []\n",
    "        for ext in self.SUPPORTED_EXTENSIONS:\n",
    "            all_files.extend(self.knowledge_base_dir.glob(f\"*{ext}\"))\n",
    "\n",
    "        if not all_files:\n",
    "            self._log(\"[警告] 未找到文件\")\n",
    "            return {}\n",
    "\n",
    "        self._log(f\"找到 {len(all_files)} 个文件\")\n",
    "        file_infos = [self.create_file_info(f) for f in all_files]\n",
    "        groups = self.group_files_by_similarity(file_infos)\n",
    "        sorted_groups = dict(sorted(groups.items(), key=lambda x: x[1].sequence))\n",
    "        self._log(f\"[完成] {len(sorted_groups)} 个知识块\\n\")\n",
    "        return {self.knowledge_base_dir.name: sorted_groups}\n",
    "\n",
    "print(\"步骤1完成：KnowledgeOrganizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2: DocumentLoader - 文档加载清洗\n",
    "\n",
    "**作用**：加载PDF/Word/PPT文件，清洗文本内容\n",
    "\n",
    "**为什么**：不同格式需要不同加载器，清洗去除噪音提高质量\n",
    "\n",
    "**关键步骤**：选择加载器 → 加载文档 → 清洗文本（去除特殊字符、多余空白）\n",
    "\n",
    "**输出**：LangChain Document列表（每页一个Document）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤2完成：DocumentLoader\n"
     ]
    }
   ],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"文档加载器\n",
    "\n",
    "    支持多种文档格式加载:\n",
    "    - PDF: PyMuPDFLoader (推荐,性能最佳)\n",
    "    - Word: Docx2txtLoader (.doc, .docx)\n",
    "    - PowerPoint: UnstructuredPowerPointLoader (.ppt, .pptx)\n",
    "    \"\"\"\n",
    "\n",
    "    def load_pdf(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"加载PDF文件\n",
    "\n",
    "        Args:\n",
    "            file_path: PDF文件路径\n",
    "\n",
    "        Returns:\n",
    "            文档列表(每页一个Document)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(file_path))\n",
    "            return loader.load()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF加载失败 {file_path.name}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def load_word(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"加载Word文件\n",
    "\n",
    "        Args:\n",
    "            file_path: Word文件路径\n",
    "\n",
    "        Returns:\n",
    "            文档列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loader = Docx2txtLoader(str(file_path))\n",
    "            return loader.load()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Word加载失败 {file_path.name}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def load_ppt(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"加载PowerPoint文件\n",
    "\n",
    "        Args:\n",
    "            file_path: PPT文件路径\n",
    "\n",
    "        Returns:\n",
    "            文档列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loader = UnstructuredPowerPointLoader(str(file_path))\n",
    "            return loader.load()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PPT加载失败 {file_path.name}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clean_document_text(self, doc: Document) -> Document:\n",
    "        \"\"\"清洗文档文本\n",
    "\n",
    "        清理内容:\n",
    "        - 特殊字符\n",
    "        - 多余空白\n",
    "        - 噪音内容\n",
    "\n",
    "        Args:\n",
    "            doc: 原始文档\n",
    "\n",
    "        Returns:\n",
    "            清洗后的文档\n",
    "        \"\"\"\n",
    "        text = doc.page_content\n",
    "        text = re.sub(r'[\\uf06c\\uf0fc]', '', text)  # 特殊字符\n",
    "        text = re.sub(r'\\s+', ' ', text)  # 多余空白\n",
    "        doc.page_content = text.strip()\n",
    "        return doc\n",
    "\n",
    "    def load_and_clean(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"加载并清洗文档(统一入口)\n",
    "\n",
    "        Args:\n",
    "            file_path: 文件路径\n",
    "\n",
    "        Returns:\n",
    "            清洗后的文档列表\n",
    "        \"\"\"\n",
    "        suffix = file_path.suffix.lower()\n",
    "\n",
    "        # 根据文件类型选择加载器\n",
    "        if suffix == '.pdf':\n",
    "            docs = self.load_pdf(file_path)\n",
    "        elif suffix in ['.doc', '.docx']:\n",
    "            docs = self.load_word(file_path)\n",
    "        elif suffix in ['.ppt', '.pptx']:\n",
    "            docs = self.load_ppt(file_path)\n",
    "        else:\n",
    "            logger.warning(f\"不支持的文件类型: {suffix}\")\n",
    "            return []\n",
    "\n",
    "        # 清洗文档\n",
    "        if docs:\n",
    "            docs = [self.clean_document_text(doc) for doc in docs]\n",
    "            logger.info(f\"加载 {file_path.name}: {len(docs)}页\")\n",
    "\n",
    "        return docs\n",
    "\n",
    "print(\"步骤2完成：DocumentLoader\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3: KnowledgeExtractor - LLM结构化提取\n",
    "\n",
    "**作用**：使用deepseek-reasoner提取结构化知识（关键概念、指标、摘要）\n",
    "\n",
    "**为什么**：LLM能理解语义，将非结构化文档转为结构化JSON，便于查询\n",
    "\n",
    "**为什么JSON结构这样设计**：\n",
    "- `topic`：主题名称，便于识别和分类\n",
    "- `key_concepts`：核心概念（名称+定义+重要性），便于快速理解\n",
    "- `indicators`：关键指标（名称+计算+解读），便于数据分析\n",
    "- `analysis_methods`：分析方法（名称+步骤+应用），便于实际应用\n",
    "- `summary`：总结，便于快速了解全貌\n",
    "\n",
    "**关键步骤**：构建prompt → 调用LLM → 解析JSON → 保存文件\n",
    "\n",
    "**输出**：JSON格式的结构化知识文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤3完成：KnowledgeExtractor\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeExtractor:\n",
    "    \"\"\"LLM知识提取器\n",
    "\n",
    "    使用LLM从文档中提取结构化知识(JSON格式)\n",
    "    技术决策: 使用deepseek-v3模型进行知识提取\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = LLM_MODEL, temperature: float = LLM_TEMPERATURE):\n",
    "        \"\"\"初始化知识提取器\n",
    "\n",
    "        Args:\n",
    "            model_name: LLM模型名称,默认使用全局配置\n",
    "            temperature: 温度参数,默认0确保输出稳定性\n",
    "        \"\"\"\n",
    "        self.llm = ChatDeepSeek(model=model_name, temperature=temperature)\n",
    "        logger.info(f\"LLM初始化: {model_name}\")\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "你是金融知识提取专家。从文档提取结构化知识,返回JSON。\n",
    "\n",
    "文档: {content}\n",
    "\n",
    "提取JSON(只返回JSON):\n",
    "{{\n",
    "  \"topic\": \"主题\",\n",
    "  \"key_concepts\": [{{\"name\": \"概念\", \"definition\": \"定义\", \"importance\": \"重要性\"}}],\n",
    "  \"indicators\": [{{\"name\": \"指标\", \"calculation\": \"计算\", \"interpretation\": \"解读\"}}],\n",
    "  \"analysis_methods\": [{{\"name\": \"方法\", \"steps\": \"步骤\", \"application\": \"应用\"}}],\n",
    "  \"summary\": \"总结\"\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "    def extract_from_documents(self, docs: List[Document], topic: str) -> Dict:\n",
    "        \"\"\"从文档列表提取结构化知识\n",
    "\n",
    "        Args:\n",
    "            docs: LangChain Document列表\n",
    "            topic: 知识主题\n",
    "\n",
    "        Returns:\n",
    "            结构化知识字典，包含topic/key_concepts/indicators/analysis_methods/summary\n",
    "\n",
    "        Note:\n",
    "            - 只使用前5页内容（避免token超限）\n",
    "            - 截断到15000字符\n",
    "            - 失败时返回空结构\n",
    "\n",
    "        Example:\n",
    "            >>> knowledge = extractor.extract_from_documents(docs, \"GDP分析\")\n",
    "            >>> knowledge['topic']\n",
    "            'GDP分析'\n",
    "        \"\"\"\n",
    "        content = \"\\n\\n\".join([d.page_content for d in docs[:5]])[:15000]\n",
    "        try:\n",
    "            chain = self.prompt | self.llm | JsonOutputParser()\n",
    "            result = chain.invoke({\"content\": content})\n",
    "            result[\"topic\"] = topic\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"[警告] LLM提取失败: {e}\")\n",
    "            return {\"topic\": topic, \"key_concepts\": [], \"indicators\": [],\n",
    "                   \"analysis_methods\": [], \"summary\": \"提取失败\"}\n",
    "\n",
    "print(\"步骤3完成：KnowledgeExtractor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4: VectorStoreManager - 向量化存储\n",
    "\n",
    "**作用**：将文档分块并向量化，存储到Chroma向量库\n",
    "\n",
    "**为什么**：向量化支持语义检索，分块避免token超限，Chroma轻量级易用\n",
    "\n",
    "**关键步骤**：文档分块 → 向量化 → 添加元数据 → 存储到Chroma\n",
    "\n",
    "**输出**：Chroma向量数据库（支持语义检索）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤4完成：VectorStoreManager\n"
     ]
    }
   ],
   "source": [
    "class VectorStoreManager:\n",
    "    \"\"\"向量存储管理器\n",
    "\n",
    "    负责文档向量化和Chroma向量数据库管理\n",
    "    技术决策: 使用Qwen3-Embedding-0.6B模型进行向量化\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_model: str = EMBEDDING_MODEL,\n",
    "                 persist_directory: str = str(VECTOR_DB_DIR)):\n",
    "        \"\"\"初始化向量存储管理器\n",
    "\n",
    "        Args:\n",
    "            embedding_model: Embedding模型名称,默认使用全局配置\n",
    "            persist_directory: 向量数据库持久化目录\n",
    "        \"\"\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        self.persist_directory = Path(persist_directory)\n",
    "        logger.info(f\"Embedding初始化: {embedding_model}\")\n",
    "        self.vector_stores = {}\n",
    "\n",
    "    def get_or_create_store(self, domain: str) -> Chroma:\n",
    "        \"\"\"获取已有向量存储或创建新的\n",
    "\n",
    "        Args:\n",
    "            domain: 领域名称（用作collection命名）\n",
    "\n",
    "        Returns:\n",
    "            Chroma向量存储对象\n",
    "\n",
    "        Note:\n",
    "            每个domain对应一个独立的collection\n",
    "        \"\"\"\n",
    "        if domain in self.vector_stores:\n",
    "            return self.vector_stores[domain]\n",
    "\n",
    "        persist_path = str(self.persist_directory / domain)\n",
    "        store = Chroma(\n",
    "            collection_name=f\"{domain}_col\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=persist_path\n",
    "        )\n",
    "        self.vector_stores[domain] = store\n",
    "        return store\n",
    "\n",
    "    def split_documents(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"将长文档分割为小chunks\n",
    "\n",
    "        Args:\n",
    "            docs: Document列表\n",
    "\n",
    "        Returns:\n",
    "            分割后的Document列表\n",
    "\n",
    "        Note:\n",
    "            chunk_size=1000, overlap=200（按全局配置）\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "        )\n",
    "        return splitter.split_documents(docs)\n",
    "\n",
    "    def add_documents(self, domain: str, docs: List[Document], metadata: Dict = None):\n",
    "        \"\"\"向量化文档并添加到存储\n",
    "\n",
    "        Args:\n",
    "            domain: 领域名称\n",
    "            docs: Document列表\n",
    "            metadata: 附加到每个chunk的元数据（可选）\n",
    "\n",
    "        Note:\n",
    "            - 自动分割文档为chunks\n",
    "            - 附加metadata到每个chunk\n",
    "            - 持久化到Chroma\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            print(\"  [警告] 无文档,跳过向量化\")\n",
    "            return\n",
    "\n",
    "        chunks = self.split_documents(docs)\n",
    "        if metadata:\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update(metadata)\n",
    "\n",
    "        store = self.get_or_create_store(domain)\n",
    "        store.add_documents(chunks)\n",
    "        print(f\"  -> 向量化: {len(chunks)} chunks\")\n",
    "\n",
    "print(\"步骤4完成：VectorStoreManager\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5: KnowledgeProcessor - 完整Pipeline协调器\n",
    "\n",
    "**作用**：协调所有模块，执行完整的知识处理流程\n",
    "\n",
    "**为什么**：统一入口管理整个流程，确保各模块按顺序执行\n",
    "\n",
    "**关键步骤**：扫描分组 → 加载文档 → 提取知识 → 向量化存储 → 保存JSON\n",
    "\n",
    "**输出**：结构化JSON + 向量数据库（双存储）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "步骤5完成：KnowledgeProcessor\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeProcessor:\n",
    "    \"\"\"知识处理Pipeline协调器\n",
    "\n",
    "    整合5个核心模块,执行完整的知识处理流程:\n",
    "    1. 文件扫描分组 (KnowledgeOrganizer)\n",
    "    2. 文档加载 (DocumentLoader)\n",
    "    3. 知识提取 (KnowledgeExtractor)\n",
    "    4. 向量化存储 (VectorStoreManager)\n",
    "    5. JSON存储\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 knowledge_base_dir: str,\n",
    "                 memories_dir: str = str(STRUCTURED_JSON_DIR),\n",
    "                 vector_db_dir: str = str(VECTOR_DB_DIR)):\n",
    "        \"\"\"初始化Pipeline协调器\n",
    "\n",
    "        Args:\n",
    "            knowledge_base_dir: 知识库根目录\n",
    "            memories_dir: JSON结构化知识存储目录\n",
    "            vector_db_dir: 向量数据库存储目录\n",
    "        \"\"\"\n",
    "        self.organizer = KnowledgeOrganizer(knowledge_base_dir)\n",
    "        self.loader = DocumentLoader()\n",
    "        self.extractor = KnowledgeExtractor()\n",
    "        self.vector_manager = VectorStoreManager(persist_directory=vector_db_dir)\n",
    "        self.memories_dir = Path(memories_dir)\n",
    "        logger.info(\"Pipeline协调器初始化完成\")\n",
    "\n",
    "    def save_to_memories(self, domain: str, group_key: str, knowledge: Dict):\n",
    "        \"\"\"将结构化知识保存为JSON文件\n",
    "\n",
    "        Args:\n",
    "            domain: 领域名称（用作子目录）\n",
    "            group_key: 知识组唯一键\n",
    "            knowledge: 结构化知识字典\n",
    "\n",
    "        Note:\n",
    "            保存路径: memories_dir/domain/group_key.json\n",
    "        \"\"\"\n",
    "        domain_dir = self.memories_dir / domain\n",
    "        domain_dir.mkdir(parents=True, exist_ok=True)\n",
    "        json_file = domain_dir / f\"{group_key}.json\"\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(knowledge, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  -> JSON: {json_file.name}\")\n",
    "\n",
    "    def process_all(self, limit: int = None):\n",
    "        \"\"\"执行完整Pipeline处理所有知识文件\n",
    "\n",
    "        Args:\n",
    "            limit: 限制处理数量（可选，用于测试）\n",
    "\n",
    "        Note:\n",
    "            处理流程：\n",
    "            1. 扫描分组（KnowledgeOrganizer）\n",
    "            2. 加载清洗（DocumentLoader）\n",
    "            3. 知识提取（KnowledgeExtractor）\n",
    "            4. 向量化存储（VectorStoreManager）\n",
    "            5. JSON保存\n",
    "\n",
    "        Example:\n",
    "            >>> processor.process_all(limit=2)  # 测试：只处理前2个\n",
    "            >>> processor.process_all()  # 正式：处理所有\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"开始完整Pipeline\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        organized = self.organizer.scan_and_organize()\n",
    "\n",
    "        for domain, groups in organized.items():\n",
    "            total = len(groups) if not limit else min(limit, len(groups))\n",
    "            print(f\"\\n领域: {domain} (共{total}个知识块)\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            count = 0\n",
    "            for group_key, group in groups.items():\n",
    "                if limit and count >= limit:\n",
    "                    print(f\"\\n达到限制({limit}),停止\")\n",
    "                    break\n",
    "\n",
    "                # 实时进度显示\n",
    "                progress = (count + 1) / total * 100\n",
    "                print(f\"\\n[{count+1}/{total}] 进度: {progress:.1f}% | {group.topic}\")\n",
    "\n",
    "                # 加载所有文件\n",
    "                all_docs = []\n",
    "                for file_info in group.files:\n",
    "                    docs = self.loader.load_and_clean(file_info.path)\n",
    "                    if docs:\n",
    "                        all_docs.extend(docs)\n",
    "                        print(f\"  -> 加载 {file_info.path.suffix}: {len(docs)} 页\")\n",
    "\n",
    "                if not all_docs:\n",
    "                    print(\"  [警告] 所有文件加载失败\")\n",
    "                    continue\n",
    "                print(f\"  -> 总计: {len(all_docs)} 页\")\n",
    "\n",
    "                # 提取\n",
    "                knowledge = self.extractor.extract_from_documents(all_docs, group.topic)\n",
    "                self.save_to_memories(domain, group_key, knowledge)\n",
    "\n",
    "                # 向量化\n",
    "                self.vector_manager.add_documents(domain, all_docs,\n",
    "                    {\"domain\": domain, \"topic\": group.topic, \"seq\": group.sequence})\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            print(f\"\\n[完成] {domain}: {count}/{total} 个\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Pipeline完成!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\n输出目录:\")\n",
    "        print(f\"  - JSON: {self.memories_dir}\")\n",
    "        print(f\"  - 向量库: {self.vector_manager.persist_directory}\")\n",
    "\n",
    "print(\"步骤5完成：KnowledgeProcessor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:46:06,673 - INFO - LLM初始化: deepseek-reasoner\n",
      "2025-12-03 22:46:09,905 - INFO - Use pytorch device_name: cpu\n",
      "2025-12-03 22:46:09,906 - INFO - Load pretrained SentenceTransformer: Qwen/Qwen3-Embedding-0.6B\n",
      "2025-12-03 22:46:36,104 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-12-03 22:46:36,105 - INFO - Embedding初始化: Qwen/Qwen3-Embedding-0.6B\n",
      "2025-12-03 22:46:36,106 - INFO - Pipeline协调器初始化完成\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline初始化完成\n",
      "知识库: ../knowledge_base\n",
      "JSON输出: output/structured_knowledge\n",
      "向量库: output/vector_db\n"
     ]
    }
   ],
   "source": [
    "# ========== Pipeline测试 ==========\n",
    "\n",
    "# 初始化Processor(使用全局配置)\n",
    "processor = KnowledgeProcessor(\n",
    "    knowledge_base_dir=str(KNOWLEDGE_BASE_DIR)\n",
    ")\n",
    "\n",
    "print(\"Pipeline初始化完成\")\n",
    "print(f\"知识库: {KNOWLEDGE_BASE_DIR}\")\n",
    "print(f\"JSON输出: {STRUCTURED_JSON_DIR}\")\n",
    "print(f\"向量库: {VECTOR_DB_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:46:36,303 - INFO - 加载 01第一节 中国经济的“三驾马车”[防断更微coc36666]_笔记.pdf: 4页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "开始完整Pipeline\n",
      "================================================================================\n",
      "\n",
      "扫描目录: ../knowledge_base\n",
      "找到 51 个文件\n",
      "[完成] 14.第十四节 汇率投资手册 (3文件)\n",
      "[完成] 07第七节 物价——快速入门读懂经 (3文件)\n",
      "[完成] 11.第十一节 基金投资手册 (3文件)\n",
      "[完成] 12.第十二节 保险投资手册 (3文件)\n",
      "[完成] 08第八节 如何读懂经济周期 (3文件)\n",
      "[完成] 15.第十五节 大宗商品投资手册 (3文件)\n",
      "[完成] 05第五节 PMI——快速入门读懂经济形势 (3文件)\n",
      "[完成] 06第六节 金融——快速入门读懂经济形势 (3文件)\n",
      "[完成] 17.第十七节 格雷厄姆：华尔街教父 (3文件)\n",
      "[完成] 13.第十三节 黄金投资手册 (3文件)\n",
      "[完成] 03第三节 投资——快速入门读懂经济形势 (3文件)\n",
      "[完成] 04第四节 出口——快速入门读懂经济形势 (3文件)\n",
      "[完成] 09第九节 看懂投资时钟，踩准投资节奏 (3文件)\n",
      "[完成] 16.第十六节 房地产投资手册 (3文件)\n",
      "[完成] 01第一节 中国经济的“三驾马车” (3文件)\n",
      "[完成] 02第二节 消费——快速入门读懂经济形势 (3文件)\n",
      "[完成] 10第十节 股市投资手册 (3文件)\n",
      "[完成] 17 个知识块\n",
      "\n",
      "\n",
      "领域: knowledge_base (共2个知识块)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1/2] 进度: 50.0% | 01第一节 中国经济的“三驾马车”\n",
      "  -> 加载 .pdf: 4 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:46:36,325 - INFO - 加载 01第一节 中国经济的“三驾马车”[防断更微coc36666].doc: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 加载 .doc: 1 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:47:05,388 - INFO - 加载 01第一节 中国经济的“三驾马车”[防断更微coc36666]_20250706193405.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:47:05,608 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-03 22:48:22,377 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 01_01第一节 中国经济的“三驾马车”.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:49:03,338 - INFO - 加载 02第二节 消费——快速入门读懂经济形势[防断更微coc36666]_笔记.pdf: 3页\n",
      "2025-12-03 22:49:03,343 - INFO - 加载 02第二节 消费——快速入门读懂经济形势[防断更微coc36666].doc: 1页\n",
      "2025-12-03 22:49:03,426 - INFO - 加载 02第二节 消费——快速入门读懂经济形势[防断更微coc36666]_20250707140225.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 8 chunks\n",
      "\n",
      "[2/2] 进度: 100.0% | 02第二节 消费——快速入门读懂经济形势\n",
      "  -> 加载 .pdf: 3 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 5 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:49:03,609 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 02_02第二节 消费——快速入门读懂经济形势.json\n",
      "  -> 向量化: 6 chunks\n",
      "\n",
      "达到限制(2),停止\n",
      "\n",
      "[完成] knowledge_base: 2/2 个\n",
      "\n",
      "================================================================================\n",
      "Pipeline完成!\n",
      "================================================================================\n",
      "\n",
      "输出目录:\n",
      "  - JSON: output/structured_knowledge\n",
      "  - 向量库: output/vector_db\n"
     ]
    }
   ],
   "source": [
    "# 测试: 处理前2个知识块\n",
    "processor.process_all(limit=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:55:16,097 - INFO - 加载 01第一节 中国经济的“三驾马车”[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 22:55:16,102 - INFO - 加载 01第一节 中国经济的“三驾马车”[防断更微coc36666].doc: 1页\n",
      "2025-12-03 22:55:16,166 - INFO - 加载 01第一节 中国经济的“三驾马车”[防断更微coc36666]_20250706193405.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "开始完整Pipeline\n",
      "================================================================================\n",
      "\n",
      "扫描目录: ../knowledge_base\n",
      "找到 51 个文件\n",
      "[完成] 14.第十四节 汇率投资手册 (3文件)\n",
      "[完成] 07第七节 物价——快速入门读懂经 (3文件)\n",
      "[完成] 11.第十一节 基金投资手册 (3文件)\n",
      "[完成] 12.第十二节 保险投资手册 (3文件)\n",
      "[完成] 08第八节 如何读懂经济周期 (3文件)\n",
      "[完成] 15.第十五节 大宗商品投资手册 (3文件)\n",
      "[完成] 05第五节 PMI——快速入门读懂经济形势 (3文件)\n",
      "[完成] 06第六节 金融——快速入门读懂经济形势 (3文件)\n",
      "[完成] 17.第十七节 格雷厄姆：华尔街教父 (3文件)\n",
      "[完成] 13.第十三节 黄金投资手册 (3文件)\n",
      "[完成] 03第三节 投资——快速入门读懂经济形势 (3文件)\n",
      "[完成] 04第四节 出口——快速入门读懂经济形势 (3文件)\n",
      "[完成] 09第九节 看懂投资时钟，踩准投资节奏 (3文件)\n",
      "[完成] 16.第十六节 房地产投资手册 (3文件)\n",
      "[完成] 01第一节 中国经济的“三驾马车” (3文件)\n",
      "[完成] 02第二节 消费——快速入门读懂经济形势 (3文件)\n",
      "[完成] 10第十节 股市投资手册 (3文件)\n",
      "[完成] 17 个知识块\n",
      "\n",
      "\n",
      "领域: knowledge_base (共17个知识块)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1/17] 进度: 5.9% | 01第一节 中国经济的“三驾马车”\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:55:16,846 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 01_01第一节 中国经济的“三驾马车”.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:57:16,315 - INFO - 加载 02第二节 消费——快速入门读懂经济形势[防断更微coc36666]_笔记.pdf: 3页\n",
      "2025-12-03 22:57:16,321 - INFO - 加载 02第二节 消费——快速入门读懂经济形势[防断更微coc36666].doc: 1页\n",
      "2025-12-03 22:57:16,415 - INFO - 加载 02第二节 消费——快速入门读懂经济形势[防断更微coc36666]_20250707140225.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 8 chunks\n",
      "\n",
      "[2/17] 进度: 11.8% | 02第二节 消费——快速入门读懂经济形势\n",
      "  -> 加载 .pdf: 3 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 5 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:57:16,602 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 02_02第二节 消费——快速入门读懂经济形势.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:59:08,519 - INFO - 加载 03第三节 投资——快速入门读懂经济形势[防断更微coc36666]_笔记.pdf: 3页\n",
      "2025-12-03 22:59:08,523 - INFO - 加载 03第三节 投资——快速入门读懂经济形势[防断更微coc36666].doc: 1页\n",
      "2025-12-03 22:59:08,597 - INFO - 加载 03第三节 投资——快速入门读懂经济形势[防断更微coc36666]_20250707140243.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 6 chunks\n",
      "\n",
      "[3/17] 进度: 17.6% | 03第三节 投资——快速入门读懂经济形势\n",
      "  -> 加载 .pdf: 3 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 5 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:59:08,784 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 03_03第三节 投资——快速入门读懂经济形势.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:01:16,339 - INFO - 加载 04第四节 出口——快速入门读懂经济形势[防断更微coc36666]_笔记.pdf: 3页\n",
      "2025-12-03 23:01:16,343 - INFO - 加载 04第四节 出口——快速入门读懂经济形势[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:01:16,397 - INFO - 加载 04第四节 出口——快速入门读懂经济形势[防断更微coc36666]_20250707140251.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 7 chunks\n",
      "\n",
      "[4/17] 进度: 23.5% | 04第四节 出口——快速入门读懂经济形势\n",
      "  -> 加载 .pdf: 3 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 5 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:01:16,842 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 04_04第四节 出口——快速入门读懂经济形势.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:03:15,682 - INFO - 加载 05第五节 PMI——快速入门读懂经济形势[防断更微coc36666]_笔记.pdf: 5页\n",
      "2025-12-03 23:03:15,686 - INFO - 加载 05第五节 PMI——快速入门读懂经济形势[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:03:15,742 - INFO - 加载 05第五节 PMI——快速入门读懂经济形势[防断更微coc36666]_20251130222243.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 6 chunks\n",
      "\n",
      "[5/17] 进度: 29.4% | 05第五节 PMI——快速入门读懂经济形势\n",
      "  -> 加载 .pdf: 5 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 7 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:03:16,194 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 05_05第五节 PMI——快速入门读懂经济形.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:04:48,150 - INFO - 加载 06第六节 金融——快速入门读懂经济形势[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:04:48,154 - INFO - 加载 06第六节 金融——快速入门读懂经济形势[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:04:48,213 - INFO - 加载 06第六节 金融——快速入门读懂经济形势[防断更微coc36666]_20251130222414.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 8 chunks\n",
      "\n",
      "[6/17] 进度: 35.3% | 06第六节 金融——快速入门读懂经济形势\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:04:48,412 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 06_06第六节 金融——快速入门读懂经济形势.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:06:22,493 - INFO - 加载 07第七节 物价——快速入门读懂经[防断更微coc36666]_笔记.pdf: 3页\n",
      "2025-12-03 23:06:22,498 - INFO - 加载 07第七节 物价——快速入门读懂经[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:06:22,554 - INFO - 加载 07第七节 物价——快速入门读懂经[防断更微coc36666]_20251130222700.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 8 chunks\n",
      "\n",
      "[7/17] 进度: 41.2% | 07第七节 物价——快速入门读懂经\n",
      "  -> 加载 .pdf: 3 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 5 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:06:22,830 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 07_07第七节 物价——快速入门读懂经.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:07:26,119 - INFO - 加载 08第八节 如何读懂经济周期[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:07:26,123 - INFO - 加载 08第八节 如何读懂经济周期[防断更微coc36666]_20251130_222945.doc: 1页\n",
      "2025-12-03 23:07:26,172 - INFO - 加载 08第八节 如何读懂经济周期[防断更微coc36666]_20251130222832.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 7 chunks\n",
      "\n",
      "[8/17] 进度: 47.1% | 08第八节 如何读懂经济周期\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:07:26,396 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-03 23:20:23,276 - INFO - Retrying request to /chat/completions in 0.404910 seconds\n",
      "2025-12-03 23:20:23,913 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 08_08第八节 如何读懂经济周期.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:22:10,517 - INFO - 加载 09第九节 看懂投资时钟，踩准投资节奏[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:22:10,522 - INFO - 加载 09第九节 看懂投资时钟，踩准投资节奏[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:22:10,592 - INFO - 加载 09第九节 看懂投资时钟，踩准投资节奏[防断更微coc36666]_20251130223006.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 7 chunks\n",
      "\n",
      "[9/17] 进度: 52.9% | 09第九节 看懂投资时钟，踩准投资节奏\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:22:11,118 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 09_09第九节 看懂投资时钟，踩准投资节奏.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:23:44,529 - INFO - 加载 10第十节 股市投资手册[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:23:44,534 - INFO - 加载 10第十节 股市投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:23:44,591 - INFO - 加载 10第十节 股市投资手册[防断更微coc36666]_20251130223842.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 10 chunks\n",
      "\n",
      "[10/17] 进度: 58.8% | 10第十节 股市投资手册\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:23:44,818 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 10_10第十节 股市投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:25:13,505 - INFO - 加载 11.第十一节 基金投资手册[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:25:13,510 - INFO - 加载 11.第十一节 基金投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:25:13,551 - INFO - 加载 11.第十一节 基金投资手册[防断更微coc36666]_20251130223902.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 9 chunks\n",
      "\n",
      "[11/17] 进度: 64.7% | 11.第十一节 基金投资手册\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:25:13,710 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 11_11.第十一节 基金投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:26:21,118 - INFO - 加载 12.第十二节 保险投资手册[防断更微coc36666]_笔记.pdf: 5页\n",
      "2025-12-03 23:26:21,121 - INFO - 加载 12.第十二节 保险投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:26:21,158 - INFO - 加载 12.第十二节 保险投资手册[防断更微coc36666]_20251130223935.pptx: 1页\n",
      "2025-12-03 23:26:21,257 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 6 chunks\n",
      "\n",
      "[12/17] 进度: 70.6% | 12.第十二节 保险投资手册\n",
      "  -> 加载 .pdf: 5 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 7 页\n",
      "  -> JSON: 12_12.第十二节 保险投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:27:41,296 - INFO - 加载 13.第十三节 黄金投资手册[防断更微coc36666]_笔记.pdf: 1页\n",
      "2025-12-03 23:27:41,301 - INFO - 加载 13.第十三节 黄金投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:27:41,367 - INFO - 加载 13.第十三节 黄金投资手册[防断更微coc36666]_20251130224009.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 7 chunks\n",
      "\n",
      "[13/17] 进度: 76.5% | 13.第十三节 黄金投资手册\n",
      "  -> 加载 .pdf: 1 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 3 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:27:41,821 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 13_13.第十三节 黄金投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:29:13,121 - INFO - 加载 14.第十四节 汇率投资手册[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:29:13,127 - INFO - 加载 14.第十四节 汇率投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:29:13,161 - INFO - 加载 14.第十四节 汇率投资手册[防断更微coc36666]_20251130224038.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 5 chunks\n",
      "\n",
      "[14/17] 进度: 82.4% | 14.第十四节 汇率投资手册\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:29:13,334 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 14_14.第十四节 汇率投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:31:06,151 - INFO - 加载 15.第十五节 大宗商品投资手册[防断更微coc36666]_笔记.pdf: 4页\n",
      "2025-12-03 23:31:06,156 - INFO - 加载 15.第十五节 大宗商品投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:31:06,205 - INFO - 加载 15.第十五节 大宗商品投资手册[防断更微coc36666]_20251130224057.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 6 chunks\n",
      "\n",
      "[15/17] 进度: 88.2% | 15.第十五节 大宗商品投资手册\n",
      "  -> 加载 .pdf: 4 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 6 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:31:06,417 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 15_15.第十五节 大宗商品投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:32:56,482 - INFO - 加载 16.第十六节 房地产投资手册[防断更微coc36666]_笔记.pdf: 5页\n",
      "2025-12-03 23:32:56,487 - INFO - 加载 16.第十六节 房地产投资手册[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:32:56,545 - INFO - 加载 16.第十六节 房地产投资手册[防断更微coc36666]_20251130224121.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 8 chunks\n",
      "\n",
      "[16/17] 进度: 94.1% | 16.第十六节 房地产投资手册\n",
      "  -> 加载 .pdf: 5 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 7 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:32:56,737 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 16_16.第十六节 房地产投资手册.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:34:18,681 - INFO - 加载 17.第十七节 格雷厄姆：华尔街教父[防断更微coc36666]_笔记.pdf: 3页\n",
      "2025-12-03 23:34:18,685 - INFO - 加载 17.第十七节 格雷厄姆：华尔街教父[防断更微coc36666].doc: 1页\n",
      "2025-12-03 23:34:18,713 - INFO - 加载 17.第十七节 格雷厄姆：华尔街教父[防断更微coc36666]_20251130224150.pptx: 1页\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 向量化: 10 chunks\n",
      "\n",
      "[17/17] 进度: 100.0% | 17.第十七节 格雷厄姆：华尔街教父\n",
      "  -> 加载 .pdf: 3 页\n",
      "  -> 加载 .doc: 1 页\n",
      "  -> 加载 .pptx: 1 页\n",
      "  -> 总计: 5 页\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:34:19,053 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> JSON: 17_17.第十七节 格雷厄姆：华尔街教父.json\n",
      "  -> 向量化: 5 chunks\n",
      "\n",
      "[完成] knowledge_base: 17/17 个\n",
      "\n",
      "================================================================================\n",
      "Pipeline完成!\n",
      "================================================================================\n",
      "\n",
      "输出目录:\n",
      "  - JSON: output/structured_knowledge\n",
      "  - 向量库: output/vector_db\n"
     ]
    }
   ],
   "source": [
    "# 正式运行: 处理所有知识块(取消注释)\n",
    "processor.process_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出结构\n",
    "\n",
    "### JSON结构\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"topic\": \"主题名称\",\n",
    "  \"key_concepts\": [\n",
    "    {\"name\": \"概念名\", \"definition\": \"定义\", \"importance\": \"重要性\"}\n",
    "  ],\n",
    "  \"indicators\": [\n",
    "    {\"name\": \"指标名\", \"calculation\": \"计算方法\", \"interpretation\": \"解读\"}\n",
    "  ],\n",
    "  \"analysis_methods\": [\n",
    "    {\"name\": \"方法名\", \"steps\": \"步骤\", \"application\": \"应用\"}\n",
    "  ],\n",
    "  \"summary\": \"总结\"\n",
    "}\n",
    "```\n",
    "\n",
    "**存储位置**：`data/processed/knowledge/structured/{domain}/*.json`\n",
    "\n",
    "### 向量库结构\n",
    "\n",
    "**存储位置**：`data/processed/knowledge/vector_db/{domain}/`\n",
    "\n",
    "**内容**：\n",
    "- 文档分块（chunk_size=1000, overlap=200）\n",
    "- 向量化（Qwen3-Embedding）\n",
    "- 元数据（domain、topic、seq）\n",
    "\n",
    "**使用方式**：通过Chroma向量库进行语义检索\n",
    "\n",
    "### 使用示例\n",
    "\n",
    "**读取JSON**：\n",
    "```python\n",
    "import json\n",
    "with open('data/processed/knowledge/structured/macro_economy/01_01第一节.json', 'r') as f:\n",
    "    knowledge = json.load(f)\n",
    "    print(knowledge['topic'])  # 主题名称\n",
    "    print(knowledge['key_concepts'][0]['name'])  # 第一个概念\n",
    "```\n",
    "\n",
    "**向量检索**：\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='Qwen/Qwen3-Embedding-0.6B')\n",
    "vector_store = Chroma(persist_directory='data/processed/knowledge/vector_db/macro_economy',\n",
    "                     embedding_function=embeddings)\n",
    "results = vector_store.similarity_search(\"经济周期\", k=3)\n",
    "```\n",
    "\n",
    "## 快速参考\n",
    "\n",
    "**核心流程**：扫描分组 → 加载清洗 → LLM提取 → 向量化 → 协调执行\n",
    "\n",
    "**输出位置**：\n",
    "- JSON: `data/processed/knowledge/structured/{domain}/*.json`\n",
    "- 向量库: `data/processed/knowledge/vector_db/{domain}/`\n",
    "\n",
    "**使用步骤**：\n",
    "1. `limit=2` 测试\n",
    "2. `process_all()` 处理全部\n",
    "3. 检查输出结果\n",
    "\n",
    "**注意事项**：首次运行会自动下载embedding模型\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyst_chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

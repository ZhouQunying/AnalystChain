{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# çŸ¥è¯†å¤„ç†Pipeline - ä¸“å®¶çº§å®ç°\n",
        "\n",
        "**æ–¹æ¡ˆ**: å‘é‡åº“+JSONåŒå­˜å‚¨  \n",
        "**æŠ€æœ¯æ ˆ**: Qwen3-Embedding + deepseek-v3 + Chroma  \n",
        "**è´¨é‡æ ‡å‡†**: éµå¾ª[AIè¡Œä¸ºçº¦æŸè§„èŒƒ](../../docs/AIè¡Œä¸ºçº¦æŸè§„èŒƒ.md)\n",
        "\n",
        "## æµç¨‹\n",
        "\n",
        "```\n",
        "PDF/Word/PPTæ–‡ä»¶ï¼ˆ51ä¸ªï¼‰\n",
        " â†“ [æ¨¡å—1] KnowledgeOrganizer\n",
        "17ä¸ªä¸»é¢˜ç»„\n",
        " â†“ [æ¨¡å—2] DocumentLoader  \n",
        "List[Document]\n",
        " â†“ [æ¨¡å—3] KnowledgeExtractor (deepseek-v3)\n",
        "ç»“æ„åŒ–JSON\n",
        " â†“ [æ¨¡å—4] VectorStoreManager (Qwen3-Embedding)\n",
        "Chromaå‘é‡åº“\n",
        " â†“ [æ¨¡å—5] KnowledgeProcessor\n",
        "å®Œæ•´çŸ¥è¯†åº“\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ========== ç¯å¢ƒå‡†å¤‡ ==========\n",
        "\n",
        "# æ ‡å‡†åº“\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from difflib import SequenceMatcher\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain - Document Loaders\n",
        "from langchain_community.document_loaders import (\n",
        "    PyMuPDFLoader,           # PDFåŠ è½½ï¼ˆæ¨èï¼‰\n",
        "    Docx2txtLoader,          # WordåŠ è½½  \n",
        "    UnstructuredPowerPointLoader  # PPTåŠ è½½\n",
        ")\n",
        "\n",
        "# LangChain - Core\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "\n",
        "# LangChain - Embeddings & VectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# LangChain - LLMï¼ˆæŠ€æœ¯å†³ç­–ï¼šdeepseek-v3ï¼‰\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "# è¿›åº¦æ˜¾ç¤º\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# æ—¥å¿—\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰ä¾èµ–å¯¼å…¥å®Œæˆ\")\n",
        "\n",
        "# ç¯å¢ƒå˜é‡\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"../../../config/.env\")\n",
        "\n",
        "print(\"âœ… ä¾èµ–å¯¼å…¥å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## é…ç½®å‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 15:33:29,936 - INFO - ğŸ“ çŸ¥è¯†åº“è·¯å¾„: ../knowledge_base\n",
            "2025-12-03 15:33:29,936 - INFO - ğŸ“ è¾“å‡ºè·¯å¾„: output\n",
            "2025-12-03 15:33:29,936 - INFO - ğŸ¤– Embedding: /Users/Qunying/.cache/huggingface/hub/models--Qwen--Qwen3-Embedding-0.6B/snapshots/c54f2e6e80b2d7b7de06f51cec4959f6b3e03418\n",
            "2025-12-03 15:33:29,937 - INFO - ğŸ¤– LLM: deepseek-reasoner\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… é…ç½®å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ========== å…¨å±€é…ç½® ==========\n",
        "\n",
        "# è·¯å¾„é…ç½®\n",
        "KNOWLEDGE_BASE_DIR = Path(\"../knowledge_base\")\n",
        "OUTPUT_DIR = Path(\"./output\")\n",
        "VECTOR_DB_DIR = OUTPUT_DIR / \"vector_db\"\n",
        "STRUCTURED_JSON_DIR = OUTPUT_DIR / \"structured_knowledge\"\n",
        "\n",
        "# æ¨¡å‹é…ç½®ï¼ˆæŒ‰æŠ€æœ¯å†³ç­–ï¼‰\n",
        "# Embeddingæ¨¡å‹ï¼šä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆé¿å…ç½‘ç»œä¸‹è½½ï¼‰\n",
        "EMBEDDING_MODEL_PATH = \"/Users/Qunying/.cache/huggingface/hub/models--Qwen--Qwen3-Embedding-0.6B/snapshots\"\n",
        "# è·å–æœ€æ–°çš„snapshotç›®å½•\n",
        "import os\n",
        "snapshot_dirs = os.listdir(EMBEDDING_MODEL_PATH)\n",
        "EMBEDDING_MODEL = os.path.join(EMBEDDING_MODEL_PATH, snapshot_dirs[0]) if snapshot_dirs else \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "\n",
        "LLM_MODEL = \"deepseek-reasoner\"  # å†³ç­–#005\n",
        "LLM_TEMPERATURE = 0  # ç¡®ä¿è¾“å‡ºç¨³å®šæ€§\n",
        "\n",
        "# æ–‡æœ¬åˆ†å‰²é…ç½®\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "VECTOR_DB_DIR.mkdir(exist_ok=True)\n",
        "STRUCTURED_JSON_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "logger.info(f\"ğŸ“ çŸ¥è¯†åº“è·¯å¾„: {KNOWLEDGE_BASE_DIR}\")\n",
        "logger.info(f\"ğŸ“ è¾“å‡ºè·¯å¾„: {OUTPUT_DIR}\")\n",
        "logger.info(f\"ğŸ¤– Embedding: {EMBEDDING_MODEL}\")\n",
        "logger.info(f\"ğŸ¤– LLM: {LLM_MODEL}\")\n",
        "\n",
        "print(\"âœ… é…ç½®å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ•°æ®ç»“æ„å®šä¹‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ========== æ•°æ®ç»“æ„å®šä¹‰ï¼ˆç¬¦åˆä¸“å®¶æ ‡å‡†ï¼‰==========\n",
        "\n",
        "from enum import IntEnum\n",
        "\n",
        "class FilePriority(IntEnum):\n",
        "    \"\"\"æ–‡ä»¶ä¼˜å…ˆçº§æšä¸¾\n",
        "    \n",
        "    ä¼˜å…ˆçº§è§„åˆ™ï¼š\n",
        "    - PDFç¬”è®°æœ€ä¼˜å…ˆï¼ˆæœ€è¯¦ç»†ï¼‰\n",
        "    - Wordæ–‡æ¡£æ¬¡ä¹‹\n",
        "    - æ™®é€šPDFç¬¬ä¸‰\n",
        "    - PPTæœ€åï¼ˆä¿¡æ¯å¯†åº¦ä½ï¼‰\n",
        "    \"\"\"\n",
        "    PDF_NOTE = 1      # PDFç¬”è®°æ–‡ä»¶ï¼ˆæ–‡ä»¶ååŒ…å«\"ç¬”è®°\"ï¼‰\n",
        "    WORD_DOC = 2      # Wordæ–‡æ¡£\n",
        "    PDF_REGULAR = 3   # æ™®é€šPDFæ–‡ä»¶\n",
        "    POWERPOINT = 4    # PowerPointæ–‡ä»¶\n",
        "    UNKNOWN = 99      # æœªçŸ¥ç±»å‹\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"æ–‡ä»¶ä¿¡æ¯æ•°æ®ç±»\n",
        "    \n",
        "    Attributes:\n",
        "        path: æ–‡ä»¶å®Œæ•´è·¯å¾„\n",
        "        original_name: åŸå§‹æ–‡ä»¶å\n",
        "        cleaned_name: æ¸…æ´—åçš„æ–‡ä»¶åï¼ˆå»é™¤å™ªéŸ³ï¼‰\n",
        "        sequence: åºå·ï¼ˆæ•´æ•°ï¼‰\n",
        "        sequence_str: åºå·å­—ç¬¦ä¸²\n",
        "        priority: æ–‡ä»¶ä¼˜å…ˆçº§\n",
        "    \"\"\"\n",
        "    path: Path\n",
        "    original_name: str\n",
        "    cleaned_name: str\n",
        "    sequence: int\n",
        "    sequence_str: str\n",
        "    priority: FilePriority\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeGroup:\n",
        "    \"\"\"çŸ¥è¯†ç»„æ•°æ®ç±»\n",
        "    \n",
        "    Attributes:\n",
        "        group_key: ç»„å”¯ä¸€æ ‡è¯†\n",
        "        topic: ä¸»é¢˜åç§°\n",
        "        sequence: ä¸»é¢˜åºå·\n",
        "        files: ç»„å†…æ‰€æœ‰æ–‡ä»¶\n",
        "        primary_file: ä¸»è¦æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰\n",
        "        file_types: æ–‡ä»¶ç±»å‹åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    group_key: str\n",
        "    topic: str\n",
        "    sequence: int\n",
        "    files: List[FileInfo]\n",
        "    primary_file: FileInfo\n",
        "    file_types: List[str]\n",
        "\n",
        "\n",
        "print(\"âœ… æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤1: KnowledgeOrganizer - æ–‡ä»¶æ‰«æåˆ†ç»„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ­¥éª¤1å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeOrganizer:\n",
        "    \"\"\"çŸ¥è¯†æ–‡ä»¶æ‰«æå’Œæ™ºèƒ½åˆ†ç»„\n",
        "    \n",
        "    åŠŸèƒ½:\n",
        "    1. æ‰«æçŸ¥è¯†åº“ç›®å½•,æ”¯æŒPDF/Word/PPT\n",
        "    2. æ¸…æ´—æ–‡ä»¶å(å»é™¤æ—¶é—´æˆ³ã€å™ªéŸ³æ ‡è®°ç­‰)\n",
        "    3. æŒ‰ç›¸ä¼¼åº¦æ™ºèƒ½åˆ†ç»„(åŒä¸»é¢˜çš„ä¸åŒæ–‡ä»¶ç±»å‹)\n",
        "    4. æŒ‰ä¼˜å…ˆçº§æ’åº(PDFç¬”è®° > Word > PDF > PPT)\n",
        "    \n",
        "    åˆ†ç»„ç®—æ³•:\n",
        "    - æå–åºå·(å¦‚\"01ç¬¬ä¸€èŠ‚\")ä½œä¸ºä¸»é”®\n",
        "    - è®¡ç®—æ–‡ä»¶åç›¸ä¼¼åº¦\n",
        "    - ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„å½’ä¸ºä¸€ç»„\n",
        "    \"\"\"\n",
        "    \n",
        "    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å\n",
        "    SUPPORTED_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}\n",
        "    \n",
        "    # æ–‡ä»¶åå™ªéŸ³æ¨¡å¼(æ­£åˆ™è¡¨è¾¾å¼)\n",
        "    NOISE_PATTERNS = [\n",
        "        r'\\[é˜²æ–­æ›´.*?\\]',  # [é˜²æ–­æ›´å¾®coc36666]\n",
        "        r'\\[.*?å¾®.*?\\]',   # [å¾®ä¿¡å·xxx]\n",
        "        r'_\\d{14}',        # _20250706193405\n",
        "        r'_\\d{8}',         # _20250706\n",
        "        r'_ç¬”è®°',          # _ç¬”è®°\n",
        "        r'\\s*\\(.*?\\)\\s*'   # (å¤‡æ³¨)\n",
        "    ]\n",
        "\n",
        "    def __init__(self, \n",
        "                 knowledge_base_dir: str, \n",
        "                 similarity_threshold: float = 0.7, \n",
        "                 verbose: bool = True):\n",
        "        \"\"\"åˆå§‹åŒ–çŸ¥è¯†ç»„ç»‡å™¨\n",
        "        \n",
        "        Args:\n",
        "            knowledge_base_dir: çŸ¥è¯†åº“æ ¹ç›®å½•\n",
        "            similarity_threshold: æ–‡ä»¶åç›¸ä¼¼åº¦é˜ˆå€¼(0-1),é»˜è®¤0.7\n",
        "            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—\n",
        "        \n",
        "        Raises:\n",
        "            ValueError: ç›®å½•ä¸å­˜åœ¨æ—¶æŠ›å‡º\n",
        "        \"\"\"\n",
        "        self.knowledge_base_dir = Path(knowledge_base_dir)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.verbose = verbose\n",
        "        if not self.knowledge_base_dir.exists():\n",
        "            raise ValueError(f\"ç›®å½•ä¸å­˜åœ¨: {self.knowledge_base_dir}\")\n",
        "\n",
        "    def _log(self, msg):\n",
        "        if self.verbose: print(msg)\n",
        "\n",
        "    def clean_filename(self, filename):\n",
        "        \"\"\"æ¸…æ´—æ–‡ä»¶å(å»é™¤å™ªéŸ³)\"\"\"\n",
        "        name = Path(filename).stem\n",
        "        for pattern in self.NOISE_PATTERNS:\n",
        "            name = re.sub(pattern, '', name)\n",
        "        return re.sub(r'\\s+', ' ', name).strip()\n",
        "\n",
        "    def extract_sequence_number(self, filename):\n",
        "        \"\"\"æå–åºå·\"\"\"\n",
        "        match = re.match(r'^(\\d+)', filename)\n",
        "        return (int(match.group(1)), match.group(1)) if match else (999999, \"\")\n",
        "\n",
        "    def calculate_similarity(self, str1, str2):\n",
        "        \"\"\"è®¡ç®—ç›¸ä¼¼åº¦\"\"\"\n",
        "        return SequenceMatcher(None, str1, str2).ratio()\n",
        "\n",
        "    def get_file_priority(self, file_path):\n",
        "        \"\"\"ç¡®å®šæ–‡ä»¶ä¼˜å…ˆçº§\"\"\"\n",
        "        name, suffix = file_path.name.lower(), file_path.suffix.lower()\n",
        "        if 'ç¬”è®°' in name and suffix == '.pdf': return FilePriority.PDF_NOTE\n",
        "        if suffix in ['.doc', '.docx']: return FilePriority.WORD_DOC\n",
        "        if suffix == '.pdf': return FilePriority.PDF_REGULAR\n",
        "        if suffix in ['.ppt', '.pptx']: return FilePriority.POWERPOINT\n",
        "        return FilePriority.UNKNOWN\n",
        "\n",
        "    def create_file_info(self, file_path):\n",
        "        \"\"\"åˆ›å»ºæ–‡ä»¶ä¿¡æ¯å¯¹è±¡\"\"\"\n",
        "        original_name = file_path.name\n",
        "        cleaned_name = self.clean_filename(original_name)\n",
        "        sequence, sequence_str = self.extract_sequence_number(cleaned_name)\n",
        "        priority = self.get_file_priority(file_path)\n",
        "        return FileInfo(file_path, original_name, cleaned_name, sequence, sequence_str, priority)\n",
        "\n",
        "    def group_files_by_similarity(self, files):\n",
        "        \"\"\"æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„\"\"\"\n",
        "        groups, processed = {}, set()\n",
        "        for i, file1 in enumerate(files):\n",
        "            if file1.path in processed: continue\n",
        "            group_key = f\"{file1.sequence_str}_{file1.cleaned_name[:20]}\"\n",
        "            group_files = [file1]\n",
        "            processed.add(file1.path)\n",
        "\n",
        "            for file2 in files[i+1:]:\n",
        "                if file2.path in processed: continue\n",
        "                if file1.sequence == file2.sequence:\n",
        "                    if self.calculate_similarity(file1.cleaned_name, file2.cleaned_name) >= self.similarity_threshold:\n",
        "                        group_files.append(file2)\n",
        "                        processed.add(file2.path)\n",
        "\n",
        "            group_files.sort(key=lambda f: (f.priority.value, f.original_name))\n",
        "            groups[group_key] = KnowledgeGroup(group_key, file1.cleaned_name, file1.sequence,\n",
        "                                              group_files, group_files[0], [f.path.suffix for f in group_files])\n",
        "            self._log(f\"âœ“ {file1.cleaned_name[:30]} ({len(group_files)}æ–‡ä»¶)\")\n",
        "        return groups\n",
        "\n",
        "    def scan_and_organize(self):\n",
        "        \"\"\"æ‰«æå¹¶ç»„ç»‡æ–‡ä»¶\"\"\"\n",
        "        self._log(f\"ğŸ“š æ‰«æ: {self.knowledge_base_dir}\")\n",
        "        all_files = []\n",
        "        for ext in self.SUPPORTED_EXTENSIONS:\n",
        "            all_files.extend(self.knowledge_base_dir.glob(f\"*{ext}\"))\n",
        "\n",
        "        if not all_files:\n",
        "            self._log(\"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶\")\n",
        "            return {}\n",
        "\n",
        "        self._log(f\"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶\")\n",
        "        file_infos = [self.create_file_info(f) for f in all_files]\n",
        "        groups = self.group_files_by_similarity(file_infos)\n",
        "        sorted_groups = dict(sorted(groups.items(), key=lambda x: x[1].sequence))\n",
        "        self._log(f\"âœ… {len(sorted_groups)} ä¸ªçŸ¥è¯†å—\\n\")\n",
        "        return {self.knowledge_base_dir.name: sorted_groups}\n",
        "\n",
        "print(\"âœ… æ­¥éª¤1å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ­¥éª¤2å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "class DocumentLoader:\n",
        "    \"\"\"æ–‡æ¡£åŠ è½½å™¨\n",
        "    \n",
        "    æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼åŠ è½½:\n",
        "    - PDF: PyMuPDFLoader (æ¨è,æ€§èƒ½æœ€ä½³)\n",
        "    - Word: Docx2txtLoader (.doc, .docx)\n",
        "    - PowerPoint: UnstructuredPowerPointLoader (.ppt, .pptx)\n",
        "    \"\"\"\n",
        "\n",
        "    def load_pdf(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PDFæ–‡ä»¶\n",
        "        \n",
        "        Args:\n",
        "            file_path: PDFæ–‡ä»¶è·¯å¾„\n",
        "            \n",
        "        Returns:\n",
        "            æ–‡æ¡£åˆ—è¡¨(æ¯é¡µä¸€ä¸ªDocument)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loader = PyMuPDFLoader(str(file_path))\n",
        "            return loader.load()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDFåŠ è½½å¤±è´¥ {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def load_word(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½Wordæ–‡ä»¶\n",
        "        \n",
        "        Args:\n",
        "            file_path: Wordæ–‡ä»¶è·¯å¾„\n",
        "            \n",
        "        Returns:\n",
        "            æ–‡æ¡£åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loader = Docx2txtLoader(str(file_path))\n",
        "            return loader.load()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"WordåŠ è½½å¤±è´¥ {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def load_ppt(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PowerPointæ–‡ä»¶\n",
        "        \n",
        "        Args:\n",
        "            file_path: PPTæ–‡ä»¶è·¯å¾„\n",
        "            \n",
        "        Returns:\n",
        "            æ–‡æ¡£åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loader = UnstructuredPowerPointLoader(str(file_path))\n",
        "            return loader.load()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PPTåŠ è½½å¤±è´¥ {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def clean_document_text(self, doc: Document) -> Document:\n",
        "        \"\"\"æ¸…æ´—æ–‡æ¡£æ–‡æœ¬\n",
        "        \n",
        "        æ¸…ç†å†…å®¹:\n",
        "        - ç‰¹æ®Šå­—ç¬¦\n",
        "        - å¤šä½™ç©ºç™½\n",
        "        - å™ªéŸ³å†…å®¹\n",
        "        \n",
        "        Args:\n",
        "            doc: åŸå§‹æ–‡æ¡£\n",
        "            \n",
        "        Returns:\n",
        "            æ¸…æ´—åçš„æ–‡æ¡£\n",
        "        \"\"\"\n",
        "        text = doc.page_content\n",
        "        text = re.sub(r'[\\uf06c\\uf0fc]', '', text)  # ç‰¹æ®Šå­—ç¬¦\n",
        "        text = re.sub(r'\\s+', ' ', text)  # å¤šä½™ç©ºç™½\n",
        "        doc.page_content = text.strip()\n",
        "        return doc\n",
        "\n",
        "    def load_and_clean(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½å¹¶æ¸…æ´—æ–‡æ¡£(ç»Ÿä¸€å…¥å£)\n",
        "        \n",
        "        Args:\n",
        "            file_path: æ–‡ä»¶è·¯å¾„\n",
        "            \n",
        "        Returns:\n",
        "            æ¸…æ´—åçš„æ–‡æ¡£åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        suffix = file_path.suffix.lower()\n",
        "        \n",
        "        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨\n",
        "        if suffix == '.pdf':\n",
        "            docs = self.load_pdf(file_path)\n",
        "        elif suffix in ['.doc', '.docx']:\n",
        "            docs = self.load_word(file_path)\n",
        "        elif suffix in ['.ppt', '.pptx']:\n",
        "            docs = self.load_ppt(file_path)\n",
        "        else:\n",
        "            logger.warning(f\"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {suffix}\")\n",
        "            return []\n",
        "        \n",
        "        # æ¸…æ´—æ–‡æ¡£\n",
        "        if docs:\n",
        "            docs = [self.clean_document_text(doc) for doc in docs]\n",
        "            logger.info(f\"âœ“ åŠ è½½ {file_path.name}: {len(docs)}é¡µ\")\n",
        "        \n",
        "        return docs\n",
        "\n",
        "print(\"âœ… æ­¥éª¤2å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤3: KnowledgeExtractor - LLMç»“æ„åŒ–æå–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ­¥éª¤3å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeExtractor:\n",
        "    \"\"\"LLMçŸ¥è¯†æå–å™¨\n",
        "    \n",
        "    ä½¿ç”¨LLMä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†(JSONæ ¼å¼)\n",
        "    æŠ€æœ¯å†³ç­–: ä½¿ç”¨deepseek-v3æ¨¡å‹è¿›è¡ŒçŸ¥è¯†æå–\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = LLM_MODEL, temperature: float = LLM_TEMPERATURE):\n",
        "        \"\"\"åˆå§‹åŒ–çŸ¥è¯†æå–å™¨\n",
        "        \n",
        "        Args:\n",
        "            model_name: LLMæ¨¡å‹åç§°,é»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®\n",
        "            temperature: æ¸©åº¦å‚æ•°,é»˜è®¤0ç¡®ä¿è¾“å‡ºç¨³å®šæ€§\n",
        "        \"\"\"\n",
        "        self.llm = ChatDeepSeek(model=model_name, temperature=temperature)\n",
        "        logger.info(f\"ğŸ¤– LLMåˆå§‹åŒ–: {model_name}\")\n",
        "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ä½ æ˜¯é‡‘èçŸ¥è¯†æå–ä¸“å®¶ã€‚ä»æ–‡æ¡£æå–ç»“æ„åŒ–çŸ¥è¯†,è¿”å›JSONã€‚\n",
        "\n",
        "æ–‡æ¡£: {content}\n",
        "\n",
        "æå–JSON(åªè¿”å›JSON):\n",
        "{{\n",
        "  \"topic\": \"ä¸»é¢˜\",\n",
        "  \"key_concepts\": [{{\"name\": \"æ¦‚å¿µ\", \"definition\": \"å®šä¹‰\", \"importance\": \"é‡è¦æ€§\"}}],\n",
        "  \"indicators\": [{{\"name\": \"æŒ‡æ ‡\", \"calculation\": \"è®¡ç®—\", \"interpretation\": \"è§£è¯»\"}}],\n",
        "  \"analysis_methods\": [{{\"name\": \"æ–¹æ³•\", \"steps\": \"æ­¥éª¤\", \"application\": \"åº”ç”¨\"}}],\n",
        "  \"summary\": \"æ€»ç»“\"\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    def extract_from_documents(self, docs: List[Document], topic: str) -> Dict:\n",
        "        \"\"\"ä»æ–‡æ¡£æå–çŸ¥è¯†\"\"\"\n",
        "        content = \"\\n\\n\".join([d.page_content for d in docs[:5]])[:15000]\n",
        "        try:\n",
        "            chain = self.prompt | self.llm | JsonOutputParser()\n",
        "            result = chain.invoke({\"content\": content})\n",
        "            result[\"topic\"] = topic\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ LLMæå–å¤±è´¥: {e}\")\n",
        "            return {\"topic\": topic, \"key_concepts\": [], \"indicators\": [],\n",
        "                   \"analysis_methods\": [], \"summary\": \"æå–å¤±è´¥\"}\n",
        "\n",
        "print(\"âœ… æ­¥éª¤3å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤4: VectorStoreManager - å‘é‡åŒ–å­˜å‚¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ­¥éª¤4å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"å‘é‡å­˜å‚¨ç®¡ç†å™¨\n",
        "    \n",
        "    è´Ÿè´£æ–‡æ¡£å‘é‡åŒ–å’ŒChromaå‘é‡æ•°æ®åº“ç®¡ç†\n",
        "    æŠ€æœ¯å†³ç­–: ä½¿ç”¨Qwen3-Embedding-0.6Bæ¨¡å‹è¿›è¡Œå‘é‡åŒ–\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 embedding_model: str = EMBEDDING_MODEL, \n",
        "                 persist_directory: str = str(VECTOR_DB_DIR)):\n",
        "        \"\"\"åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨\n",
        "        \n",
        "        Args:\n",
        "            embedding_model: Embeddingæ¨¡å‹åç§°,é»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®\n",
        "            persist_directory: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•\n",
        "        \"\"\"\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "        self.persist_directory = Path(persist_directory)\n",
        "        logger.info(f\"ğŸ¤– Embeddingåˆå§‹åŒ–: {embedding_model}\")\n",
        "        self.vector_stores = {}\n",
        "\n",
        "    def get_or_create_store(self, domain: str) -> Chroma:\n",
        "        \"\"\"è·å–æˆ–åˆ›å»ºå‘é‡å­˜å‚¨\"\"\"\n",
        "        if domain in self.vector_stores:\n",
        "            return self.vector_stores[domain]\n",
        "\n",
        "        persist_path = str(self.persist_directory / domain)\n",
        "        store = Chroma(\n",
        "            collection_name=f\"{domain}_col\",\n",
        "            embedding_function=self.embeddings,\n",
        "            persist_directory=persist_path\n",
        "        )\n",
        "        self.vector_stores[domain] = store\n",
        "        return store\n",
        "\n",
        "    def split_documents(self, docs: List[Document]) -> List[Document]:\n",
        "        \"\"\"åˆ†å‰²æ–‡æ¡£ä¸ºchunks\"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        "        )\n",
        "        return splitter.split_documents(docs)\n",
        "\n",
        "    def add_documents(self, domain: str, docs: List[Document], metadata: Dict = None):\n",
        "        \"\"\"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨\"\"\"\n",
        "        if not docs:\n",
        "            print(\"  âš ï¸ æ— æ–‡æ¡£,è·³è¿‡å‘é‡åŒ–\")\n",
        "            return\n",
        "\n",
        "        chunks = self.split_documents(docs)\n",
        "        if metadata:\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata.update(metadata)\n",
        "\n",
        "        store = self.get_or_create_store(domain)\n",
        "        store.add_documents(chunks)\n",
        "        print(f\"  â†’ å‘é‡åŒ–: {len(chunks)} chunks\")\n",
        "\n",
        "print(\"âœ… æ­¥éª¤4å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤5: KnowledgeProcessor - å®Œæ•´Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ­¥éª¤5å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeProcessor:\n",
        "    \"\"\"çŸ¥è¯†å¤„ç†Pipelineåè°ƒå™¨\n",
        "    \n",
        "    æ•´åˆ5ä¸ªæ ¸å¿ƒæ¨¡å—,æ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†å¤„ç†æµç¨‹:\n",
        "    1. æ–‡ä»¶æ‰«æåˆ†ç»„ (KnowledgeOrganizer)\n",
        "    2. æ–‡æ¡£åŠ è½½ (DocumentLoader)\n",
        "    3. çŸ¥è¯†æå– (KnowledgeExtractor)\n",
        "    4. å‘é‡åŒ–å­˜å‚¨ (VectorStoreManager)\n",
        "    5. JSONå­˜å‚¨\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 knowledge_base_dir: str, \n",
        "                 memories_dir: str = str(STRUCTURED_JSON_DIR), \n",
        "                 vector_db_dir: str = str(VECTOR_DB_DIR)):\n",
        "        \"\"\"åˆå§‹åŒ–Pipelineåè°ƒå™¨\n",
        "        \n",
        "        Args:\n",
        "            knowledge_base_dir: çŸ¥è¯†åº“æ ¹ç›®å½•\n",
        "            memories_dir: JSONç»“æ„åŒ–çŸ¥è¯†å­˜å‚¨ç›®å½•\n",
        "            vector_db_dir: å‘é‡æ•°æ®åº“å­˜å‚¨ç›®å½•\n",
        "        \"\"\"\n",
        "        self.organizer = KnowledgeOrganizer(knowledge_base_dir)\n",
        "        self.loader = DocumentLoader()\n",
        "        self.extractor = KnowledgeExtractor()\n",
        "        self.vector_manager = VectorStoreManager(persist_directory=vector_db_dir)\n",
        "        self.memories_dir = Path(memories_dir)\n",
        "        logger.info(\"ğŸš€ Pipelineåè°ƒå™¨åˆå§‹åŒ–å®Œæˆ\")\n",
        "\n",
        "    def save_to_memories(self, domain: str, group_key: str, knowledge: Dict):\n",
        "        \"\"\"ä¿å­˜çŸ¥è¯†åˆ°JSON\"\"\"\n",
        "        domain_dir = self.memories_dir / domain\n",
        "        domain_dir.mkdir(parents=True, exist_ok=True)\n",
        "        json_file = domain_dir / f\"{group_key}.json\"\n",
        "        with open(json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(knowledge, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"  â†’ JSON: {json_file.name}\")\n",
        "\n",
        "    def process_all(self, limit: int = None):\n",
        "        \"\"\"å¤„ç†æ‰€æœ‰çŸ¥è¯†æ–‡ä»¶\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸš€ å¼€å§‹å®Œæ•´Pipeline\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        organized = self.organizer.scan_and_organize()\n",
        "\n",
        "        for domain, groups in organized.items():\n",
        "            print(f\"\\nğŸ“‚ é¢†åŸŸ: {domain}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            count = 0\n",
        "            for group_key, group in groups.items():\n",
        "                if limit and count >= limit:\n",
        "                    print(f\"\\nâ¸ï¸  è¾¾åˆ°é™åˆ¶({limit}),åœæ­¢\")\n",
        "                    break\n",
        "\n",
        "                print(f\"\\n[{count+1}] {group.topic}\")\n",
        "\n",
        "                # åŠ è½½\n",
        "                docs = self.loader.load_and_clean(group.primary_file.path)\n",
        "                if not docs:\n",
        "                    print(\"  âš ï¸ åŠ è½½å¤±è´¥\")\n",
        "                    continue\n",
        "                print(f\"  â†’ åŠ è½½: {len(docs)} é¡µ\")\n",
        "\n",
        "                # æå–\n",
        "                knowledge = self.extractor.extract_from_documents(docs, group.topic)\n",
        "                self.save_to_memories(domain, group_key, knowledge)\n",
        "\n",
        "                # å‘é‡åŒ–\n",
        "                self.vector_manager.add_documents(domain, docs,\n",
        "                    {\"domain\": domain, \"topic\": group.topic, \"seq\": group.sequence})\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            print(f\"\\nâœ… {domain}: å®Œæˆ {count} ä¸ª\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ‰ Pipelineå®Œæˆ!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nğŸ“ è¾“å‡º:\")\n",
        "        print(f\"  - JSON: {self.memories_dir}\")\n",
        "        print(f\"  - å‘é‡åº“: {self.vector_manager.persist_directory}\")\n",
        "\n",
        "print(\"âœ… æ­¥éª¤5å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 15:33:30,131 - INFO - ğŸ¤– LLMåˆå§‹åŒ–: deepseek-reasoner\n",
            "2025-12-03 15:33:30,134 - INFO - Use pytorch device_name: mps\n",
            "2025-12-03 15:33:30,134 - INFO - Load pretrained SentenceTransformer: /Users/Qunying/.cache/huggingface/hub/models--Qwen--Qwen3-Embedding-0.6B/snapshots/c54f2e6e80b2d7b7de06f51cec4959f6b3e03418\n",
            "2025-12-03 15:33:34,327 - INFO - 1 prompt is loaded, with the key: query\n",
            "2025-12-03 15:33:34,328 - INFO - ğŸ¤– Embeddingåˆå§‹åŒ–: /Users/Qunying/.cache/huggingface/hub/models--Qwen--Qwen3-Embedding-0.6B/snapshots/c54f2e6e80b2d7b7de06f51cec4959f6b3e03418\n",
            "2025-12-03 15:33:34,329 - INFO - ğŸš€ Pipelineåè°ƒå™¨åˆå§‹åŒ–å®Œæˆ\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processoråˆå§‹åŒ–å®Œæˆ\n",
            "ğŸ“ çŸ¥è¯†åº“: ../knowledge_base\n",
            "ğŸ“ JSONè¾“å‡º: output/structured_knowledge\n",
            "ğŸ“ å‘é‡åº“: output/vector_db\n"
          ]
        }
      ],
      "source": [
        "# ========== Pipelineæµ‹è¯• ==========\n",
        "\n",
        "# åˆå§‹åŒ–Processor(ä½¿ç”¨å…¨å±€é…ç½®)\n",
        "processor = KnowledgeProcessor(\n",
        "    knowledge_base_dir=str(KNOWLEDGE_BASE_DIR)\n",
        ")\n",
        "\n",
        "print(\"âœ… Processoråˆå§‹åŒ–å®Œæˆ\")\n",
        "print(f\"ğŸ“ çŸ¥è¯†åº“: {KNOWLEDGE_BASE_DIR}\")\n",
        "print(f\"ğŸ“ JSONè¾“å‡º: {STRUCTURED_JSON_DIR}\")\n",
        "print(f\"ğŸ“ å‘é‡åº“: {VECTOR_DB_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 15:33:34,396 - INFO - âœ“ åŠ è½½ 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€[é˜²æ–­æ›´å¾®coc36666]_ç¬”è®°.pdf: 4é¡µ\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸš€ å¼€å§‹å®Œæ•´Pipeline\n",
            "================================================================================\n",
            "\n",
            "ğŸ“š æ‰«æ: ../knowledge_base\n",
            "æ‰¾åˆ° 51 ä¸ªæ–‡ä»¶\n",
            "âœ“ 14.ç¬¬åå››èŠ‚ æ±‡ç‡æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ“ 07ç¬¬ä¸ƒèŠ‚ ç‰©ä»·â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç» (3æ–‡ä»¶)\n",
            "âœ“ 11.ç¬¬åä¸€èŠ‚ åŸºé‡‘æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ“ 12.ç¬¬åäºŒèŠ‚ ä¿é™©æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ“ 08ç¬¬å…«èŠ‚ å¦‚ä½•è¯»æ‡‚ç»æµå‘¨æœŸ (3æ–‡ä»¶)\n",
            "âœ“ 15.ç¬¬åäº”èŠ‚ å¤§å®—å•†å“æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ“ 05ç¬¬äº”èŠ‚ PMIâ€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "âœ“ 06ç¬¬å…­èŠ‚ é‡‘èâ€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "âœ“ 17.ç¬¬åä¸ƒèŠ‚ æ ¼é›·å„å§†ï¼šåå°”è¡—æ•™çˆ¶ (3æ–‡ä»¶)\n",
            "âœ“ 13.ç¬¬åä¸‰èŠ‚ é»„é‡‘æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ“ 03ç¬¬ä¸‰èŠ‚ æŠ•èµ„â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "âœ“ 04ç¬¬å››èŠ‚ å‡ºå£â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "âœ“ 09ç¬¬ä¹èŠ‚ çœ‹æ‡‚æŠ•èµ„æ—¶é’Ÿï¼Œè¸©å‡†æŠ•èµ„èŠ‚å¥ (3æ–‡ä»¶)\n",
            "âœ“ 16.ç¬¬åå…­èŠ‚ æˆ¿åœ°äº§æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ“ 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€ (3æ–‡ä»¶)\n",
            "âœ“ 02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "âœ“ 10ç¬¬åèŠ‚ è‚¡å¸‚æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "âœ… 17 ä¸ªçŸ¥è¯†å—\n",
            "\n",
            "\n",
            "ğŸ“‚ é¢†åŸŸ: knowledge_base\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1] 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€\n",
            "  â†’ åŠ è½½: 4 é¡µ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 15:33:34,616 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "# æµ‹è¯•: å¤„ç†å‰2ä¸ªçŸ¥è¯†å—\n",
        "processor.process_all(limit=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­£å¼è¿è¡Œ: å¤„ç†æ‰€æœ‰çŸ¥è¯†å—(å–æ¶ˆæ³¨é‡Š)\n",
        "# processor.process_all()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ£€æŸ¥è¾“å‡º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹ç”Ÿæˆçš„JSON\n",
        "memories_path = Path(\"./memories/knowledge\")\n",
        "if memories_path.exists():\n",
        "    for domain_dir in memories_path.iterdir():\n",
        "        if domain_dir.is_dir():\n",
        "            print(f\"\\nğŸ“‚ {domain_dir.name}:\")\n",
        "            for json_file in sorted(domain_dir.glob(\"*.json\")):\n",
        "                print(f\"  - {json_file.name}\")\n",
        "else:\n",
        "    print(\"âš ï¸ å°šæœªç”Ÿæˆè¾“å‡º\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¯»å–ç¬¬ä¸€ä¸ªJSONæŸ¥çœ‹\n",
        "json_files = list(Path(\"./memories/knowledge\").rglob(\"*.json\"))\n",
        "if json_files:\n",
        "    with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "        sample = json.load(f)\n",
        "    print(f\"ç¤ºä¾‹ ({json_files[0].name}):\")\n",
        "    print(json.dumps(sample, ensure_ascii=False, indent=2))\n",
        "else:\n",
        "    print(\"âš ï¸ æ— JSON\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“\n",
        "\n",
        "### âœ… åŠŸèƒ½å®Œæ•´\n",
        "\n",
        "- æ­¥éª¤1: æ–‡ä»¶æ‰«æåˆ†ç»„ âœ“\n",
        "- æ­¥éª¤2: æ–‡æ¡£åŠ è½½æ¸…æ´— âœ“ (ä»…PDF)\n",
        "- æ­¥éª¤3: LLMç»“æ„åŒ–æå– âœ“\n",
        "- æ­¥éª¤4: å‘é‡åŒ–å­˜å‚¨ âœ“\n",
        "- æ­¥éª¤5: Pipelineåè°ƒ âœ“\n",
        "\n",
        "### ğŸ“¦ è¾“å‡º\n",
        "\n",
        "- `memories/knowledge/{domain}/*.json` - ç»“æ„åŒ–çŸ¥è¯†\n",
        "- `vector_db/{domain}/` - å‘é‡æ•°æ®åº“\n",
        "\n",
        "### ğŸš€ ä½¿ç”¨\n",
        "\n",
        "1. å…ˆç”¨`limit=2`æµ‹è¯•\n",
        "2. ç¡®è®¤å`process_all()`å¤„ç†å…¨éƒ¨\n",
        "3. æ£€æŸ¥è¾“å‡ºç»“æœ\n",
        "\n",
        "### âš™ï¸ è¯´æ˜\n",
        "\n",
        "- **ä»…æ”¯æŒPDF**: ç®€åŒ–ç‰ˆåªå¤„ç†PDF(ä¸»è¦æ ¼å¼)\n",
        "- **éœ€API Key**: DeepSeek API (åœ¨.envé…ç½®)\n",
        "- **é¦–æ¬¡ä¸‹è½½**: embeddingæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "analyst_chain (conda)",
      "language": "python",
      "name": "analyst_chain"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

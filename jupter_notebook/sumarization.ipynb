{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d70a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 智能文档摘要系统 - 基于Map-Reduce架构的并行化摘要生成\n",
    "# =============================================================================\n",
    "# 核心思想：将大文档拆分成小块并行处理，然后逐步合并成最终摘要\n",
    "# 优势：处理超长文档，避免单次调用token限制，提高处理效率\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,      # 异步合并文档摘要\n",
    "    split_list_of_docs,  # 按token数量智能分割文档列表\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# 全局配置：单次处理的最大token数量\n",
    "token_max = 1000\n",
    "\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"计算文档列表的总token数量\"\"\"\n",
    "    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 状态定义：整个摘要流程的数据结构\n",
    "# =============================================================================\n",
    "class OverallState(TypedDict):\n",
    "    \"\"\"主图状态：管理整个摘要流程的数据流\"\"\"\n",
    "    contents: List[str]                    # 原始文档内容列表\n",
    "    summaries: Annotated[list, operator.add]  # 并行生成的摘要列表（使用operator.add自动合并）\n",
    "    collapsed_summaries: List[Document]   # 合并后的摘要文档\n",
    "    final_summary: str                     # 最终生成的摘要\n",
    "\n",
    "\n",
    "class SummaryState(TypedDict):\n",
    "    \"\"\"单次摘要状态：处理单个文档的摘要生成\"\"\"\n",
    "    content: str  # 待摘要的文档内容\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 核心处理函数：Map阶段 - 并行生成摘要\n",
    "# =============================================================================\n",
    "async def generate_summary(state: SummaryState):\n",
    "    \"\"\"为单个文档生成摘要（Map阶段的核心函数）\"\"\"\n",
    "    prompt = map_prompt.invoke(state[\"content\"])  # 调用摘要提示模板\n",
    "    response = await llm.ainvoke(prompt)          # 异步调用LLM生成摘要\n",
    "    return {\"summaries\": [response.content]}      # 返回摘要结果\n",
    "\n",
    "\n",
    "def map_summaries(state: OverallState):\n",
    "    \"\"\"分发任务：将每个文档内容发送给摘要生成节点\"\"\"\n",
    "    # 为每个文档内容创建一个Send对象，实现并行处理\n",
    "    return [\n",
    "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_summaries(state: OverallState):\n",
    "    \"\"\"收集摘要：将并行生成的摘要收集成文档列表\"\"\"\n",
    "    return {\n",
    "        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 核心处理函数：Reduce阶段 - 合并摘要\n",
    "# =============================================================================\n",
    "async def _reduce(input: dict) -> str:\n",
    "    \"\"\"内部合并函数：将多个摘要合并成一个\"\"\"\n",
    "    prompt = reduce_prompt.invoke(input)  # 调用合并提示模板\n",
    "    response = await llm.ainvoke(prompt)  # 异步调用LLM进行合并\n",
    "    return response.content\n",
    "\n",
    "\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    \"\"\"智能合并摘要：根据token限制分批合并摘要\"\"\"\n",
    "    # 按token数量智能分割摘要列表\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"], length_function, token_max\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    # 对每个分割的摘要组进行合并\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list, _reduce))\n",
    "    \n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 流程控制：决定是否需要继续合并\n",
    "# =============================================================================\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    \"\"\"判断是否需要继续合并摘要\"\"\"\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    \n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"      # 超过限制，继续合并\n",
    "    else:\n",
    "        return \"generate_final_summary\"  # 在限制内，生成最终摘要\n",
    "\n",
    "\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    \"\"\"生成最终摘要：将合并后的摘要转换为最终结果\"\"\"\n",
    "    response = await _reduce(state[\"collapsed_summaries\"])\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 图结构构建：定义整个摘要流程的执行逻辑\n",
    "# =============================================================================\n",
    "# 创建状态图\n",
    "graph = StateGraph(OverallState)\n",
    "\n",
    "# 添加节点：定义处理步骤\n",
    "graph.add_node(\"generate_summary\", generate_summary)        # 并行摘要生成\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)     # 摘要收集\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)   # 摘要合并\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)  # 最终摘要\n",
    "\n",
    "# 添加边：定义执行流程\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])  # 开始→分发任务\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")                  # 生成→收集\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)        # 收集→判断是否需要合并\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)       # 合并→判断是否需要继续合并\n",
    "graph.add_edge(\"generate_final_summary\", END)                            # 最终摘要→结束\n",
    "\n",
    "# 编译图：生成可执行的摘要应用\n",
    "app = graph.compile()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

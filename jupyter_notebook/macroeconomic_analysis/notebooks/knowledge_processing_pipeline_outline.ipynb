{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# çŸ¥è¯†å¤„ç†å®Œæ•´Pipeline - 5æ­¥éª¤å®ç°\n",
        "\n",
        "## ğŸ“‹ æµç¨‹æ¦‚è§ˆ\n",
        "\n",
        "æœ¬notebookå®ç°ä»åŸå§‹çŸ¥è¯†æ–‡ä»¶åˆ°æœ€ç»ˆçŸ¥è¯†åº“çš„å®Œæ•´æµç¨‹:\n",
        "\n",
        "```\n",
        "åŸå§‹æ–‡ä»¶(PDF/DOC/PPTX)\n",
        "    â†“\n",
        "ã€æ­¥éª¤1ã€‘æ–‡ä»¶æ‰«æåˆ†ç»„ (KnowledgeOrganizer)\n",
        "    â†“\n",
        "ã€æ­¥éª¤2ã€‘æ–‡æ¡£åŠ è½½æ¸…æ´— (DocumentLoader)\n",
        "    â†“\n",
        "ã€æ­¥éª¤3ã€‘ç»“æ„åŒ–æå– (KnowledgeExtractor)\n",
        "    â†“\n",
        "ã€æ­¥éª¤4ã€‘å‘é‡åŒ–å­˜å‚¨ (VectorStoreManager)\n",
        "    â†“\n",
        "ã€æ­¥éª¤5ã€‘å®Œæ•´Pipeline (KnowledgeProcessor)\n",
        "    â†“\n",
        "æœ€ç»ˆè¾“å‡º:\n",
        "  - /memories/knowledge/{domain}/structured.json\n",
        "  - vector_db/{domain}/\n",
        "```\n",
        "\n",
        "## ğŸ¯ ç›®æ ‡\n",
        "\n",
        "- **ä¸€æ¬¡ç”Ÿæˆ,æ°¸ä¹…ä½¿ç”¨**: é¢„å¤„ç†çŸ¥è¯†,é¿å…é‡å¤è®¡ç®—\n",
        "- **åŒå­˜å‚¨**: JSON(ç»“æ„åŒ–) + å‘é‡åº“(è¯­ä¹‰æ£€ç´¢)\n",
        "- **æ¨¡å—åŒ–**: æ¯ä¸ªæ­¥éª¤ç‹¬ç«‹å¯æµ‹è¯•\n",
        "- **ä¸“ä¸šåŒ–**: å®Œæ•´ç±»å‹æç¤ºã€é”™è¯¯å¤„ç†ã€æ—¥å¿—\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. å¯¼å…¥ä¾èµ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ ‡å‡†åº“\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from difflib import SequenceMatcher\n",
        "from enum import IntEnum\n",
        "\n",
        "# LangChain - æ–‡æ¡£åŠ è½½\n",
        "from langchain_community.document_loaders import (\n",
        "    PyMuPDFLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        "    UnstructuredPowerPointLoader\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LangChain - å‘é‡å­˜å‚¨\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# LangChain - LLM\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# ç¯å¢ƒå˜é‡\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# åŠ è½½ç¯å¢ƒå˜é‡\n",
        "load_dotenv(\"../../../config/.env\")\n",
        "\n",
        "print(\"âœ… ä¾èµ–å¯¼å…¥å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤1: æ–‡ä»¶æ‰«æä¸æ™ºèƒ½åˆ†ç»„ (KnowledgeOrganizer)\n",
        "\n",
        "**åŠŸèƒ½**: æ‰«æç›®å½•,æ™ºèƒ½è¯†åˆ«åŒä¸€çŸ¥è¯†å—çš„å¤šæ ¼å¼æ–‡ä»¶,é€‰æ‹©ä¸»æ–‡ä»¶\n",
        "\n",
        "**è¾“å‡º**: `organized_files` - åˆ†ç»„åçš„æ–‡ä»¶ä¿¡æ¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®ç»“æ„å®šä¹‰\n",
        "class FilePriority(IntEnum):\n",
        "    \"\"\"æ–‡ä»¶ä¼˜å…ˆçº§æšä¸¾\"\"\"\n",
        "    PDF_NOTE = 1\n",
        "    WORD_DOC = 2\n",
        "    PDF_REGULAR = 3\n",
        "    POWERPOINT = 4\n",
        "    UNKNOWN = 99\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"æ–‡ä»¶ä¿¡æ¯\"\"\"\n",
        "    path: Path\n",
        "    original_name: str\n",
        "    cleaned_name: str\n",
        "    sequence: int\n",
        "    sequence_str: str\n",
        "    priority: FilePriority\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"FileInfo({self.sequence_str}_{self.cleaned_name[:20]}..., {self.priority.name})\"\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeGroup:\n",
        "    \"\"\"çŸ¥è¯†å—åˆ†ç»„\"\"\"\n",
        "    group_key: str\n",
        "    topic: str\n",
        "    sequence: int\n",
        "    files: List[FileInfo]\n",
        "    primary_file: FileInfo\n",
        "    file_types: List[str]\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            \"group_key\": self.group_key,\n",
        "            \"topic\": self.topic,\n",
        "            \"sequence\": self.sequence,\n",
        "            \"primary_file\": str(self.primary_file.path),\n",
        "            \"files\": [str(f.path) for f in self.files],\n",
        "            \"file_types\": self.file_types,\n",
        "        }\n",
        "\n",
        "print(\"âœ… æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# æ­¥éª¤1: KnowledgeOrganizer ç±»å®ç°\n",
        "# ============================================================\n",
        "# TODO: å¤åˆ¶step1_knowledge_organizer.ipynbä¸­çš„å®Œæ•´å®ç°\n",
        "# æˆ–è€…ä»å¤´å®ç°ä»¥ä¸‹æ–¹æ³•:\n",
        "\n",
        "class KnowledgeOrganizer:\n",
        "    \"\"\"çŸ¥è¯†æ–‡ä»¶æ™ºèƒ½ç»„ç»‡å™¨\"\"\"\n",
        "\n",
        "    SUPPORTED_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}\n",
        "    NOISE_PATTERNS = [\n",
        "        r'\\[é˜²æ–­æ›´.*?\\]', r'\\[.*?å¾®.*?\\]', r'_\\d{14}',\n",
        "        r'_\\d{8}', r'_ç¬”è®°', r'\\s*\\(.*?\\)\\s*',\n",
        "    ]\n",
        "\n",
        "    def __init__(self, knowledge_base_dir, similarity_threshold=0.7, verbose=True):\n",
        "        # TODO: å®ç°åˆå§‹åŒ–\n",
        "        pass\n",
        "\n",
        "    def clean_filename(self, filename: str) -> str:\n",
        "        # TODO: å»é™¤æ–‡ä»¶åå™ªéŸ³\n",
        "        pass\n",
        "\n",
        "    def extract_sequence_number(self, filename: str) -> Tuple[int, str]:\n",
        "        # TODO: æå–åºå·\n",
        "        pass\n",
        "\n",
        "    def calculate_similarity(self, str1: str, str2: str) -> float:\n",
        "        # TODO: è®¡ç®—ç›¸ä¼¼åº¦\n",
        "        pass\n",
        "\n",
        "    def get_file_priority(self, file_path: Path) -> FilePriority:\n",
        "        # TODO: è·å–æ–‡ä»¶ä¼˜å…ˆçº§\n",
        "        pass\n",
        "\n",
        "    def create_file_info(self, file_path: Path) -> FileInfo:\n",
        "        # TODO: åˆ›å»ºæ–‡ä»¶ä¿¡æ¯\n",
        "        pass\n",
        "\n",
        "    def group_files_by_similarity(self, files: List[FileInfo]) -> Dict[str, KnowledgeGroup]:\n",
        "        # TODO: æ™ºèƒ½åˆ†ç»„(æ ¸å¿ƒç®—æ³•)\n",
        "        pass\n",
        "\n",
        "    def scan_and_organize(self) -> Dict[str, Dict[str, KnowledgeGroup]]:\n",
        "        # TODO: æ‰«æå¹¶ç»„ç»‡\n",
        "        pass\n",
        "\n",
        "print(\"âš ï¸  KnowledgeOrganizer - å¾…å®ç°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤2: æ–‡æ¡£åŠ è½½ä¸æ¸…æ´— (DocumentLoader)\n",
        "\n",
        "**åŠŸèƒ½**: åŠ è½½PDF/DOC/PPTæ–‡æ¡£,æ¸…æ´—æ–‡æœ¬å™ªéŸ³\n",
        "\n",
        "**è¾“å‡º**: `List[Document]` - LangChain Documentå¯¹è±¡åˆ—è¡¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# æ­¥éª¤2: DocumentLoader ç±»å®ç°\n",
        "# ============================================================\n",
        "\n",
        "class DocumentLoader:\n",
        "    \"\"\"å¤šæ ¼å¼æ–‡æ¡£åŠ è½½å™¨\"\"\"\n",
        "\n",
        "    def load_pdf(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PDFæ–‡æ¡£\"\"\"\n",
        "        # TODO: ä½¿ç”¨PyMuPDFLoaderåŠ è½½PDF\n",
        "        # loader = PyMuPDFLoader(str(file_path))\n",
        "        # return loader.load()\n",
        "        pass\n",
        "\n",
        "    def load_word(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½Wordæ–‡æ¡£\"\"\"\n",
        "        # TODO: ä½¿ç”¨UnstructuredWordDocumentLoader\n",
        "        pass\n",
        "\n",
        "    def load_ppt(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PPTæ–‡æ¡£\"\"\"\n",
        "        # TODO: ä½¿ç”¨UnstructuredPowerPointLoader\n",
        "        pass\n",
        "\n",
        "    def load_document(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£\"\"\"\n",
        "        # TODO: æ ¹æ®file_path.suffixé€‰æ‹©åˆé€‚çš„loader\n",
        "        suffix = file_path.suffix.lower()\n",
        "        if suffix == '.pdf':\n",
        "            return self.load_pdf(file_path)\n",
        "        elif suffix in ['.doc', '.docx']:\n",
        "            return self.load_word(file_path)\n",
        "        elif suffix in ['.ppt', '.pptx']:\n",
        "            return self.load_ppt(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {suffix}\")\n",
        "\n",
        "    def clean_document_text(self, doc: Document) -> Document:\n",
        "        \"\"\"æ¸…æ´—æ–‡æ¡£æ–‡æœ¬\"\"\"\n",
        "        # TODO: å»é™¤ç‰¹æ®Šå­—ç¬¦ã€è§„èŒƒåŒ–ç©ºç™½\n",
        "        # ä¾‹å¦‚: doc.page_content = re.sub(r'\\\\uf06c', '', doc.page_content)\n",
        "        pass\n",
        "\n",
        "    def load_and_clean(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½å¹¶æ¸…æ´—æ–‡æ¡£\"\"\"\n",
        "        # TODO:\n",
        "        # 1. docs = self.load_document(file_path)\n",
        "        # 2. cleaned_docs = [self.clean_document_text(doc) for doc in docs]\n",
        "        # 3. return cleaned_docs\n",
        "        pass\n",
        "\n",
        "print(\"âš ï¸  DocumentLoader - å¾…å®ç°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤3: ç»“æ„åŒ–çŸ¥è¯†æå– (KnowledgeExtractor)\n",
        "\n",
        "**åŠŸèƒ½**: ä½¿ç”¨LLMä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†\n",
        "\n",
        "**è¾“å‡º**: `Dict` - ç»“æ„åŒ–JSON (key_concepts, indicators, analysis_methodsç­‰)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# æ­¥éª¤3: KnowledgeExtractor ç±»å®ç°\n",
        "# ============================================================\n",
        "\n",
        "class KnowledgeExtractor:\n",
        "    \"\"\"LLMç»“æ„åŒ–çŸ¥è¯†æå–å™¨\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"deepseek-chat\"):\n",
        "        # TODO: åˆå§‹åŒ–LLMå’ŒPrompt\n",
        "        # self.llm = ChatDeepSeek(model=model_name)\n",
        "        # self.prompt = self._create_extraction_prompt()\n",
        "        pass\n",
        "\n",
        "    def _create_extraction_prompt(self) -> ChatPromptTemplate:\n",
        "        \"\"\"åˆ›å»ºæå–Prompt\"\"\"\n",
        "        # TODO: è®¾è®¡Prompt,æå–ä»¥ä¸‹å†…å®¹:\n",
        "        # - topic: ä¸»é¢˜\n",
        "        # - key_concepts: æ ¸å¿ƒæ¦‚å¿µåˆ—è¡¨ [{name, definition, importance}]\n",
        "        # - indicators: ç»æµæŒ‡æ ‡åˆ—è¡¨ [{name, calculation, interpretation}]\n",
        "        # - analysis_methods: åˆ†ææ–¹æ³•åˆ—è¡¨ [{name, steps, application}]\n",
        "        # - cases: æ¡ˆä¾‹åˆ—è¡¨ [{title, description}]\n",
        "        # - summary: æ€»ç»“\n",
        "\n",
        "        prompt_template = \"\"\"\n",
        "        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èçŸ¥è¯†æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ã€‚\n",
        "\n",
        "        æ–‡æ¡£å†…å®¹:\n",
        "        {document_content}\n",
        "\n",
        "        è¯·æå–å¹¶è¿”å›JSONæ ¼å¼:\n",
        "        {{\n",
        "            \"topic\": \"æ–‡æ¡£ä¸»é¢˜\",\n",
        "            \"key_concepts\": [\n",
        "                {{\"name\": \"æ¦‚å¿µå\", \"definition\": \"å®šä¹‰\", \"importance\": \"é‡è¦æ€§\"}}\n",
        "            ],\n",
        "            \"indicators\": [\n",
        "                {{\"name\": \"æŒ‡æ ‡å\", \"calculation\": \"è®¡ç®—æ–¹å¼\", \"interpretation\": \"è§£è¯»æ–¹æ³•\"}}\n",
        "            ],\n",
        "            \"analysis_methods\": [\n",
        "                {{\"name\": \"æ–¹æ³•å\", \"steps\": \"æ­¥éª¤\", \"application\": \"åº”ç”¨åœºæ™¯\"}}\n",
        "            ],\n",
        "            \"cases\": [\n",
        "                {{\"title\": \"æ¡ˆä¾‹æ ‡é¢˜\", \"description\": \"æè¿°\"}}\n",
        "            ],\n",
        "            \"summary\": \"æ€»ç»“\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "        # TODO: è¿”å›ChatPromptTemplate.from_template(prompt_template)\n",
        "        pass\n",
        "\n",
        "    def extract_from_documents(self, docs: List[Document], topic: str) -> Dict:\n",
        "        \"\"\"ä»æ–‡æ¡£åˆ—è¡¨ä¸­æå–çŸ¥è¯†\"\"\"\n",
        "        # TODO:\n",
        "        # 1. åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹: content = \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\n",
        "        # 2. è°ƒç”¨LLM: chain = self.prompt | self.llm | JsonOutputParser()\n",
        "        # 3. result = chain.invoke({\"document_content\": content})\n",
        "        # 4. éªŒè¯: validate_extracted_knowledge(result)\n",
        "        # 5. return result\n",
        "        pass\n",
        "\n",
        "    def validate_extracted_knowledge(self, knowledge: Dict) -> bool:\n",
        "        \"\"\"éªŒè¯æå–çš„çŸ¥è¯†å®Œæ•´æ€§\"\"\"\n",
        "        # TODO: æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨\n",
        "        # required_fields = [\"topic\", \"key_concepts\", \"summary\"]\n",
        "        pass\n",
        "\n",
        "print(\"âš ï¸  KnowledgeExtractor - å¾…å®ç°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤4: å‘é‡åŒ–å­˜å‚¨ (VectorStoreManager)\n",
        "\n",
        "**åŠŸèƒ½**: æ–‡æ¡£åˆ‡å—ã€å‘é‡åŒ–ã€å­˜å…¥ChromaDB\n",
        "\n",
        "**è¾“å‡º**: æŒä¹…åŒ–å‘é‡åº“ `vector_db/{domain}/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# æ­¥éª¤4: VectorStoreManager ç±»å®ç°\n",
        "# ============================================================\n",
        "\n",
        "class VectorStoreManager:\n",
        "    \"\"\"å‘é‡å­˜å‚¨ç®¡ç†å™¨\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        persist_directory=\"./vector_db\"\n",
        "    ):\n",
        "        # TODO: åˆå§‹åŒ–embeddingså’Œpersist_directory\n",
        "        # self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "        # self.persist_directory = Path(persist_directory)\n",
        "        # self.vector_stores: Dict[str, Chroma] = {}\n",
        "        pass\n",
        "\n",
        "    def create_vector_store(self, domain: str) -> Chroma:\n",
        "        \"\"\"ä¸ºç‰¹å®šdomainåˆ›å»ºå‘é‡åº“\"\"\"\n",
        "        # TODO:\n",
        "        # collection_name = f\"{domain}_collection\"\n",
        "        # persist_path = str(self.persist_directory / domain)\n",
        "        # vectorstore = Chroma(\n",
        "        #     collection_name=collection_name,\n",
        "        #     embedding_function=self.embeddings,\n",
        "        #     persist_directory=persist_path\n",
        "        # )\n",
        "        # self.vector_stores[domain] = vectorstore\n",
        "        # return vectorstore\n",
        "        pass\n",
        "\n",
        "    def split_documents(\n",
        "        self,\n",
        "        docs: List[Document],\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"åˆ‡åˆ†æ–‡æ¡£\"\"\"\n",
        "        # TODO:\n",
        "        # text_splitter = RecursiveCharacterTextSplitter(\n",
        "        #     chunk_size=chunk_size,\n",
        "        #     chunk_overlap=chunk_overlap,\n",
        "        #     add_start_index=True\n",
        "        # )\n",
        "        # return text_splitter.split_documents(docs)\n",
        "        pass\n",
        "\n",
        "    def add_documents_to_store(\n",
        "        self,\n",
        "        domain: str,\n",
        "        docs: List[Document],\n",
        "        metadata: Dict = None\n",
        "    ):\n",
        "        \"\"\"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“\"\"\"\n",
        "        # TODO:\n",
        "        # 1. åˆ‡åˆ†: chunks = self.split_documents(docs)\n",
        "        # 2. æ·»åŠ metadataåˆ°æ¯ä¸ªchunk\n",
        "        # 3. è·å–/åˆ›å»ºvectorstore\n",
        "        # 4. vectorstore.add_documents(chunks)\n",
        "        pass\n",
        "\n",
        "    def search_similar(\n",
        "        self,\n",
        "        domain: str,\n",
        "        query: str,\n",
        "        k=5\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"è¯­ä¹‰æ£€ç´¢\"\"\"\n",
        "        # TODO:\n",
        "        # vectorstore = self.vector_stores.get(domain)\n",
        "        # return vectorstore.similarity_search(query, k=k)\n",
        "        pass\n",
        "\n",
        "print(\"âš ï¸  VectorStoreManager - å¾…å®ç°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤5: å®Œæ•´Pipeline (KnowledgeProcessor)\n",
        "\n",
        "**åŠŸèƒ½**: åè°ƒæ‰€æœ‰æ¨¡å—,å®Œæˆå®Œæ•´çŸ¥è¯†å¤„ç†æµç¨‹\n",
        "\n",
        "**è¾“å‡º**: \n",
        "- `/memories/knowledge/{domain}/structured.json`\n",
        "- `vector_db/{domain}/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# æ­¥éª¤5: KnowledgeProcessor ç±»å®ç°\n",
        "# ============================================================\n",
        "\n",
        "class KnowledgeProcessor:\n",
        "    \"\"\"çŸ¥è¯†å¤„ç†Pipelineåè°ƒå™¨\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        knowledge_base_dir: str,\n",
        "        memories_dir=\"./memories/knowledge\",\n",
        "        vector_db_dir=\"./vector_db\"\n",
        "    ):\n",
        "        # TODO: åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶\n",
        "        # self.organizer = KnowledgeOrganizer(knowledge_base_dir)\n",
        "        # self.loader = DocumentLoader()\n",
        "        # self.extractor = KnowledgeExtractor()\n",
        "        # self.vector_manager = VectorStoreManager(persist_directory=vector_db_dir)\n",
        "        # self.memories_dir = Path(memories_dir)\n",
        "        pass\n",
        "\n",
        "    def save_to_memories(self, domain: str, knowledge: Dict):\n",
        "        \"\"\"ä¿å­˜ç»“æ„åŒ–çŸ¥è¯†åˆ°/memories/\"\"\"\n",
        "        # TODO:\n",
        "        # 1. åˆ›å»ºç›®å½•: (self.memories_dir / domain).mkdir(parents=True, exist_ok=True)\n",
        "        # 2. ä¿å­˜JSON: json.dump(knowledge, f, ensure_ascii=False, indent=2)\n",
        "        # 3. å¯é€‰: ç”Ÿæˆmarkdownæ‘˜è¦\n",
        "        pass\n",
        "\n",
        "    def process_all(self):\n",
        "        \"\"\"å¤„ç†æ‰€æœ‰çŸ¥è¯†å—(ä¸»æ–¹æ³•)\"\"\"\n",
        "        # TODO: å®Œæ•´æµç¨‹\n",
        "        # 1. organized = self.organizer.scan_and_organize()\n",
        "        # 2. for domain, groups in organized.items():\n",
        "        #     3. for group_key, group in groups.items():\n",
        "        #         4. primary_file = group.primary_file.path\n",
        "        #         5. docs = self.loader.load_and_clean(primary_file)\n",
        "        #         6. knowledge = self.extractor.extract_from_documents(docs, group.topic)\n",
        "        #         7. self.save_to_memories(domain, knowledge)\n",
        "        #         8. self.vector_manager.add_documents_to_store(domain, docs, metadata={...})\n",
        "        #     9. printè¿›åº¦\n",
        "        pass\n",
        "\n",
        "print(\"âš ï¸  KnowledgeProcessor - å¾…å®ç°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

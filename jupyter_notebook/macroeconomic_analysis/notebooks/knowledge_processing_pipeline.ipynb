{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# çŸ¥è¯†å¤„ç†Pipeline - ä¸“å®¶çº§å®ç°\n",
        "\n",
        "**æ–¹æ¡ˆ**: å‘é‡åº“+JSONåŒå­˜å‚¨  \n",
        "**æŠ€æœ¯æ ˆ**: Qwen3-Embedding + deepseek-v3 + Chroma  \n",
        "**è´¨é‡æ ‡å‡†**: éµå¾ª[AIè¡Œä¸ºçº¦æŸè§„èŒƒ](../../docs/AIè¡Œä¸ºçº¦æŸè§„èŒƒ.md)\n",
        "\n",
        "## æµç¨‹\n",
        "\n",
        "```\n",
        "PDF/Word/PPTæ–‡ä»¶ï¼ˆ51ä¸ªï¼‰\n",
        " â†“ [æ¨¡å—1] KnowledgeOrganizer\n",
        "17ä¸ªä¸»é¢˜ç»„\n",
        " â†“ [æ¨¡å—2] DocumentLoader  \n",
        "List[Document]\n",
        " â†“ [æ¨¡å—3] KnowledgeExtractor (deepseek-v3)\n",
        "ç»“æ„åŒ–JSON\n",
        " â†“ [æ¨¡å—4] VectorStoreManager (Qwen3-Embedding)\n",
        "Chromaå‘é‡åº“\n",
        " â†“ [æ¨¡å—5] KnowledgeProcessor\n",
        "å®Œæ•´çŸ¥è¯†åº“\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ‰€æœ‰ä¾èµ–å¯¼å…¥å®Œæˆ\n",
            "ä¾èµ–å¯¼å…¥å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ========== ç¯å¢ƒå‡†å¤‡ ==========\n",
        "\n",
        "# æ ‡å‡†åº“\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from dataclasses import dataclass\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# LangChain - Document Loaders\n",
        "from langchain_community.document_loaders import (\n",
        "    PyMuPDFLoader,           # PDFåŠ è½½ï¼ˆæ¨èï¼‰\n",
        "    Docx2txtLoader,          # WordåŠ è½½\n",
        "    UnstructuredPowerPointLoader  # PPTåŠ è½½\n",
        ")\n",
        "\n",
        "# LangChain - Core\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# LangChain - Embeddings & VectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# LangChain - LLMï¼ˆæŠ€æœ¯å†³ç­–ï¼šdeepseek-v3ï¼‰\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "# è¿›åº¦æ˜¾ç¤º\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# æ—¥å¿—\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"æ‰€æœ‰ä¾èµ–å¯¼å…¥å®Œæˆ\")\n",
        "\n",
        "# ç¯å¢ƒå˜é‡\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"../../../config/.env\")\n",
        "\n",
        "print(\"ä¾èµ–å¯¼å…¥å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## é…ç½®å‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:53:52,852 - INFO - çŸ¥è¯†åº“è·¯å¾„: ../knowledge_base\n",
            "2025-12-03 21:53:52,854 - INFO - è¾“å‡ºè·¯å¾„: output\n",
            "2025-12-03 21:53:52,856 - INFO - Embedding: Qwen/Qwen3-Embedding-0.6B\n",
            "2025-12-03 21:53:52,857 - INFO - LLM: deepseek-reasoner\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é…ç½®å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ========== å…¨å±€é…ç½® ==========\n",
        "\n",
        "# è·¯å¾„é…ç½®\n",
        "KNOWLEDGE_BASE_DIR = Path(\"../knowledge_base\")\n",
        "OUTPUT_DIR = Path(\"./output\")\n",
        "VECTOR_DB_DIR = OUTPUT_DIR / \"vector_db\"\n",
        "STRUCTURED_JSON_DIR = OUTPUT_DIR / \"structured_knowledge\"\n",
        "\n",
        "# æ¨¡å‹é…ç½®ï¼ˆæŒ‰æŠ€æœ¯å†³ç­–ï¼‰\n",
        "# Embeddingæ¨¡å‹ï¼šHuggingFaceè‡ªåŠ¨ç®¡ç†ç¼“å­˜\n",
        "EMBEDDING_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "\n",
        "LLM_MODEL = \"deepseek-reasoner\"  # å†³ç­–#005\n",
        "LLM_TEMPERATURE = 0  # ç¡®ä¿è¾“å‡ºç¨³å®šæ€§\n",
        "\n",
        "# æ–‡æœ¬åˆ†å‰²é…ç½®\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "VECTOR_DB_DIR.mkdir(exist_ok=True)\n",
        "STRUCTURED_JSON_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "logger.info(f\"çŸ¥è¯†åº“è·¯å¾„: {KNOWLEDGE_BASE_DIR}\")\n",
        "logger.info(f\"è¾“å‡ºè·¯å¾„: {OUTPUT_DIR}\")\n",
        "logger.info(f\"Embedding: {EMBEDDING_MODEL}\")\n",
        "logger.info(f\"LLM: {LLM_MODEL}\")\n",
        "\n",
        "print(\"é…ç½®å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ•°æ®ç»“æ„å®šä¹‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ========== æ•°æ®ç»“æ„å®šä¹‰ï¼ˆç¬¦åˆä¸“å®¶æ ‡å‡†ï¼‰==========\n",
        "\n",
        "from enum import IntEnum\n",
        "\n",
        "class FilePriority(IntEnum):\n",
        "    \"\"\"æ–‡ä»¶ä¼˜å…ˆçº§æšä¸¾\n",
        "\n",
        "    ä¼˜å…ˆçº§è§„åˆ™ï¼š\n",
        "    - PDFç¬”è®°æœ€ä¼˜å…ˆï¼ˆæœ€è¯¦ç»†ï¼‰\n",
        "    - Wordæ–‡æ¡£æ¬¡ä¹‹\n",
        "    - æ™®é€šPDFç¬¬ä¸‰\n",
        "    - PPTæœ€åï¼ˆä¿¡æ¯å¯†åº¦ä½ï¼‰\n",
        "    \"\"\"\n",
        "    PDF_NOTE = 1      # PDFç¬”è®°æ–‡ä»¶ï¼ˆæ–‡ä»¶ååŒ…å«\"ç¬”è®°\"ï¼‰\n",
        "    WORD_DOC = 2      # Wordæ–‡æ¡£\n",
        "    PDF_REGULAR = 3   # æ™®é€šPDFæ–‡ä»¶\n",
        "    POWERPOINT = 4    # PowerPointæ–‡ä»¶\n",
        "    UNKNOWN = 99      # æœªçŸ¥ç±»å‹\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"æ–‡ä»¶ä¿¡æ¯æ•°æ®ç±»\n",
        "\n",
        "    Attributes:\n",
        "        path: æ–‡ä»¶å®Œæ•´è·¯å¾„\n",
        "        original_name: åŸå§‹æ–‡ä»¶å\n",
        "        cleaned_name: æ¸…æ´—åçš„æ–‡ä»¶åï¼ˆå»é™¤å™ªéŸ³ï¼‰\n",
        "        sequence: åºå·ï¼ˆæ•´æ•°ï¼‰\n",
        "        sequence_str: åºå·å­—ç¬¦ä¸²\n",
        "        priority: æ–‡ä»¶ä¼˜å…ˆçº§\n",
        "    \"\"\"\n",
        "    path: Path\n",
        "    original_name: str\n",
        "    cleaned_name: str\n",
        "    sequence: int\n",
        "    sequence_str: str\n",
        "    priority: FilePriority\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeGroup:\n",
        "    \"\"\"çŸ¥è¯†ç»„æ•°æ®ç±»\n",
        "\n",
        "    Attributes:\n",
        "        group_key: ç»„å”¯ä¸€æ ‡è¯†\n",
        "        topic: ä¸»é¢˜åç§°\n",
        "        sequence: ä¸»é¢˜åºå·\n",
        "        files: ç»„å†…æ‰€æœ‰æ–‡ä»¶\n",
        "        primary_file: ä¸»è¦æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰\n",
        "        file_types: æ–‡ä»¶ç±»å‹åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    group_key: str\n",
        "    topic: str\n",
        "    sequence: int\n",
        "    files: List[FileInfo]\n",
        "    primary_file: FileInfo\n",
        "    file_types: List[str]\n",
        "\n",
        "\n",
        "print(\"æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤1: KnowledgeOrganizer - æ–‡ä»¶æ‰«æåˆ†ç»„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­¥éª¤1å®Œæˆï¼šKnowledgeOrganizer\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeOrganizer:\n",
        "    \"\"\"çŸ¥è¯†æ–‡ä»¶æ‰«æå’Œæ™ºèƒ½åˆ†ç»„\n",
        "\n",
        "    åŠŸèƒ½:\n",
        "    1. æ‰«æçŸ¥è¯†åº“ç›®å½•,æ”¯æŒPDF/Word/PPT\n",
        "    2. æ¸…æ´—æ–‡ä»¶å(å»é™¤æ—¶é—´æˆ³ã€å™ªéŸ³æ ‡è®°ç­‰)\n",
        "    3. æŒ‰ç›¸ä¼¼åº¦æ™ºèƒ½åˆ†ç»„(åŒä¸»é¢˜çš„ä¸åŒæ–‡ä»¶ç±»å‹)\n",
        "    4. æŒ‰ä¼˜å…ˆçº§æ’åº(PDFç¬”è®° > Word > PDF > PPT)\n",
        "\n",
        "    åˆ†ç»„ç®—æ³•:\n",
        "    - æå–åºå·(å¦‚\"01ç¬¬ä¸€èŠ‚\")ä½œä¸ºä¸»é”®\n",
        "    - è®¡ç®—æ–‡ä»¶åç›¸ä¼¼åº¦\n",
        "    - ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„å½’ä¸ºä¸€ç»„\n",
        "    \"\"\"\n",
        "\n",
        "    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å\n",
        "    SUPPORTED_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}\n",
        "\n",
        "    # æ–‡ä»¶åå™ªéŸ³æ¨¡å¼(æ­£åˆ™è¡¨è¾¾å¼)\n",
        "    NOISE_PATTERNS = [\n",
        "        r'\\[é˜²æ–­æ›´.*?\\]',  # [é˜²æ–­æ›´å¾®coc36666]\n",
        "        r'\\[.*?å¾®.*?\\]',   # [å¾®ä¿¡å·xxx]\n",
        "        r'_\\d{14}',        # _20250706193405\n",
        "        r'_\\d{8}',         # _20250706\n",
        "        r'_ç¬”è®°',          # _ç¬”è®°\n",
        "        r'\\s*\\(.*?\\)\\s*'   # (å¤‡æ³¨)\n",
        "    ]\n",
        "\n",
        "    def __init__(self,\n",
        "                 knowledge_base_dir: str,\n",
        "                 similarity_threshold: float = 0.7,\n",
        "                 verbose: bool = True):\n",
        "        \"\"\"åˆå§‹åŒ–çŸ¥è¯†ç»„ç»‡å™¨\n",
        "\n",
        "        Args:\n",
        "            knowledge_base_dir: çŸ¥è¯†åº“æ ¹ç›®å½•\n",
        "            similarity_threshold: æ–‡ä»¶åç›¸ä¼¼åº¦é˜ˆå€¼(0-1),é»˜è®¤0.7\n",
        "            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—\n",
        "\n",
        "        Raises:\n",
        "            ValueError: ç›®å½•ä¸å­˜åœ¨æ—¶æŠ›å‡º\n",
        "        \"\"\"\n",
        "        self.knowledge_base_dir = Path(knowledge_base_dir)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.verbose = verbose\n",
        "        if not self.knowledge_base_dir.exists():\n",
        "            raise ValueError(f\"ç›®å½•ä¸å­˜åœ¨: {self.knowledge_base_dir}\")\n",
        "\n",
        "    def _log(self, msg):\n",
        "        if self.verbose: print(msg)\n",
        "\n",
        "    def clean_filename(self, filename: str) -> str:\n",
        "        \"\"\"æ¸…æ´—æ–‡ä»¶åï¼Œå»é™¤æ—¶é—´æˆ³å’Œå™ªéŸ³æ ‡è®°\n",
        "\n",
        "        Args:\n",
        "            filename: åŸå§‹æ–‡ä»¶åï¼ˆå«æ‰©å±•åï¼‰\n",
        "\n",
        "        Returns:\n",
        "            æ¸…æ´—åçš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰\n",
        "\n",
        "        Example:\n",
        "            >>> clean_filename(\"01æŠ¥å‘Š_20231201[é˜²æ–­æ›´].pdf\")\n",
        "            \"01æŠ¥å‘Š\"\n",
        "        \"\"\"\n",
        "        name = Path(filename).stem\n",
        "        for pattern in self.NOISE_PATTERNS:\n",
        "            name = re.sub(pattern, '', name)\n",
        "        return re.sub(r'\\s+', ' ', name).strip()\n",
        "\n",
        "    def extract_sequence_number(self, filename: str) -> Tuple[int, str]:\n",
        "        \"\"\"æå–æ–‡ä»¶åå¼€å¤´çš„åºå·\n",
        "\n",
        "        Args:\n",
        "            filename: æ–‡ä»¶å\n",
        "\n",
        "        Returns:\n",
        "            (åºå·æ•´æ•°, åºå·å­—ç¬¦ä¸²)ï¼Œå¦‚(1, \"01\")æˆ–(999999, \"\")è¡¨ç¤ºæ— åºå·\n",
        "\n",
        "        Example:\n",
        "            >>> extract_sequence_number(\"01æŠ¥å‘Š\")\n",
        "            (1, \"01\")\n",
        "        \"\"\"\n",
        "        match = re.match(r'^(\\d+)', filename)\n",
        "        return (int(match.group(1)), match.group(1)) if match else (999999, \"\")\n",
        "\n",
        "    def calculate_similarity(self, str1: str, str2: str) -> float:\n",
        "        \"\"\"è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦\n",
        "\n",
        "        Args:\n",
        "            str1: å­—ç¬¦ä¸²1\n",
        "            str2: å­—ç¬¦ä¸²2\n",
        "\n",
        "        Returns:\n",
        "            ç›¸ä¼¼åº¦åˆ†æ•°(0-1)ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ\n",
        "        \"\"\"\n",
        "        return SequenceMatcher(None, str1, str2).ratio()\n",
        "\n",
        "    def get_file_priority(self, file_path: Path) -> FilePriority:\n",
        "        \"\"\"æ ¹æ®æ–‡ä»¶ç±»å‹å’Œåç§°ç¡®å®šä¼˜å…ˆçº§\n",
        "\n",
        "        Args:\n",
        "            file_path: æ–‡ä»¶è·¯å¾„å¯¹è±¡\n",
        "\n",
        "        Returns:\n",
        "            FilePriorityæšä¸¾å€¼\n",
        "\n",
        "        Note:\n",
        "            ä¼˜å…ˆçº§ï¼šPDFç¬”è®° > Word > PDF > PPT > æœªçŸ¥\n",
        "        \"\"\"\n",
        "        name, suffix = file_path.name.lower(), file_path.suffix.lower()\n",
        "        if 'ç¬”è®°' in name and suffix == '.pdf': return FilePriority.PDF_NOTE\n",
        "        if suffix in ['.doc', '.docx']: return FilePriority.WORD_DOC\n",
        "        if suffix == '.pdf': return FilePriority.PDF_REGULAR\n",
        "        if suffix in ['.ppt', '.pptx']: return FilePriority.POWERPOINT\n",
        "        return FilePriority.UNKNOWN\n",
        "\n",
        "    def create_file_info(self, file_path: Path) -> FileInfo:\n",
        "        \"\"\"ä¸ºå•ä¸ªæ–‡ä»¶åˆ›å»ºä¿¡æ¯å¯¹è±¡\n",
        "\n",
        "        Args:\n",
        "            file_path: æ–‡ä»¶è·¯å¾„\n",
        "\n",
        "        Returns:\n",
        "            FileInfoå¯¹è±¡ï¼ŒåŒ…å«åŸå§‹åã€æ¸…æ´—åã€åºå·ã€ä¼˜å…ˆçº§ç­‰\n",
        "        \"\"\"\n",
        "        original_name = file_path.name\n",
        "        cleaned_name = self.clean_filename(original_name)\n",
        "        sequence, sequence_str = self.extract_sequence_number(cleaned_name)\n",
        "        priority = self.get_file_priority(file_path)\n",
        "        return FileInfo(file_path, original_name, cleaned_name, sequence, sequence_str, priority)\n",
        "\n",
        "    def group_files_by_similarity(self, files: List[FileInfo]) -> Dict[str, KnowledgeGroup]:\n",
        "        \"\"\"æŒ‰åºå·å’Œç›¸ä¼¼åº¦æ™ºèƒ½åˆ†ç»„æ–‡ä»¶\n",
        "\n",
        "        Args:\n",
        "            files: FileInfoå¯¹è±¡åˆ—è¡¨\n",
        "\n",
        "        Returns:\n",
        "            å­—å…¸{group_key: KnowledgeGroup}ï¼Œæ¯ç»„åŒ…å«ç›¸ä¼¼çš„æ–‡ä»¶\n",
        "\n",
        "        Note:\n",
        "            åŒåºå·ä¸”ç›¸ä¼¼åº¦>=thresholdçš„æ–‡ä»¶å½’ä¸ºä¸€ç»„\n",
        "        \"\"\"\n",
        "        groups, processed = {}, set()\n",
        "        for i, file1 in enumerate(files):\n",
        "            if file1.path in processed: continue\n",
        "            group_key = f\"{file1.sequence_str}_{file1.cleaned_name[:20]}\"\n",
        "            group_files = [file1]\n",
        "            processed.add(file1.path)\n",
        "\n",
        "            for file2 in files[i+1:]:\n",
        "                if file2.path in processed: continue\n",
        "                if file1.sequence == file2.sequence:\n",
        "                    if self.calculate_similarity(file1.cleaned_name, file2.cleaned_name) >= self.similarity_threshold:\n",
        "                        group_files.append(file2)\n",
        "                        processed.add(file2.path)\n",
        "\n",
        "            group_files.sort(key=lambda f: (f.priority.value, f.original_name))\n",
        "            groups[group_key] = KnowledgeGroup(group_key, file1.cleaned_name, file1.sequence,\n",
        "                                              group_files, group_files[0], [f.path.suffix for f in group_files])\n",
        "            self._log(f\"[å®Œæˆ] {file1.cleaned_name[:30]} ({len(group_files)}æ–‡ä»¶)\")\n",
        "        return groups\n",
        "\n",
        "    def scan_and_organize(self) -> Dict[str, Dict[str, KnowledgeGroup]]:\n",
        "        \"\"\"æ‰«æçŸ¥è¯†åº“ç›®å½•å¹¶æ™ºèƒ½åˆ†ç»„\n",
        "\n",
        "        Returns:\n",
        "            å­—å…¸{domain: {group_key: KnowledgeGroup}}\n",
        "\n",
        "        Example:\n",
        "            >>> result = organizer.scan_and_organize()\n",
        "            >>> # {'macroeconomic': {'01_ä¸­å›½ç»æµ': KnowledgeGroup(...)}}\n",
        "        \"\"\"\n",
        "        self._log(f\"æ‰«æç›®å½•: {self.knowledge_base_dir}\")\n",
        "        all_files = []\n",
        "        for ext in self.SUPPORTED_EXTENSIONS:\n",
        "            all_files.extend(self.knowledge_base_dir.glob(f\"*{ext}\"))\n",
        "\n",
        "        if not all_files:\n",
        "            self._log(\"[è­¦å‘Š] æœªæ‰¾åˆ°æ–‡ä»¶\")\n",
        "            return {}\n",
        "\n",
        "        self._log(f\"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶\")\n",
        "        file_infos = [self.create_file_info(f) for f in all_files]\n",
        "        groups = self.group_files_by_similarity(file_infos)\n",
        "        sorted_groups = dict(sorted(groups.items(), key=lambda x: x[1].sequence))\n",
        "        self._log(f\"[å®Œæˆ] {len(sorted_groups)} ä¸ªçŸ¥è¯†å—\\n\")\n",
        "        return {self.knowledge_base_dir.name: sorted_groups}\n",
        "\n",
        "print(\"æ­¥éª¤1å®Œæˆï¼šKnowledgeOrganizer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­¥éª¤2å®Œæˆï¼šDocumentLoader\n"
          ]
        }
      ],
      "source": [
        "class DocumentLoader:\n",
        "    \"\"\"æ–‡æ¡£åŠ è½½å™¨\n",
        "\n",
        "    æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼åŠ è½½:\n",
        "    - PDF: PyMuPDFLoader (æ¨è,æ€§èƒ½æœ€ä½³)\n",
        "    - Word: Docx2txtLoader (.doc, .docx)\n",
        "    - PowerPoint: UnstructuredPowerPointLoader (.ppt, .pptx)\n",
        "    \"\"\"\n",
        "\n",
        "    def load_pdf(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PDFæ–‡ä»¶\n",
        "\n",
        "        Args:\n",
        "            file_path: PDFæ–‡ä»¶è·¯å¾„\n",
        "\n",
        "        Returns:\n",
        "            æ–‡æ¡£åˆ—è¡¨(æ¯é¡µä¸€ä¸ªDocument)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loader = PyMuPDFLoader(str(file_path))\n",
        "            return loader.load()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDFåŠ è½½å¤±è´¥ {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def load_word(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½Wordæ–‡ä»¶\n",
        "\n",
        "        Args:\n",
        "            file_path: Wordæ–‡ä»¶è·¯å¾„\n",
        "\n",
        "        Returns:\n",
        "            æ–‡æ¡£åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loader = Docx2txtLoader(str(file_path))\n",
        "            return loader.load()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"WordåŠ è½½å¤±è´¥ {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def load_ppt(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½PowerPointæ–‡ä»¶\n",
        "\n",
        "        Args:\n",
        "            file_path: PPTæ–‡ä»¶è·¯å¾„\n",
        "\n",
        "        Returns:\n",
        "            æ–‡æ¡£åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loader = UnstructuredPowerPointLoader(str(file_path))\n",
        "            return loader.load()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PPTåŠ è½½å¤±è´¥ {file_path.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def clean_document_text(self, doc: Document) -> Document:\n",
        "        \"\"\"æ¸…æ´—æ–‡æ¡£æ–‡æœ¬\n",
        "\n",
        "        æ¸…ç†å†…å®¹:\n",
        "        - ç‰¹æ®Šå­—ç¬¦\n",
        "        - å¤šä½™ç©ºç™½\n",
        "        - å™ªéŸ³å†…å®¹\n",
        "\n",
        "        Args:\n",
        "            doc: åŸå§‹æ–‡æ¡£\n",
        "\n",
        "        Returns:\n",
        "            æ¸…æ´—åçš„æ–‡æ¡£\n",
        "        \"\"\"\n",
        "        text = doc.page_content\n",
        "        text = re.sub(r'[\\uf06c\\uf0fc]', '', text)  # ç‰¹æ®Šå­—ç¬¦\n",
        "        text = re.sub(r'\\s+', ' ', text)  # å¤šä½™ç©ºç™½\n",
        "        doc.page_content = text.strip()\n",
        "        return doc\n",
        "\n",
        "    def load_and_clean(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"åŠ è½½å¹¶æ¸…æ´—æ–‡æ¡£(ç»Ÿä¸€å…¥å£)\n",
        "\n",
        "        Args:\n",
        "            file_path: æ–‡ä»¶è·¯å¾„\n",
        "\n",
        "        Returns:\n",
        "            æ¸…æ´—åçš„æ–‡æ¡£åˆ—è¡¨\n",
        "        \"\"\"\n",
        "        suffix = file_path.suffix.lower()\n",
        "\n",
        "        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨\n",
        "        if suffix == '.pdf':\n",
        "            docs = self.load_pdf(file_path)\n",
        "        elif suffix in ['.doc', '.docx']:\n",
        "            docs = self.load_word(file_path)\n",
        "        elif suffix in ['.ppt', '.pptx']:\n",
        "            docs = self.load_ppt(file_path)\n",
        "        else:\n",
        "            logger.warning(f\"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {suffix}\")\n",
        "            return []\n",
        "\n",
        "        # æ¸…æ´—æ–‡æ¡£\n",
        "        if docs:\n",
        "            docs = [self.clean_document_text(doc) for doc in docs]\n",
        "            logger.info(f\"åŠ è½½ {file_path.name}: {len(docs)}é¡µ\")\n",
        "\n",
        "        return docs\n",
        "\n",
        "print(\"æ­¥éª¤2å®Œæˆï¼šDocumentLoader\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤3: KnowledgeExtractor - LLMç»“æ„åŒ–æå–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­¥éª¤3å®Œæˆï¼šKnowledgeExtractor\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeExtractor:\n",
        "    \"\"\"LLMçŸ¥è¯†æå–å™¨\n",
        "\n",
        "    ä½¿ç”¨LLMä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†(JSONæ ¼å¼)\n",
        "    æŠ€æœ¯å†³ç­–: ä½¿ç”¨deepseek-v3æ¨¡å‹è¿›è¡ŒçŸ¥è¯†æå–\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = LLM_MODEL, temperature: float = LLM_TEMPERATURE):\n",
        "        \"\"\"åˆå§‹åŒ–çŸ¥è¯†æå–å™¨\n",
        "\n",
        "        Args:\n",
        "            model_name: LLMæ¨¡å‹åç§°,é»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®\n",
        "            temperature: æ¸©åº¦å‚æ•°,é»˜è®¤0ç¡®ä¿è¾“å‡ºç¨³å®šæ€§\n",
        "        \"\"\"\n",
        "        self.llm = ChatDeepSeek(model=model_name, temperature=temperature)\n",
        "        logger.info(f\"LLMåˆå§‹åŒ–: {model_name}\")\n",
        "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ä½ æ˜¯é‡‘èçŸ¥è¯†æå–ä¸“å®¶ã€‚ä»æ–‡æ¡£æå–ç»“æ„åŒ–çŸ¥è¯†,è¿”å›JSONã€‚\n",
        "\n",
        "æ–‡æ¡£: {content}\n",
        "\n",
        "æå–JSON(åªè¿”å›JSON):\n",
        "{{\n",
        "  \"topic\": \"ä¸»é¢˜\",\n",
        "  \"key_concepts\": [{{\"name\": \"æ¦‚å¿µ\", \"definition\": \"å®šä¹‰\", \"importance\": \"é‡è¦æ€§\"}}],\n",
        "  \"indicators\": [{{\"name\": \"æŒ‡æ ‡\", \"calculation\": \"è®¡ç®—\", \"interpretation\": \"è§£è¯»\"}}],\n",
        "  \"analysis_methods\": [{{\"name\": \"æ–¹æ³•\", \"steps\": \"æ­¥éª¤\", \"application\": \"åº”ç”¨\"}}],\n",
        "  \"summary\": \"æ€»ç»“\"\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    def extract_from_documents(self, docs: List[Document], topic: str) -> Dict:\n",
        "        \"\"\"ä»æ–‡æ¡£åˆ—è¡¨æå–ç»“æ„åŒ–çŸ¥è¯†\n",
        "\n",
        "        Args:\n",
        "            docs: LangChain Documentåˆ—è¡¨\n",
        "            topic: çŸ¥è¯†ä¸»é¢˜\n",
        "\n",
        "        Returns:\n",
        "            ç»“æ„åŒ–çŸ¥è¯†å­—å…¸ï¼ŒåŒ…å«topic/key_concepts/indicators/analysis_methods/summary\n",
        "\n",
        "        Note:\n",
        "            - åªä½¿ç”¨å‰5é¡µå†…å®¹ï¼ˆé¿å…tokenè¶…é™ï¼‰\n",
        "            - æˆªæ–­åˆ°15000å­—ç¬¦\n",
        "            - å¤±è´¥æ—¶è¿”å›ç©ºç»“æ„\n",
        "\n",
        "        Example:\n",
        "            >>> knowledge = extractor.extract_from_documents(docs, \"GDPåˆ†æ\")\n",
        "            >>> knowledge['topic']\n",
        "            'GDPåˆ†æ'\n",
        "        \"\"\"\n",
        "        content = \"\\n\\n\".join([d.page_content for d in docs[:5]])[:15000]\n",
        "        try:\n",
        "            chain = self.prompt | self.llm | JsonOutputParser()\n",
        "            result = chain.invoke({\"content\": content})\n",
        "            result[\"topic\"] = topic\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"[è­¦å‘Š] LLMæå–å¤±è´¥: {e}\")\n",
        "            return {\"topic\": topic, \"key_concepts\": [], \"indicators\": [],\n",
        "                   \"analysis_methods\": [], \"summary\": \"æå–å¤±è´¥\"}\n",
        "\n",
        "print(\"æ­¥éª¤3å®Œæˆï¼šKnowledgeExtractor\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤4: VectorStoreManager - å‘é‡åŒ–å­˜å‚¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­¥éª¤4å®Œæˆï¼šVectorStoreManager\n"
          ]
        }
      ],
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"å‘é‡å­˜å‚¨ç®¡ç†å™¨\n",
        "\n",
        "    è´Ÿè´£æ–‡æ¡£å‘é‡åŒ–å’ŒChromaå‘é‡æ•°æ®åº“ç®¡ç†\n",
        "    æŠ€æœ¯å†³ç­–: ä½¿ç”¨Qwen3-Embedding-0.6Bæ¨¡å‹è¿›è¡Œå‘é‡åŒ–\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_model: str = EMBEDDING_MODEL,\n",
        "                 persist_directory: str = str(VECTOR_DB_DIR)):\n",
        "        \"\"\"åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Embeddingæ¨¡å‹åç§°,é»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®\n",
        "            persist_directory: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•\n",
        "        \"\"\"\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "        self.persist_directory = Path(persist_directory)\n",
        "        logger.info(f\"Embeddingåˆå§‹åŒ–: {embedding_model}\")\n",
        "        self.vector_stores = {}\n",
        "\n",
        "    def get_or_create_store(self, domain: str) -> Chroma:\n",
        "        \"\"\"è·å–å·²æœ‰å‘é‡å­˜å‚¨æˆ–åˆ›å»ºæ–°çš„\n",
        "\n",
        "        Args:\n",
        "            domain: é¢†åŸŸåç§°ï¼ˆç”¨ä½œcollectionå‘½åï¼‰\n",
        "\n",
        "        Returns:\n",
        "            Chromaå‘é‡å­˜å‚¨å¯¹è±¡\n",
        "\n",
        "        Note:\n",
        "            æ¯ä¸ªdomainå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„collection\n",
        "        \"\"\"\n",
        "        if domain in self.vector_stores:\n",
        "            return self.vector_stores[domain]\n",
        "\n",
        "        persist_path = str(self.persist_directory / domain)\n",
        "        store = Chroma(\n",
        "            collection_name=f\"{domain}_col\",\n",
        "            embedding_function=self.embeddings,\n",
        "            persist_directory=persist_path\n",
        "        )\n",
        "        self.vector_stores[domain] = store\n",
        "        return store\n",
        "\n",
        "    def split_documents(self, docs: List[Document]) -> List[Document]:\n",
        "        \"\"\"å°†é•¿æ–‡æ¡£åˆ†å‰²ä¸ºå°chunks\n",
        "\n",
        "        Args:\n",
        "            docs: Documentåˆ—è¡¨\n",
        "\n",
        "        Returns:\n",
        "            åˆ†å‰²åçš„Documentåˆ—è¡¨\n",
        "\n",
        "        Note:\n",
        "            chunk_size=1000, overlap=200ï¼ˆæŒ‰å…¨å±€é…ç½®ï¼‰\n",
        "        \"\"\"\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        "        )\n",
        "        return splitter.split_documents(docs)\n",
        "\n",
        "    def add_documents(self, domain: str, docs: List[Document], metadata: Dict = None):\n",
        "        \"\"\"å‘é‡åŒ–æ–‡æ¡£å¹¶æ·»åŠ åˆ°å­˜å‚¨\n",
        "\n",
        "        Args:\n",
        "            domain: é¢†åŸŸåç§°\n",
        "            docs: Documentåˆ—è¡¨\n",
        "            metadata: é™„åŠ åˆ°æ¯ä¸ªchunkçš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰\n",
        "\n",
        "        Note:\n",
        "            - è‡ªåŠ¨åˆ†å‰²æ–‡æ¡£ä¸ºchunks\n",
        "            - é™„åŠ metadataåˆ°æ¯ä¸ªchunk\n",
        "            - æŒä¹…åŒ–åˆ°Chroma\n",
        "        \"\"\"\n",
        "        if not docs:\n",
        "            print(\"  [è­¦å‘Š] æ— æ–‡æ¡£,è·³è¿‡å‘é‡åŒ–\")\n",
        "            return\n",
        "\n",
        "        chunks = self.split_documents(docs)\n",
        "        if metadata:\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata.update(metadata)\n",
        "\n",
        "        store = self.get_or_create_store(domain)\n",
        "        store.add_documents(chunks)\n",
        "        print(f\"  -> å‘é‡åŒ–: {len(chunks)} chunks\")\n",
        "\n",
        "print(\"æ­¥éª¤4å®Œæˆï¼šVectorStoreManager\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤5: KnowledgeProcessor - å®Œæ•´Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­¥éª¤5å®Œæˆï¼šKnowledgeProcessor\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeProcessor:\n",
        "    \"\"\"çŸ¥è¯†å¤„ç†Pipelineåè°ƒå™¨\n",
        "\n",
        "    æ•´åˆ5ä¸ªæ ¸å¿ƒæ¨¡å—,æ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†å¤„ç†æµç¨‹:\n",
        "    1. æ–‡ä»¶æ‰«æåˆ†ç»„ (KnowledgeOrganizer)\n",
        "    2. æ–‡æ¡£åŠ è½½ (DocumentLoader)\n",
        "    3. çŸ¥è¯†æå– (KnowledgeExtractor)\n",
        "    4. å‘é‡åŒ–å­˜å‚¨ (VectorStoreManager)\n",
        "    5. JSONå­˜å‚¨\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 knowledge_base_dir: str,\n",
        "                 memories_dir: str = str(STRUCTURED_JSON_DIR),\n",
        "                 vector_db_dir: str = str(VECTOR_DB_DIR)):\n",
        "        \"\"\"åˆå§‹åŒ–Pipelineåè°ƒå™¨\n",
        "\n",
        "        Args:\n",
        "            knowledge_base_dir: çŸ¥è¯†åº“æ ¹ç›®å½•\n",
        "            memories_dir: JSONç»“æ„åŒ–çŸ¥è¯†å­˜å‚¨ç›®å½•\n",
        "            vector_db_dir: å‘é‡æ•°æ®åº“å­˜å‚¨ç›®å½•\n",
        "        \"\"\"\n",
        "        self.organizer = KnowledgeOrganizer(knowledge_base_dir)\n",
        "        self.loader = DocumentLoader()\n",
        "        self.extractor = KnowledgeExtractor()\n",
        "        self.vector_manager = VectorStoreManager(persist_directory=vector_db_dir)\n",
        "        self.memories_dir = Path(memories_dir)\n",
        "        logger.info(\"Pipelineåè°ƒå™¨åˆå§‹åŒ–å®Œæˆ\")\n",
        "\n",
        "    def save_to_memories(self, domain: str, group_key: str, knowledge: Dict):\n",
        "        \"\"\"å°†ç»“æ„åŒ–çŸ¥è¯†ä¿å­˜ä¸ºJSONæ–‡ä»¶\n",
        "\n",
        "        Args:\n",
        "            domain: é¢†åŸŸåç§°ï¼ˆç”¨ä½œå­ç›®å½•ï¼‰\n",
        "            group_key: çŸ¥è¯†ç»„å”¯ä¸€é”®\n",
        "            knowledge: ç»“æ„åŒ–çŸ¥è¯†å­—å…¸\n",
        "\n",
        "        Note:\n",
        "            ä¿å­˜è·¯å¾„: memories_dir/domain/group_key.json\n",
        "        \"\"\"\n",
        "        domain_dir = self.memories_dir / domain\n",
        "        domain_dir.mkdir(parents=True, exist_ok=True)\n",
        "        json_file = domain_dir / f\"{group_key}.json\"\n",
        "        with open(json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(knowledge, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"  -> JSON: {json_file.name}\")\n",
        "\n",
        "    def process_all(self, limit: int = None):\n",
        "        \"\"\"æ‰§è¡Œå®Œæ•´Pipelineå¤„ç†æ‰€æœ‰çŸ¥è¯†æ–‡ä»¶\n",
        "\n",
        "        Args:\n",
        "            limit: é™åˆ¶å¤„ç†æ•°é‡ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰\n",
        "\n",
        "        Note:\n",
        "            å¤„ç†æµç¨‹ï¼š\n",
        "            1. æ‰«æåˆ†ç»„ï¼ˆKnowledgeOrganizerï¼‰\n",
        "            2. åŠ è½½æ¸…æ´—ï¼ˆDocumentLoaderï¼‰\n",
        "            3. çŸ¥è¯†æå–ï¼ˆKnowledgeExtractorï¼‰\n",
        "            4. å‘é‡åŒ–å­˜å‚¨ï¼ˆVectorStoreManagerï¼‰\n",
        "            5. JSONä¿å­˜\n",
        "\n",
        "        Example:\n",
        "            >>> processor.process_all(limit=2)  # æµ‹è¯•ï¼šåªå¤„ç†å‰2ä¸ª\n",
        "            >>> processor.process_all()  # æ­£å¼ï¼šå¤„ç†æ‰€æœ‰\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"å¼€å§‹å®Œæ•´Pipeline\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        organized = self.organizer.scan_and_organize()\n",
        "\n",
        "        for domain, groups in organized.items():\n",
        "            total = len(groups) if not limit else min(limit, len(groups))\n",
        "            print(f\"\\né¢†åŸŸ: {domain} (å…±{total}ä¸ªçŸ¥è¯†å—)\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            count = 0\n",
        "            for group_key, group in groups.items():\n",
        "                if limit and count >= limit:\n",
        "                    print(f\"\\nè¾¾åˆ°é™åˆ¶({limit}),åœæ­¢\")\n",
        "                    break\n",
        "\n",
        "                # å®æ—¶è¿›åº¦æ˜¾ç¤º\n",
        "                progress = (count + 1) / total * 100\n",
        "                print(f\"\\n[{count+1}/{total}] è¿›åº¦: {progress:.1f}% | {group.topic}\")\n",
        "\n",
        "                # åŠ è½½æ‰€æœ‰æ–‡ä»¶\n",
        "                all_docs = []\n",
        "                for file_info in group.files:\n",
        "                    docs = self.loader.load_and_clean(file_info.path)\n",
        "                    if docs:\n",
        "                        all_docs.extend(docs)\n",
        "                        print(f\"  -> åŠ è½½ {file_info.path.suffix}: {len(docs)} é¡µ\")\n",
        "\n",
        "                if not all_docs:\n",
        "                    print(\"  [è­¦å‘Š] æ‰€æœ‰æ–‡ä»¶åŠ è½½å¤±è´¥\")\n",
        "                    continue\n",
        "                print(f\"  -> æ€»è®¡: {len(all_docs)} é¡µ\")\n",
        "\n",
        "                # æå–\n",
        "                knowledge = self.extractor.extract_from_documents(all_docs, group.topic)\n",
        "                self.save_to_memories(domain, group_key, knowledge)\n",
        "\n",
        "                # å‘é‡åŒ–\n",
        "                self.vector_manager.add_documents(domain, all_docs,\n",
        "                    {\"domain\": domain, \"topic\": group.topic, \"seq\": group.sequence})\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            print(f\"\\n[å®Œæˆ] {domain}: {count}/{total} ä¸ª\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Pipelineå®Œæˆ!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nè¾“å‡ºç›®å½•:\")\n",
        "        print(f\"  - JSON: {self.memories_dir}\")\n",
        "        print(f\"  - å‘é‡åº“: {self.vector_manager.persist_directory}\")\n",
        "\n",
        "print(\"æ­¥éª¤5å®Œæˆï¼šKnowledgeProcessor\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:53:53,096 - INFO - LLMåˆå§‹åŒ–: deepseek-reasoner\n",
            "2025-12-03 21:53:53,100 - INFO - Use pytorch device_name: cpu\n",
            "2025-12-03 21:53:53,101 - INFO - Load pretrained SentenceTransformer: Qwen/Qwen3-Embedding-0.6B\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:54:08,067 - INFO - 1 prompt is loaded, with the key: query\n",
            "2025-12-03 21:54:08,075 - INFO - Embeddingåˆå§‹åŒ–: Qwen/Qwen3-Embedding-0.6B\n",
            "2025-12-03 21:54:08,076 - INFO - Pipelineåè°ƒå™¨åˆå§‹åŒ–å®Œæˆ\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipelineåˆå§‹åŒ–å®Œæˆ\n",
            "çŸ¥è¯†åº“: ../knowledge_base\n",
            "JSONè¾“å‡º: output/structured_knowledge\n",
            "å‘é‡åº“: output/vector_db\n"
          ]
        }
      ],
      "source": [
        "# ========== Pipelineæµ‹è¯• ==========\n",
        "\n",
        "# åˆå§‹åŒ–Processor(ä½¿ç”¨å…¨å±€é…ç½®)\n",
        "processor = KnowledgeProcessor(\n",
        "    knowledge_base_dir=str(KNOWLEDGE_BASE_DIR)\n",
        ")\n",
        "\n",
        "print(\"Pipelineåˆå§‹åŒ–å®Œæˆ\")\n",
        "print(f\"çŸ¥è¯†åº“: {KNOWLEDGE_BASE_DIR}\")\n",
        "print(f\"JSONè¾“å‡º: {STRUCTURED_JSON_DIR}\")\n",
        "print(f\"å‘é‡åº“: {VECTOR_DB_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:54:08,161 - INFO - åŠ è½½ 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€[é˜²æ–­æ›´å¾®coc36666]_ç¬”è®°.pdf: 4é¡µ\n",
            "2025-12-03 21:54:08,163 - ERROR - WordåŠ è½½å¤±è´¥ 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€[é˜²æ–­æ›´å¾®coc36666].doc: No module named 'docx2txt'\n",
            "2025-12-03 21:54:08,165 - ERROR - PPTåŠ è½½å¤±è´¥ 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€[é˜²æ–­æ›´å¾®coc36666]_20250706193405.pptx: unstructured package not found, please install it with `pip install unstructured`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "å¼€å§‹å®Œæ•´Pipeline\n",
            "================================================================================\n",
            "\n",
            "æ‰«æç›®å½•: ../knowledge_base\n",
            "æ‰¾åˆ° 51 ä¸ªæ–‡ä»¶\n",
            "[å®Œæˆ] 14.ç¬¬åå››èŠ‚ æ±‡ç‡æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 07ç¬¬ä¸ƒèŠ‚ ç‰©ä»·â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç» (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 11.ç¬¬åä¸€èŠ‚ åŸºé‡‘æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 12.ç¬¬åäºŒèŠ‚ ä¿é™©æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 08ç¬¬å…«èŠ‚ å¦‚ä½•è¯»æ‡‚ç»æµå‘¨æœŸ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 15.ç¬¬åäº”èŠ‚ å¤§å®—å•†å“æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 05ç¬¬äº”èŠ‚ PMIâ€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 06ç¬¬å…­èŠ‚ é‡‘èâ€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 17.ç¬¬åä¸ƒèŠ‚ æ ¼é›·å„å§†ï¼šåå°”è¡—æ•™çˆ¶ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 13.ç¬¬åä¸‰èŠ‚ é»„é‡‘æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 03ç¬¬ä¸‰èŠ‚ æŠ•èµ„â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 04ç¬¬å››èŠ‚ å‡ºå£â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 09ç¬¬ä¹èŠ‚ çœ‹æ‡‚æŠ•èµ„æ—¶é’Ÿï¼Œè¸©å‡†æŠ•èµ„èŠ‚å¥ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 16.ç¬¬åå…­èŠ‚ æˆ¿åœ°äº§æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 10ç¬¬åèŠ‚ è‚¡å¸‚æŠ•èµ„æ‰‹å†Œ (3æ–‡ä»¶)\n",
            "[å®Œæˆ] 17 ä¸ªçŸ¥è¯†å—\n",
            "\n",
            "\n",
            "é¢†åŸŸ: knowledge_base (å…±2ä¸ªçŸ¥è¯†å—)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/2] è¿›åº¦: 50.0% | 01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€\n",
            "  -> åŠ è½½ .pdf: 4 é¡µ\n",
            "  -> æ€»è®¡: 4 é¡µ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:54:08,400 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  -> JSON: 01_01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„â€œä¸‰é©¾é©¬è½¦â€.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:55:13,259 - INFO - åŠ è½½ 02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿[é˜²æ–­æ›´å¾®coc36666]_ç¬”è®°.pdf: 3é¡µ\n",
            "2025-12-03 21:55:13,263 - ERROR - WordåŠ è½½å¤±è´¥ 02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿[é˜²æ–­æ›´å¾®coc36666].doc: No module named 'docx2txt'\n",
            "2025-12-03 21:55:13,265 - ERROR - PPTåŠ è½½å¤±è´¥ 02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿[é˜²æ–­æ›´å¾®coc36666]_20250707140225.pptx: unstructured package not found, please install it with `pip install unstructured`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  -> å‘é‡åŒ–: 4 chunks\n",
            "\n",
            "[2/2] è¿›åº¦: 100.0% | 02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿\n",
            "  -> åŠ è½½ .pdf: 3 é¡µ\n",
            "  -> æ€»è®¡: 3 é¡µ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-03 21:55:13,389 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  -> JSON: 02_02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨è¯»æ‡‚ç»æµå½¢åŠ¿.json\n",
            "  -> å‘é‡åŒ–: 3 chunks\n",
            "\n",
            "è¾¾åˆ°é™åˆ¶(2),åœæ­¢\n",
            "\n",
            "[å®Œæˆ] knowledge_base: 2/2 ä¸ª\n",
            "\n",
            "================================================================================\n",
            "Pipelineå®Œæˆ!\n",
            "================================================================================\n",
            "\n",
            "è¾“å‡ºç›®å½•:\n",
            "  - JSON: output/structured_knowledge\n",
            "  - å‘é‡åº“: output/vector_db\n"
          ]
        }
      ],
      "source": [
        "# æµ‹è¯•: å¤„ç†å‰2ä¸ªçŸ¥è¯†å—\n",
        "processor.process_all(limit=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­£å¼è¿è¡Œ: å¤„ç†æ‰€æœ‰çŸ¥è¯†å—(å–æ¶ˆæ³¨é‡Š)\n",
        "# processor.process_all()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ£€æŸ¥è¾“å‡º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[è­¦å‘Š] å°šæœªç”Ÿæˆè¾“å‡º\n"
          ]
        }
      ],
      "source": [
        "# æŸ¥çœ‹ç”Ÿæˆçš„JSON\n",
        "memories_path = Path(\"./memories/knowledge\")\n",
        "if memories_path.exists():\n",
        "    for domain_dir in memories_path.iterdir():\n",
        "        if domain_dir.is_dir():\n",
        "            print(f\"\\nğŸ“‚ {domain_dir.name}:\")\n",
        "            for json_file in sorted(domain_dir.glob(\"*.json\")):\n",
        "                print(f\"  - {json_file.name}\")\n",
        "else:\n",
        "    print(\"[è­¦å‘Š] å°šæœªç”Ÿæˆè¾“å‡º\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[è­¦å‘Š] æ— JSON\n"
          ]
        }
      ],
      "source": [
        "# è¯»å–ç¬¬ä¸€ä¸ªJSONæŸ¥çœ‹\n",
        "json_files = list(Path(\"./memories/knowledge\").rglob(\"*.json\"))\n",
        "if json_files:\n",
        "    with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "        sample = json.load(f)\n",
        "    print(f\"ç¤ºä¾‹ ({json_files[0].name}):\")\n",
        "    print(json.dumps(sample, ensure_ascii=False, indent=2))\n",
        "else:\n",
        "    print(\"[è­¦å‘Š] æ— JSON\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“\n",
        "\n",
        "### âœ… åŠŸèƒ½å®Œæ•´\n",
        "\n",
        "- æ­¥éª¤1: æ–‡ä»¶æ‰«æåˆ†ç»„ âœ“\n",
        "- æ­¥éª¤2: æ–‡æ¡£åŠ è½½æ¸…æ´— âœ“ (ä»…PDF)\n",
        "- æ­¥éª¤3: LLMç»“æ„åŒ–æå– âœ“\n",
        "- æ­¥éª¤4: å‘é‡åŒ–å­˜å‚¨ âœ“\n",
        "- æ­¥éª¤5: Pipelineåè°ƒ âœ“\n",
        "\n",
        "### ğŸ“¦ è¾“å‡º\n",
        "\n",
        "- `memories/knowledge/{domain}/*.json` - ç»“æ„åŒ–çŸ¥è¯†\n",
        "- `vector_db/{domain}/` - å‘é‡æ•°æ®åº“\n",
        "\n",
        "### ğŸš€ ä½¿ç”¨\n",
        "\n",
        "1. å…ˆç”¨`limit=2`æµ‹è¯•\n",
        "2. ç¡®è®¤å`process_all()`å¤„ç†å…¨éƒ¨\n",
        "3. æ£€æŸ¥è¾“å‡ºç»“æœ\n",
        "\n",
        "### âš™ï¸ è¯´æ˜\n",
        "\n",
        "- **ä»…æ”¯æŒPDF**: ç®€åŒ–ç‰ˆåªå¤„ç†PDF(ä¸»è¦æ ¼å¼)\n",
        "- **éœ€API Key**: DeepSeek API (åœ¨.envé…ç½®)\n",
        "- **é¦–æ¬¡ä¸‹è½½**: embeddingæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "analyst_chain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

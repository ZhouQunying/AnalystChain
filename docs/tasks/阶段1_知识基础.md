# 阶段1：知识基础设施

**目标**：建立知识库（向量库+JSON）
**状态**：✅ 已完成

## 为什么这样做

**架构定位**：3层架构的【知识层】

**双存储设计理由**：
- **向量库**：支持语义检索（如"经济周期转折信号"能匹配到"领先指标变化"）
- **JSON**：支持快速精确查询（如直接获取"投资时钟"完整框架）

**技术选型**：
- Chroma向量库：轻量级、持久化、Python原生
- Qwen3-Embedding：中文效果好、模型小（0.6B）、本地运行快
- DeepSeek-v3：知识提取质量高、成本低（¥1/M tokens）

## 任务

| # | 任务 | What | Why | How | How Much | 状态 |
|---|------|------|-----|-----|----------|------|
| 1 | 环境准备 | 验证依赖和文件 | 确保环境可用 | 验证analyst_chain环境已激活+data/raw/knowledge_base/macro_economy/有知识文件 | 依赖完整+文件就绪 | ✅ |
| 2 | Pipeline实现 | 构建知识处理流程 | 生成向量库和JSON | 创建notebooks/stage1_macro_knowledge_pipeline.ipynb+按"代码示例"区实现5个模块+执行验证输出 | Pipeline可运行+输出正确 | ✅ |
| 3 | 检索验证 | 验证检索准确性 | 确保知识可用 | 在Pipeline notebook最后添加cell+运行5个测试查询（如"什么是经济周期"）+验证相关性 | 5个查询结果准确 | ✅ |
| 4 | 向量库构建 | 生成完整向量库 | 供Agent检索知识 | 执行Pipeline（任务#2）+验证data/processed/knowledge/vector_db/有chroma.sqlite3 | data/processed/knowledge/vector_db/可用 | ✅ |

---

## 代码示例

### Pipeline核心结构（5个模块）

**模块1：KnowledgeOrganizer** - 文件分组
```python
from pathlib import Path
from dataclasses import dataclass

@dataclass
class TopicGroup:
    topic_name: str
    keywords: list
    files: list

class KnowledgeOrganizer:
    def __init__(self, knowledge_dir: Path):
        self.knowledge_dir = knowledge_dir

    def organize_by_topic(self) -> List[TopicGroup]:
        # 定义17个主题及关键词（如"经济周期": ["周期", "复苏", "衰退"]）
        # 遍历51个文件，根据文件名匹配主题
        # 返回分组结果
        pass
```

**模块2：DocumentLoader** - 加载文档
```python
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader

def load_documents(file_path: Path) -> List[Document]:
    if file_path.suffix == '.pdf':
        loader = PyMuPDFLoader(str(file_path))
    elif file_path.suffix == '.docx':
        loader = Docx2txtLoader(str(file_path))
    return loader.load()
```

**模块3：KnowledgeExtractor** - 提取结构化知识
```python
from langchain_openai import ChatOpenAI

def extract_knowledge(documents: List[Document], topic: str) -> dict:
    llm = ChatOpenAI(model="deepseek-v3")
    prompt = f"从以下文档中提取关于{topic}的核心知识..."
    # 调用LLM提取知识
    # 返回JSON格式：{"topic": "经济周期", "key_concepts": [...], "frameworks": [...]}
    pass
```

**模块4：VectorStoreManager** - 构建向量库
```python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma

def build_vector_store(documents: List[Document], persist_dir: Path):
    embeddings = HuggingFaceEmbeddings(model_name="Qwen3-Embedding-0.6B")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=str(persist_dir)
    )
    return vectorstore
```

**模块5：KnowledgeProcessor** - 主流程
```python
def process_all_knowledge():
    # 1. 组织文件（模块1）
    organizer = KnowledgeOrganizer(Path("data/raw/knowledge_base/macro_economy/"))
    topic_groups = organizer.organize_by_topic()

    # 2. 对每个主题：加载文档（模块2）→ 提取知识（模块3）→ 构建向量库（模块4）
    for group in topic_groups:
        documents = []
        for file in group.files:
            documents.extend(load_documents(file))

        # 提取JSON知识
        json_knowledge = extract_knowledge(documents, group.topic_name)
        save_json(json_knowledge, f"data/processed/knowledge/json/{group.topic_name}.json")

        # 构建向量库
        build_vector_store(documents, Path("data/processed/knowledge/vector_db/"))
```

**完整实现**：参考现有的`notebooks/stage1_macro_knowledge_pipeline.ipynb`（已实现）

---

**规范**：[AI工作规范](../AI工作规范.md)

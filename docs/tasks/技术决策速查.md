# 技术决策速查

> **快速查询项目技术选型,完整推理过程见历史版本**

---

## 决策总览

| # | 决策 | 结论 | 阶段 |
|---|------|------|------|
| #001 | 知识存储 | 向量库+JSON双存储 | 阶段1 |
| #002 | 向量库 | Chroma | 阶段1 |
| #003 | Embedding | Qwen3-Embedding-0.6B | 阶段1 |
| #004 | Agent框架 | DeepAgents | 阶段2 |
| #005 | 知识提取LLM | deepseek-reasoner | 阶段1 |
| #006 | Agent LLM | deepseek-chat | 阶段2 |
| #007 | Chunk大小 | 800字符 | 阶段1 |
| #008 | Chunk重叠 | 100字符 | 阶段1 |
| #009 | 内容预览长度 | 150字符 | 阶段1 |
| #010 | 检索Top-K | 3个结果 | 阶段1 |

---

## 决策详情

### #001: 知识存储方式
**结论**: 向量库 + JSON双存储
**理由**: 向量库做语义检索,JSON做结构化查询,优势互补
**影响**: 需LLM提取模块,开发时间+1天

### #002: 向量库选择
**结论**: Chroma
**理由**: LangChain官方推荐,轻量级,本地持久化,API简单
**影响**: 本地开发更方便,无云服务依赖

### #003: Embedding模型
**结论**: Qwen3-Embedding-0.6B
**理由**: 中文优化,免费离线,1024维度表达能力强
**影响**: 首次下载~600MB,中文检索效果优秀

### #004: Agent框架
**结论**: DeepAgents
**理由**: LangChain官方新框架,内置Planning/FileSystem/SubAgent,适合投资分析
**影响**: Agent开发效率提升,内存管理简单

### #005: 知识提取LLM
**结论**: deepseek-reasoner
**理由**: 深度推理能力强,逻辑分析准确,中文能力出色,长文本处理强
**影响**: 知识提取质量高,分析深度好

### #006: Agent LLM模型
**结论**: deepseek-chat
**理由**: 支持工具调用+流式输出,成本低(¥1/M),响应稳定,中文能力强
**影响**: Agent开发成本低,响应质量高

**影响**: 2-3分钟响应是ReAct正常表现,成本¥1/M

### #007: Chunk大小配置
**结论**: 800字符
**理由**: 平衡完整性与精度,约1个完整段落,过小碎片化,过大稀释相关性
**影响**: 宏观经济文档平均1400字/页,800字约半页,检索精度提升

### #008: Chunk重叠配置
**结论**: 100字符(12.5%重叠)
**理由**: 保证上下文连续性,避免关键信息被切断在chunk边界
**影响**: 略增存储量(12.5%),显著提升检索连贯性

### #009: 内容预览长度
**结论**: 150字符(约75字)
**理由**: Token经济(节省90%:337 vs 3600 tokens),同时保留2-3句核心观点
**影响**: 80%查询满意,20%需补充查询JSON

### #010: 检索结果数量
**结论**: Top-3
**理由**: 多结果综合提高覆盖率,3个平衡信息量与token消耗
**影响**: 3×150字≈450 tokens,在上下文预算内

---

## 完整推理过程

如需查看完整的背景/候选方案/对比分析,请查看:
- 历史版本: `docs/tasks/技术决策.md` (已归档到_archive/)
- 或询问AI重新生成

---

**此文档只保留结论,避免重复阅读。**


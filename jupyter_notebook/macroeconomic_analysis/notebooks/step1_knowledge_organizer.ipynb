{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# çŸ¥è¯†æ–‡ä»¶æ‰«æä¸æ™ºèƒ½åˆ†ç»„ - æ­¥éª¤1å®ç°\n",
        "\n",
        "## ç›®æ ‡\n",
        "\n",
        "å®ç°ä¸€ä¸ªæ™ºèƒ½çš„çŸ¥è¯†æ–‡ä»¶ç»„ç»‡å™¨ï¼Œèƒ½å¤Ÿï¼š\n",
        "\n",
        "1. **è‡ªåŠ¨æ‰«æ**ç›®å½•ä¸‹çš„æ‰€æœ‰çŸ¥è¯†æ–‡ä»¶ï¼ˆPDF/DOC/PPTXï¼‰\n",
        "2. **æ™ºèƒ½è¯†åˆ«**åŒä¸€çŸ¥è¯†å—çš„ä¸åŒæ ¼å¼æ–‡ä»¶ï¼ˆåŸºäºæ–‡ä»¶åç›¸ä¼¼åº¦ï¼‰\n",
        "3. **è‡ªåŠ¨æ¸…æ´—**æ–‡ä»¶åä¸­çš„å™ªéŸ³æ ‡è®°ï¼ˆ`[é˜²æ–­æ›´å¾®xxx]`ã€æ—¶é—´æˆ³ç­‰ï¼‰\n",
        "4. **æå–åºå·**å¹¶æŒ‰åºå·æ’åºçŸ¥è¯†å—\n",
        "5. **é€‰æ‹©ä¸»æ–‡ä»¶**ï¼ˆä¼˜å…ˆçº§ï¼šPDFç¬”è®° > DOC > PPTXï¼‰\n",
        "\n",
        "## è®¾è®¡åŸåˆ™\n",
        "\n",
        "- **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨ Type Hints\n",
        "- **é”™è¯¯å¤„ç†**ï¼šä¼˜é›…å¤„ç†å¼‚å¸¸æƒ…å†µ\n",
        "- **å¯é…ç½®**ï¼šå…³é”®å‚æ•°å¯è°ƒæ•´\n",
        "- **å¯æµ‹è¯•**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºå•å…ƒæµ‹è¯•\n",
        "- **æ–‡æ¡£åŒ–**ï¼šè¯¦ç»†çš„ Docstring\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. å¯¼å…¥ä¾èµ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Set\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import defaultdict\n",
        "from difflib import SequenceMatcher\n",
        "from enum import IntEnum\n",
        "\n",
        "print(\"âœ… ä¾èµ–å¯¼å…¥å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. æ•°æ®ç»“æ„å®šä¹‰\n",
        "\n",
        "ä½¿ç”¨ `dataclass` å®šä¹‰æ¸…æ™°çš„æ•°æ®ç»“æ„ï¼Œä¾¿äºç»´æŠ¤å’Œåºåˆ—åŒ–ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FilePriority(IntEnum):\n",
        "    \"\"\"æ–‡ä»¶ä¼˜å…ˆçº§æšä¸¾ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰\"\"\"\n",
        "    PDF_NOTE = 1      # PDFç¬”è®°ï¼šå†…å®¹æœ€å®Œæ•´\n",
        "    WORD_DOC = 2      # Wordæ–‡æ¡£ï¼šè¯¦ç»†è¯´æ˜\n",
        "    PDF_REGULAR = 3   # æ™®é€šPDF\n",
        "    POWERPOINT = 4    # PPTï¼šå›¾è¡¨å¤šä½†æ–‡å­—å°‘\n",
        "    UNKNOWN = 99      # æœªçŸ¥æ ¼å¼\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"æ–‡ä»¶ä¿¡æ¯æ•°æ®ç±»\"\"\"\n",
        "    path: Path                    # æ–‡ä»¶è·¯å¾„\n",
        "    original_name: str            # åŸå§‹æ–‡ä»¶å\n",
        "    cleaned_name: str             # æ¸…æ´—åçš„æ–‡ä»¶å\n",
        "    sequence: int                 # åºå·ï¼ˆç”¨äºæ’åºï¼‰\n",
        "    sequence_str: str             # åºå·å­—ç¬¦ä¸²ï¼ˆå¦‚\"01\"ï¼‰\n",
        "    priority: FilePriority        # æ–‡ä»¶ä¼˜å…ˆçº§\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"FileInfo({self.sequence_str}_{self.cleaned_name[:20]}..., priority={self.priority.name})\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeGroup:\n",
        "    \"\"\"çŸ¥è¯†å—åˆ†ç»„æ•°æ®ç±»\"\"\"\n",
        "    group_key: str                # ç»„çš„å”¯ä¸€æ ‡è¯†\n",
        "    topic: str                    # çŸ¥è¯†ä¸»é¢˜\n",
        "    sequence: int                 # åºå·\n",
        "    files: List[FileInfo]         # è¯¥ç»„çš„æ‰€æœ‰æ–‡ä»¶\n",
        "    primary_file: FileInfo        # ä¸»æ–‡ä»¶ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰\n",
        "    file_types: List[str]         # æ–‡ä»¶ç±»å‹åˆ—è¡¨\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"è½¬æ¢ä¸ºå­—å…¸ï¼ˆä¾¿äºåºåˆ—åŒ–ï¼‰\"\"\"\n",
        "        return {\n",
        "            \"group_key\": self.group_key,\n",
        "            \"topic\": self.topic,\n",
        "            \"sequence\": self.sequence,\n",
        "            \"primary_file\": str(self.primary_file.path),\n",
        "            \"files\": [str(f.path) for f in self.files],\n",
        "            \"file_types\": self.file_types,\n",
        "        }\n",
        "\n",
        "print(\"âœ… æ•°æ®ç»“æ„å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Knowledge Organizer æ ¸å¿ƒç±»\n",
        "\n",
        "èµ„æ·±Pythonå¼€å‘çš„ä¸“ä¸šå®ç°ï¼š\n",
        "- å®Œæ•´ç±»å‹æç¤º\n",
        "- è¯¦ç»†Docstring  \n",
        "- ä¼˜é›…é”™è¯¯å¤„ç†\n",
        "- å¯é…ç½®å‚æ•°\n",
        "- æ—¥å¿—è¾“å‡º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeOrganizer:\n",
        "    \"\"\"\n",
        "    çŸ¥è¯†æ–‡ä»¶æ™ºèƒ½ç»„ç»‡å™¨\n",
        "\n",
        "    èŒè´£ï¼šæ‰«æã€æ¸…æ´—ã€åˆ†ç»„çŸ¥è¯†æ–‡ä»¶\n",
        "    è®¾è®¡æ¨¡å¼ï¼šå•ä¸€èŒè´£åŸåˆ™ï¼ˆSRPï¼‰\n",
        "    \"\"\"\n",
        "\n",
        "    SUPPORTED_EXTENSIONS: Set[str] = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}\n",
        "\n",
        "    NOISE_PATTERNS: List[str] = [\n",
        "        r'\\[é˜²æ–­æ›´.*?\\]',\n",
        "        r'\\[.*?å¾®.*?\\]',\n",
        "        r'_\\d{14}',\n",
        "        r'_\\d{8}',\n",
        "        r'_ç¬”è®°',\n",
        "        r'\\s*\\(.*?\\)\\s*',\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        knowledge_base_dir: str | Path,\n",
        "        similarity_threshold: float = 0.7,\n",
        "        verbose: bool = True\n",
        "    ):\n",
        "        self.knowledge_base_dir = Path(knowledge_base_dir)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if not self.knowledge_base_dir.exists():\n",
        "            raise ValueError(f\"ç›®å½•ä¸å­˜åœ¨: {self.knowledge_base_dir}\")\n",
        "        if not self.knowledge_base_dir.is_dir():\n",
        "            raise ValueError(f\"ä¸æ˜¯ç›®å½•: {self.knowledge_base_dir}\")\n",
        "\n",
        "        self._log(f\"âœ… åˆå§‹åŒ–å®Œæˆ: {self.knowledge_base_dir}\")\n",
        "\n",
        "    def _log(self, message: str) -> None:\n",
        "        if self.verbose:\n",
        "            print(message)\n",
        "\n",
        "    def clean_filename(self, filename: str) -> str:\n",
        "        \"\"\"æ¸…æ´—æ–‡ä»¶åï¼Œå»é™¤å™ªéŸ³æ ‡è®°\"\"\"\n",
        "        name = Path(filename).stem\n",
        "        for pattern in self.NOISE_PATTERNS:\n",
        "            name = re.sub(pattern, '', name)\n",
        "        name = re.sub(r'\\s+', ' ', name).strip()\n",
        "        return name\n",
        "\n",
        "    def extract_sequence_number(self, filename: str) -> Tuple[int, str]:\n",
        "        \"\"\"æå–åºå·\"\"\"\n",
        "        match = re.match(r'^(\\d+)', filename)\n",
        "        if match:\n",
        "            seq_str = match.group(1)\n",
        "            return (int(seq_str), seq_str)\n",
        "        return (999999, \"\")\n",
        "\n",
        "    def calculate_similarity(self, str1: str, str2: str) -> float:\n",
        "        \"\"\"è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰\"\"\"\n",
        "        return SequenceMatcher(None, str1, str2).ratio()\n",
        "\n",
        "    def get_file_priority(self, file_path: Path) -> FilePriority:\n",
        "        \"\"\"è·å–æ–‡ä»¶ä¼˜å…ˆçº§\"\"\"\n",
        "        name_lower = file_path.name.lower()\n",
        "        suffix = file_path.suffix.lower()\n",
        "\n",
        "        if 'ç¬”è®°' in name_lower and suffix == '.pdf':\n",
        "            return FilePriority.PDF_NOTE\n",
        "        if suffix in ['.doc', '.docx']:\n",
        "            return FilePriority.WORD_DOC\n",
        "        if suffix == '.pdf':\n",
        "            return FilePriority.PDF_REGULAR\n",
        "        if suffix in ['.ppt', '.pptx']:\n",
        "            return FilePriority.POWERPOINT\n",
        "        return FilePriority.UNKNOWN\n",
        "\n",
        "    def create_file_info(self, file_path: Path) -> FileInfo:\n",
        "        \"\"\"åˆ›å»ºæ–‡ä»¶ä¿¡æ¯å¯¹è±¡\"\"\"\n",
        "        original_name = file_path.name\n",
        "        cleaned_name = self.clean_filename(original_name)\n",
        "        sequence, sequence_str = self.extract_sequence_number(cleaned_name)\n",
        "        priority = self.get_file_priority(file_path)\n",
        "\n",
        "        return FileInfo(\n",
        "            path=file_path,\n",
        "            original_name=original_name,\n",
        "            cleaned_name=cleaned_name,\n",
        "            sequence=sequence,\n",
        "            sequence_str=sequence_str,\n",
        "            priority=priority\n",
        "        )\n",
        "\n",
        "    def group_files_by_similarity(\n",
        "        self,\n",
        "        files: List[FileInfo]\n",
        "    ) -> Dict[str, KnowledgeGroup]:\n",
        "        \"\"\"æ™ºèƒ½åˆ†ç»„ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰\"\"\"\n",
        "        if not files:\n",
        "            return {}\n",
        "\n",
        "        groups: Dict[str, KnowledgeGroup] = {}\n",
        "        processed: Set[Path] = set()\n",
        "\n",
        "        for i, file1 in enumerate(files):\n",
        "            if file1.path in processed:\n",
        "                continue\n",
        "\n",
        "            group_key = f\"{file1.sequence_str}_{file1.cleaned_name[:20]}\"\n",
        "            group_files = [file1]\n",
        "            processed.add(file1.path)\n",
        "\n",
        "            # æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶\n",
        "            for file2 in files[i+1:]:\n",
        "                if file2.path in processed:\n",
        "                    continue\n",
        "\n",
        "                if file1.sequence == file2.sequence:\n",
        "                    similarity = self.calculate_similarity(\n",
        "                        file1.cleaned_name,\n",
        "                        file2.cleaned_name\n",
        "                    )\n",
        "\n",
        "                    if similarity >= self.similarity_threshold:\n",
        "                        group_files.append(file2)\n",
        "                        processed.add(file2.path)\n",
        "                        self._log(f\"  â†³ ç›¸ä¼¼åº¦={similarity:.2f}: {file2.original_name}\")\n",
        "\n",
        "            # é€‰æ‹©ä¸»æ–‡ä»¶\n",
        "            group_files.sort(key=lambda f: (f.priority.value, f.original_name))\n",
        "            primary_file = group_files[0]\n",
        "\n",
        "            group = KnowledgeGroup(\n",
        "                group_key=group_key,\n",
        "                topic=file1.cleaned_name,\n",
        "                sequence=file1.sequence,\n",
        "                files=group_files,\n",
        "                primary_file=primary_file,\n",
        "                file_types=[f.path.suffix for f in group_files]\n",
        "            )\n",
        "\n",
        "            groups[group_key] = group\n",
        "            self._log(\n",
        "                f\"âœ“ çŸ¥è¯†å—: {file1.cleaned_name} \"\n",
        "                f\"({len(group_files)}ä¸ªæ–‡ä»¶, ä¸»: {primary_file.path.name})\"\n",
        "            )\n",
        "\n",
        "        return groups\n",
        "\n",
        "    def scan_and_organize(self) -> Dict[str, Dict[str, KnowledgeGroup]]:\n",
        "        \"\"\"æ‰«æå¹¶ç»„ç»‡ç›®å½•ï¼ˆä¸»æ–¹æ³•ï¼‰\"\"\"\n",
        "        result: Dict[str, Dict[str, KnowledgeGroup]] = defaultdict(dict)\n",
        "\n",
        "        self._log(f\"\\n{'='*80}\")\n",
        "        self._log(f\"ğŸ“š å¼€å§‹æ‰«æ: {self.knowledge_base_dir}\")\n",
        "        self._log(f\"{'='*80}\\n\")\n",
        "\n",
        "        # è·å–æ‰€æœ‰æ–‡ä»¶\n",
        "        all_files: List[Path] = []\n",
        "        for ext in self.SUPPORTED_EXTENSIONS:\n",
        "            all_files.extend(self.knowledge_base_dir.glob(f\"*{ext}\"))\n",
        "\n",
        "        if not all_files:\n",
        "            self._log(\"âš ï¸  æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶\")\n",
        "            return dict(result)\n",
        "\n",
        "        self._log(f\"ğŸ“‚ æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶\\n\")\n",
        "\n",
        "        # åˆ›å»ºFileInfo\n",
        "        file_infos = [self.create_file_info(f) for f in all_files]\n",
        "\n",
        "        # åˆ†ç»„\n",
        "        groups = self.group_files_by_similarity(file_infos)\n",
        "\n",
        "        # æ’åº\n",
        "        sorted_groups = dict(sorted(groups.items(), key=lambda x: x[1].sequence))\n",
        "\n",
        "        domain_name = self.knowledge_base_dir.name\n",
        "        result[domain_name] = sorted_groups\n",
        "\n",
        "        self._log(f\"\\n{'='*80}\")\n",
        "        self._log(f\"âœ… å®Œæˆ: {len(sorted_groups)} ä¸ªçŸ¥è¯†å—\")\n",
        "        self._log(f\"{'='*80}\\n\")\n",
        "\n",
        "        return dict(result)\n",
        "\n",
        "    def print_organization(self) -> Dict[str, Dict[str, KnowledgeGroup]]:\n",
        "        \"\"\"æ‰“å°ç»„ç»‡ç»“æ„ï¼ˆç”¨äºæ£€æŸ¥ï¼‰\"\"\"\n",
        "        organized = self.scan_and_organize()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ“Š çŸ¥è¯†åº“ç»„ç»‡ç»“æ„\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        for domain, groups in organized.items():\n",
        "            print(f\"ğŸ¯ é¢†åŸŸ: {domain}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            for group_key, group in groups.items():\n",
        "                print(f\"\\n  ğŸ“– #{group.sequence}: {group.topic}\")\n",
        "                print(f\"     ä¸»æ–‡ä»¶: {group.primary_file.original_name}\")\n",
        "                print(f\"     ä¼˜å…ˆçº§: {group.primary_file.priority.name}\")\n",
        "                print(f\"     æ–‡ä»¶æ•°: {len(group.files)}\")\n",
        "\n",
        "                if len(group.files) > 1:\n",
        "                    print(f\"     æ‰€æœ‰æ–‡ä»¶:\")\n",
        "                    for f in group.files:\n",
        "                        print(f\"       â€¢ {f.original_name} [{f.priority.name}]\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        return organized\n",
        "\n",
        "    def export_to_json(self, output_path: str | Path) -> None:\n",
        "        \"\"\"å¯¼å‡ºä¸ºJSON\"\"\"\n",
        "        organized = self.scan_and_organize()\n",
        "\n",
        "        export_data = {}\n",
        "        for domain, groups in organized.items():\n",
        "            export_data[domain] = {\n",
        "                group_key: group.to_dict()\n",
        "                for group_key, group in groups.items()\n",
        "            }\n",
        "\n",
        "        output_path = Path(output_path)\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        self._log(f\"âœ… å·²å¯¼å‡ºåˆ°: {output_path}\")\n",
        "\n",
        "\n",
        "print(\"âœ… KnowledgeOrganizer ç±»å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ä½¿ç”¨ç¤ºä¾‹\n",
        "\n",
        "**æ­¥éª¤1:** åˆå§‹åŒ–ç»„ç»‡å™¨\n",
        "**æ­¥éª¤2:** æ‰«æå¹¶æ‰“å°ç»„ç»‡ç»“æ„\n",
        "**æ­¥éª¤3:** å¯¼å‡ºJSONï¼ˆå¯é€‰ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­¥éª¤1: åˆå§‹åŒ–\n",
        "organizer = KnowledgeOrganizer(\n",
        "    knowledge_base_dir=\"/Users/zhou/Project/AnalystChain/jupyter_notebook/macroeconomic_analysis/knowledge_base\",\n",
        "    similarity_threshold=0.7,\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­¥éª¤2: æ‰“å°ç»„ç»‡ç»“æ„\n",
        "organized = organizer.print_organization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­¥éª¤3: å¯¼å‡ºJSONï¼ˆå¯é€‰ï¼‰\n",
        "organizer.export_to_json(\"knowledge_organization.json\")\n",
        "\n",
        "# æŸ¥çœ‹JSONå†…å®¹\n",
        "with open(\"knowledge_organization.json\", 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    print(json.dumps(data, ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. å•å…ƒæµ‹è¯•\n",
        "\n",
        "æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£å¸¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æµ‹è¯•: æ–‡ä»¶åæ¸…æ´—\n",
        "print(\"=\"*80)\n",
        "print(\"æµ‹è¯•: æ–‡ä»¶åæ¸…æ´—\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_cases = [\n",
        "    \"01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„ä¸‰é©¾é©¬è½¦[é˜²æ–­æ›´å¾®coc36666]_ç¬”è®°.pdf\",\n",
        "    \"02ç¬¬äºŒèŠ‚ æ¶ˆè´¹â€”â€”å¿«é€Ÿå…¥é—¨[é˜²æ–­æ›´å¾®coc36666]_20250707140225.pptx\",\n",
        "]\n",
        "\n",
        "for filename in test_cases:\n",
        "    cleaned = organizer.clean_filename(filename)\n",
        "    print(f\"åŸå§‹: {filename}\")\n",
        "    print(f\"æ¸…æ´—: {cleaned}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æµ‹è¯•: ç›¸ä¼¼åº¦è®¡ç®—\n",
        "print(\"=\"*80)\n",
        "print(\"æµ‹è¯•: ç›¸ä¼¼åº¦è®¡ç®—\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "pairs = [\n",
        "    (\"01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„ä¸‰é©¾é©¬è½¦\", \"01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµä¸‰é©¾é©¬è½¦\"),\n",
        "    (\"01ç¬¬ä¸€èŠ‚ ä¸­å›½ç»æµçš„ä¸‰é©¾é©¬è½¦\", \"02ç¬¬äºŒèŠ‚ æ¶ˆè´¹\"),\n",
        "]\n",
        "\n",
        "for str1, str2 in pairs:\n",
        "    sim = organizer.calculate_similarity(str1, str2)\n",
        "    print(f\"ç›¸ä¼¼åº¦: {sim:.3f}\")\n",
        "    print(f\"  æ–‡æœ¬1: {str1}\")\n",
        "    print(f\"  æ–‡æœ¬2: {str2}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. æ€»ç»“\n",
        "\n",
        "### âœ… å·²å®ç°åŠŸèƒ½\n",
        "\n",
        "1. **è‡ªåŠ¨æ‰«æ** - æ”¯æŒPDF/DOC/PPTXå¤šæ ¼å¼\n",
        "2. **æ™ºèƒ½æ¸…æ´—** - å»é™¤å™ªéŸ³æ ‡è®°\n",
        "3. **åºå·æå–** - è‡ªåŠ¨è¯†åˆ«æ’åº\n",
        "4. **ç›¸ä¼¼åº¦åˆ†ç»„** - difflib.SequenceMatcherç®—æ³•\n",
        "5. **ä¼˜å…ˆçº§é€‰æ‹©** - PDFç¬”è®°ä¼˜å…ˆ\n",
        "6. **ç±»å‹å®‰å…¨** - å®Œæ•´Type Hints\n",
        "7. **é”™è¯¯å¤„ç†** - ä¼˜é›…å¼‚å¸¸å¤„ç†\n",
        "8. **å¯é…ç½®** - similarity_thresholdå¯è°ƒ\n",
        "9. **å¯æµ‹è¯•** - æ¨¡å—åŒ–è®¾è®¡\n",
        "10. **å¯å¯¼å‡º** - JSONåºåˆ—åŒ–\n",
        "\n",
        "### ğŸ¯ ä»£ç è´¨é‡\n",
        "\n",
        "- âœ… Type Hints (PEP 484)\n",
        "- âœ… Docstrings (PEP 257)\n",
        "- âœ… Error Handling\n",
        "- âœ… Logging\n",
        "- âœ… PEP 8 Style\n",
        "- âœ… å•ä¸€èŒè´£åŸåˆ™\n",
        "- âœ… å¼€é—­åŸåˆ™\n",
        "\n",
        "### ğŸ’¡ ä½¿ç”¨å»ºè®®\n",
        "\n",
        "1. å…ˆç”¨ `print_organization()` æ£€æŸ¥åˆ†ç»„\n",
        "2. å¦‚åˆ†ç»„ä¸å‡†ç¡®,è°ƒæ•´ `similarity_threshold`\n",
        "3. å¦‚æœ‰æ–°å™ªéŸ³æ¨¡å¼,æ·»åŠ åˆ° `NOISE_PATTERNS`\n",
        "4. ç”¨ `export_to_json()` ä¿å­˜ç»“æœ\n",
        "\n",
        "### ğŸ“š ä¸‹ä¸€æ­¥\n",
        "\n",
        "- **æ­¥éª¤2**: æ–‡æ¡£åŠ è½½ä¸æ¸…æ´—\n",
        "- **æ­¥éª¤3**: ç»“æ„åŒ–çŸ¥è¯†æå–  \n",
        "- **æ­¥éª¤4**: å‘é‡åŒ–å­˜å‚¨\n",
        "- **æ­¥éª¤5**: ä¸»æ§åˆ¶å™¨é›†æˆ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
